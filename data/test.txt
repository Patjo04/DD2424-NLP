#define <token> KBUILD_MODNAME ": " fmt <answer> pr_fmt(fmt) 
<token> <linux/errno.h> <answer> #include 
<token> <linux/types.h> <answer> #include 
<token> <linux/socket.h> <answer> #include 
#include <token> <answer> <linux/in.h> 
#include <token> <answer> <linux/kernel.h> 
<token> <linux/timer.h> <answer> #include 
<token> <linux/string.h> <answer> #include 
#include <token> <answer> <linux/sockios.h> 
#include <token> <answer> <linux/net.h> 
#include <token> <answer> <linux/inet.h> 
#include <token> <answer> <linux/netdevice.h> 
#include <token> <answer> <linux/skbuff.h> 
#include <token> <answer> <linux/slab.h> 
<token> <net/sock.h> <answer> #include 
#include <token> <answer> <linux/uaccess.h> 
<token> <linux/fcntl.h> <answer> #include 
<token> <linux/mm.h> <answer> #include 
#include <token> <answer> <linux/interrupt.h> 
#include <token> <answer> <net/lapb.h> 
static void lapb_state0_machine(struct lapb_cb *lapb, struct <token> *skb, <answer> sk_buff 
struct lapb_frame <token> <answer> *frame) 
<token> (frame->type) { <answer> switch 
case <token> <answer> LAPB_SABM: 
lapb_dbg(1, "(%p) S0 RX <token> lapb->dev, frame->pf); <answer> SABM(%d)\n", 
if (lapb->mode & LAPB_EXTENDED) <token> <answer> { 
lapb_dbg(1, "(%p) S0 <token> DM(%d)\n", <answer> TX 
<token> frame->pf); <answer> lapb->dev, 
lapb_send_control(lapb, LAPB_DM, <token> <answer> frame->pf, 
} <token> { <answer> else 
lapb_dbg(1, "(%p) <token> TX UA(%d)\n", <answer> S0 
<token> frame->pf); <answer> lapb->dev, 
lapb_dbg(0, "(%p) S0 -> <token> lapb->dev); <answer> S3\n", 
lapb_send_control(lapb, LAPB_UA, <token> <answer> frame->pf, 
lapb->state = <token> <answer> LAPB_STATE_3; 
<token> = 0x00; <answer> lapb->condition 
<token> = 0; <answer> lapb->n2count 
lapb->vs <token> 0; <answer> = 
lapb->vr <token> 0; <answer> = 
lapb->va = <token> <answer> 0; 
<token> LAPB_OK); <answer> lapb_connect_indication(lapb, 
<token> LAPB_SABME: <answer> case 
lapb_dbg(1, "(%p) S0 RX SABME(%d)\n", lapb->dev, <token> <answer> frame->pf); 
<token> (lapb->mode & LAPB_EXTENDED) { <answer> if 
lapb_dbg(1, "(%p) S0 TX <token> <answer> UA(%d)\n", 
<token> frame->pf); <answer> lapb->dev, 
lapb_dbg(0, "(%p) S0 -> <token> lapb->dev); <answer> S3\n", 
<token> LAPB_UA, frame->pf, <answer> lapb_send_control(lapb, 
lapb->state <token> LAPB_STATE_3; <answer> = 
lapb->condition <token> 0x00; <answer> = 
<token> = 0; <answer> lapb->n2count 
lapb->vs = <token> <answer> 0; 
lapb->vr = <token> <answer> 0; 
<token> = 0; <answer> lapb->va 
lapb_connect_indication(lapb, <token> <answer> LAPB_OK); 
<token> else { <answer> } 
lapb_dbg(1, <token> S0 TX DM(%d)\n", <answer> "(%p) 
lapb->dev, <token> <answer> frame->pf); 
lapb_send_control(lapb, LAPB_DM, <token> <answer> frame->pf, 
<token> LAPB_DISC: <answer> case 
lapb_dbg(1, "(%p) S0 RX DISC(%d)\n", lapb->dev, <token> <answer> frame->pf); 
lapb_dbg(1, "(%p) <token> TX UA(%d)\n", lapb->dev, frame->pf); <answer> S0 
lapb_send_control(lapb, LAPB_UA, frame->pf, <token> <answer> LAPB_RESPONSE); 
static void lapb_state1_machine(struct lapb_cb <token> struct sk_buff *skb, <answer> *lapb, 
struct lapb_frame <token> <answer> *frame) 
switch <token> { <answer> (frame->type) 
<token> LAPB_SABM: <answer> case 
lapb_dbg(1, "(%p) S1 RX <token> lapb->dev, frame->pf); <answer> SABM(%d)\n", 
if (lapb->mode <token> LAPB_EXTENDED) { <answer> & 
lapb_dbg(1, "(%p) S1 TX <token> <answer> DM(%d)\n", 
lapb->dev, <token> <answer> frame->pf); 
lapb_send_control(lapb, <token> frame->pf, <answer> LAPB_DM, 
} <token> { <answer> else 
<token> "(%p) S1 TX UA(%d)\n", <answer> lapb_dbg(1, 
lapb->dev, <token> <answer> frame->pf); 
<token> LAPB_UA, frame->pf, <answer> lapb_send_control(lapb, 
case <token> <answer> LAPB_SABME: 
lapb_dbg(1, "(%p) S1 RX SABME(%d)\n", lapb->dev, <token> <answer> frame->pf); 
if <token> & LAPB_EXTENDED) { <answer> (lapb->mode 
lapb_dbg(1, "(%p) S1 TX <token> <answer> UA(%d)\n", 
lapb->dev, <token> <answer> frame->pf); 
lapb_send_control(lapb, LAPB_UA, <token> <answer> frame->pf, 
} <token> { <answer> else 
lapb_dbg(1, "(%p) S1 TX <token> <answer> DM(%d)\n", 
<token> frame->pf); <answer> lapb->dev, 
lapb_send_control(lapb, LAPB_DM, <token> <answer> frame->pf, 
case <token> <answer> LAPB_DISC: 
lapb_dbg(1, "(%p) S1 RX DISC(%d)\n", <token> frame->pf); <answer> lapb->dev, 
lapb_dbg(1, "(%p) S1 TX <token> lapb->dev, frame->pf); <answer> DM(%d)\n", 
lapb_send_control(lapb, <token> frame->pf, LAPB_RESPONSE); <answer> LAPB_DM, 
<token> LAPB_UA: <answer> case 
<token> "(%p) S1 RX UA(%d)\n", lapb->dev, frame->pf); <answer> lapb_dbg(1, 
<token> (frame->pf) { <answer> if 
lapb_dbg(0, "(%p) S1 <token> S3\n", lapb->dev); <answer> -> 
<token> = LAPB_STATE_3; <answer> lapb->state 
<token> = 0x00; <answer> lapb->condition 
<token> = 0; <answer> lapb->n2count 
lapb->vs <token> 0; <answer> = 
<token> = 0; <answer> lapb->vr 
lapb->va <token> 0; <answer> = 
<token> LAPB_OK); <answer> lapb_connect_confirmation(lapb, 
<token> LAPB_DM: <answer> case 
lapb_dbg(1, <token> S1 RX DM(%d)\n", lapb->dev, frame->pf); <answer> "(%p) 
if <token> { <answer> (frame->pf) 
lapb_dbg(0, <token> S1 -> S0\n", lapb->dev); <answer> "(%p) 
<token> = LAPB_STATE_0; <answer> lapb->state 
<token> LAPB_REFUSED); <answer> lapb_disconnect_indication(lapb, 
static void lapb_state2_machine(struct lapb_cb *lapb, struct sk_buff <token> <answer> *skb, 
<token> lapb_frame *frame) <answer> struct 
<token> (frame->type) { <answer> switch 
case <token> <answer> LAPB_SABM: 
case <token> <answer> LAPB_SABME: 
lapb_dbg(1, "(%p) S2 RX <token> <answer> {SABM,SABME}(%d)\n", 
<token> frame->pf); <answer> lapb->dev, 
lapb_dbg(1, "(%p) <token> TX DM(%d)\n", lapb->dev, frame->pf); <answer> S2 
<token> LAPB_DM, frame->pf, LAPB_RESPONSE); <answer> lapb_send_control(lapb, 
case <token> <answer> LAPB_DISC: 
lapb_dbg(1, "(%p) S2 RX DISC(%d)\n", <token> frame->pf); <answer> lapb->dev, 
lapb_dbg(1, "(%p) S2 TX UA(%d)\n", <token> frame->pf); <answer> lapb->dev, 
lapb_send_control(lapb, <token> frame->pf, LAPB_RESPONSE); <answer> LAPB_UA, 
case <token> <answer> LAPB_UA: 
lapb_dbg(1, "(%p) S2 <token> UA(%d)\n", lapb->dev, frame->pf); <answer> RX 
<token> (frame->pf) { <answer> if 
lapb_dbg(0, "(%p) <token> -> S0\n", lapb->dev); <answer> S2 
lapb->state = <token> <answer> LAPB_STATE_0; 
lapb_disconnect_confirmation(lapb, <token> <answer> LAPB_OK); 
<token> LAPB_DM: <answer> case 
lapb_dbg(1, "(%p) S2 <token> DM(%d)\n", lapb->dev, frame->pf); <answer> RX 
if <token> { <answer> (frame->pf) 
lapb_dbg(0, "(%p) S2 <token> S0\n", lapb->dev); <answer> -> 
<token> = LAPB_STATE_0; <answer> lapb->state 
<token> LAPB_NOTCONNECTED); <answer> lapb_disconnect_confirmation(lapb, 
case <token> <answer> LAPB_I: 
case <token> <answer> LAPB_REJ: 
<token> LAPB_RNR: <answer> case 
<token> LAPB_RR: <answer> case 
lapb_dbg(1, <token> S2 RX {I,REJ,RNR,RR}(%d)\n", <answer> "(%p) 
<token> frame->pf); <answer> lapb->dev, 
lapb_dbg(1, "(%p) <token> RX DM(%d)\n", lapb->dev, frame->pf); <answer> S2 
if <token> <answer> (frame->pf) 
<token> LAPB_DM, frame->pf, <answer> lapb_send_control(lapb, 
static void lapb_state3_machine(struct lapb_cb <token> struct sk_buff *skb, <answer> *lapb, 
struct <token> *frame) <answer> lapb_frame 
int queued <token> 0; <answer> = 
int modulus = (lapb->mode & LAPB_EXTENDED) <token> LAPB_EMODULUS : <answer> ? 
switch <token> { <answer> (frame->type) 
case <token> <answer> LAPB_SABM: 
lapb_dbg(1, <token> S3 RX SABM(%d)\n", lapb->dev, frame->pf); <answer> "(%p) 
if (lapb->mode & <token> { <answer> LAPB_EXTENDED) 
lapb_dbg(1, "(%p) S3 <token> DM(%d)\n", <answer> TX 
lapb->dev, <token> <answer> frame->pf); 
<token> LAPB_DM, frame->pf, <answer> lapb_send_control(lapb, 
} else <token> <answer> { 
lapb_dbg(1, "(%p) <token> TX UA(%d)\n", <answer> S3 
lapb->dev, <token> <answer> frame->pf); 
lapb_send_control(lapb, <token> frame->pf, <answer> LAPB_UA, 
lapb->condition = <token> <answer> 0x00; 
<token> = 0; <answer> lapb->n2count 
lapb->vs <token> 0; <answer> = 
<token> = 0; <answer> lapb->vr 
lapb->va <token> 0; <answer> = 
case <token> <answer> LAPB_SABME: 
lapb_dbg(1, "(%p) S3 RX SABME(%d)\n", lapb->dev, <token> <answer> frame->pf); 
if (lapb->mode & <token> { <answer> LAPB_EXTENDED) 
<token> "(%p) S3 TX UA(%d)\n", <answer> lapb_dbg(1, 
<token> frame->pf); <answer> lapb->dev, 
lapb_send_control(lapb, <token> frame->pf, <answer> LAPB_UA, 
lapb->condition = <token> <answer> 0x00; 
lapb->n2count = <token> <answer> 0; 
lapb->vs = <token> <answer> 0; 
lapb->vr <token> 0; <answer> = 
<token> = 0; <answer> lapb->va 
} else <token> <answer> { 
lapb_dbg(1, <token> S3 TX DM(%d)\n", <answer> "(%p) 
lapb->dev, <token> <answer> frame->pf); 
lapb_send_control(lapb, <token> frame->pf, <answer> LAPB_DM, 
<token> LAPB_DISC: <answer> case 
lapb_dbg(1, "(%p) S3 RX DISC(%d)\n", <token> frame->pf); <answer> lapb->dev, 
lapb_dbg(0, "(%p) S3 -> S0\n", <token> <answer> lapb->dev); 
lapb_send_control(lapb, LAPB_UA, frame->pf, <token> <answer> LAPB_RESPONSE); 
lapb->state = <token> <answer> LAPB_STATE_0; 
<token> LAPB_OK); <answer> lapb_disconnect_indication(lapb, 
<token> LAPB_DM: <answer> case 
<token> "(%p) S3 RX DM(%d)\n", lapb->dev, frame->pf); <answer> lapb_dbg(1, 
lapb_dbg(0, "(%p) <token> -> S0\n", lapb->dev); <answer> S3 
lapb->state = <token> <answer> LAPB_STATE_0; 
lapb_disconnect_indication(lapb, <token> <answer> LAPB_NOTCONNECTED); 
<token> LAPB_RNR: <answer> case 
lapb_dbg(1, "(%p) S3 RX <token> R%d\n", <answer> RNR(%d) 
lapb->dev, <token> frame->nr); <answer> frame->pf, 
lapb->condition |= <token> <answer> LAPB_PEER_RX_BUSY_CONDITION; 
lapb_check_need_response(lapb, frame->cr, <token> <answer> frame->pf); 
if <token> frame->nr)) { <answer> (lapb_validate_nr(lapb, 
<token> frame->nr); <answer> lapb_check_iframes_acked(lapb, 
} <token> { <answer> else 
lapb->frmr_data = <token> <answer> *frame; 
lapb->frmr_type = <token> <answer> LAPB_FRMR_Z; 
lapb_dbg(0, "(%p) S3 -> S4\n", <token> <answer> lapb->dev); 
lapb->state <token> LAPB_STATE_4; <answer> = 
lapb->n2count = <token> <answer> 0; 
<token> LAPB_RR: <answer> case 
lapb_dbg(1, "(%p) S3 RX <token> R%d\n", <answer> RR(%d) 
lapb->dev, frame->pf, <token> <answer> frame->nr); 
lapb->condition &= <token> <answer> ~LAPB_PEER_RX_BUSY_CONDITION; 
lapb_check_need_response(lapb, frame->cr, <token> <answer> frame->pf); 
if (lapb_validate_nr(lapb, frame->nr)) <token> <answer> { 
<token> frame->nr); <answer> lapb_check_iframes_acked(lapb, 
<token> else { <answer> } 
lapb->frmr_data <token> *frame; <answer> = 
lapb->frmr_type <token> LAPB_FRMR_Z; <answer> = 
lapb_dbg(0, "(%p) <token> -> S4\n", lapb->dev); <answer> S3 
lapb->state <token> LAPB_STATE_4; <answer> = 
<token> = 0; <answer> lapb->n2count 
<token> LAPB_REJ: <answer> case 
lapb_dbg(1, <token> S3 RX REJ(%d) R%d\n", <answer> "(%p) 
lapb->dev, <token> frame->nr); <answer> frame->pf, 
<token> &= ~LAPB_PEER_RX_BUSY_CONDITION; <answer> lapb->condition 
<token> frame->cr, frame->pf); <answer> lapb_check_need_response(lapb, 
if (lapb_validate_nr(lapb, frame->nr)) <token> <answer> { 
<token> frame->nr); <answer> lapb_frames_acked(lapb, 
lapb->n2count = <token> <answer> 0; 
} <token> { <answer> else 
lapb->frmr_data = <token> <answer> *frame; 
<token> = LAPB_FRMR_Z; <answer> lapb->frmr_type 
lapb_dbg(0, "(%p) S3 <token> S4\n", lapb->dev); <answer> -> 
<token> = LAPB_STATE_4; <answer> lapb->state 
lapb->n2count <token> 0; <answer> = 
case <token> <answer> LAPB_I: 
lapb_dbg(1, "(%p) S3 RX <token> S%d R%d\n", <answer> I(%d) 
<token> frame->pf, frame->ns, frame->nr); <answer> lapb->dev, 
if (!lapb_validate_nr(lapb, <token> { <answer> frame->nr)) 
<token> = *frame; <answer> lapb->frmr_data 
<token> = LAPB_FRMR_Z; <answer> lapb->frmr_type 
lapb_dbg(0, "(%p) <token> -> S4\n", lapb->dev); <answer> S3 
lapb->state <token> LAPB_STATE_4; <answer> = 
lapb->n2count <token> 0; <answer> = 
if (lapb->condition & <token> <answer> LAPB_PEER_RX_BUSY_CONDITION) 
<token> frame->nr); <answer> lapb_frames_acked(lapb, 
lapb_check_iframes_acked(lapb, <token> <answer> frame->nr); 
if (frame->ns == <token> { <answer> lapb->vr) 
int <token> <answer> cn; 
cn = lapb_data_indication(lapb, <token> <answer> skb); 
<token> = 1; <answer> queued 
if <token> == NET_RX_DROP) { <answer> (cn 
<token> congestion\n"); <answer> pr_debug("rx 
lapb->vr = <token> + 1) % modulus; <answer> (lapb->vr 
lapb->condition &= <token> <answer> ~LAPB_REJECT_CONDITION; 
if <token> <answer> (frame->pf) 
else <token> <answer> { 
if (!(lapb->condition <token> <answer> & 
LAPB_ACK_PENDING_CONDITION)) <token> <answer> { 
<token> |= LAPB_ACK_PENDING_CONDITION; <answer> lapb->condition 
} else <token> <answer> { 
<token> (lapb->condition & LAPB_REJECT_CONDITION) { <answer> if 
<token> (frame->pf) <answer> if 
<token> else { <answer> } 
lapb_dbg(1, "(%p) S3 TX REJ(%d) <token> <answer> R%d\n", 
lapb->dev, frame->pf, <token> <answer> lapb->vr); 
lapb->condition |= <token> <answer> LAPB_REJECT_CONDITION; 
lapb_send_control(lapb, <token> frame->pf, <answer> LAPB_REJ, 
<token> &= ~LAPB_ACK_PENDING_CONDITION; <answer> lapb->condition 
case <token> <answer> LAPB_FRMR: 
lapb_dbg(1, "(%p) S3 <token> FRMR(%d) %5ph\n", <answer> RX 
<token> frame->pf, <answer> lapb->dev, 
lapb_dbg(0, "(%p) S3 -> S1\n", <token> <answer> lapb->dev); 
lapb->state <token> LAPB_STATE_1; <answer> = 
<token> LAPB_ILLEGAL: <answer> case 
lapb_dbg(1, <token> S3 RX ILLEGAL(%d)\n", lapb->dev, frame->pf); <answer> "(%p) 
<token> = *frame; <answer> lapb->frmr_data 
<token> = LAPB_FRMR_W; <answer> lapb->frmr_type 
lapb_dbg(0, "(%p) S3 -> S4\n", <token> <answer> lapb->dev); 
lapb->state = <token> <answer> LAPB_STATE_4; 
lapb->n2count = <token> <answer> 0; 
if <token> <answer> (!queued) 
static <token> lapb_state4_machine(struct lapb_cb *lapb, struct sk_buff *skb, <answer> void 
struct lapb_frame <token> <answer> *frame) 
switch (frame->type) <token> <answer> { 
<token> LAPB_SABM: <answer> case 
lapb_dbg(1, <token> S4 RX SABM(%d)\n", lapb->dev, frame->pf); <answer> "(%p) 
if <token> & LAPB_EXTENDED) { <answer> (lapb->mode 
<token> "(%p) S4 TX DM(%d)\n", <answer> lapb_dbg(1, 
lapb->dev, <token> <answer> frame->pf); 
<token> LAPB_DM, frame->pf, <answer> lapb_send_control(lapb, 
} else <token> <answer> { 
lapb_dbg(1, "(%p) <token> TX UA(%d)\n", <answer> S4 
lapb->dev, <token> <answer> frame->pf); 
lapb_dbg(0, "(%p) S4 -> <token> lapb->dev); <answer> S3\n", 
<token> LAPB_UA, frame->pf, <answer> lapb_send_control(lapb, 
lapb->state = <token> <answer> LAPB_STATE_3; 
lapb->condition = <token> <answer> 0x00; 
<token> = 0; <answer> lapb->n2count 
lapb->vs = <token> <answer> 0; 
lapb->vr = <token> <answer> 0; 
lapb->va = <token> <answer> 0; 
lapb_connect_indication(lapb, <token> <answer> LAPB_OK); 
case <token> <answer> LAPB_SABME: 
lapb_dbg(1, "(%p) S4 RX SABME(%d)\n", <token> frame->pf); <answer> lapb->dev, 
if (lapb->mode <token> LAPB_EXTENDED) { <answer> & 
<token> "(%p) S4 TX UA(%d)\n", <answer> lapb_dbg(1, 
lapb->dev, <token> <answer> frame->pf); 
lapb_dbg(0, "(%p) <token> -> S3\n", lapb->dev); <answer> S4 
lapb_send_control(lapb, LAPB_UA, <token> <answer> frame->pf, 
lapb->state <token> LAPB_STATE_3; <answer> = 
<token> = 0x00; <answer> lapb->condition 
<token> = 0; <answer> lapb->n2count 
lapb->vs = <token> <answer> 0; 
<token> = 0; <answer> lapb->vr 
<token> = 0; <answer> lapb->va 
lapb_connect_indication(lapb, <token> <answer> LAPB_OK); 
<token> else { <answer> } 
lapb_dbg(1, "(%p) <token> TX DM(%d)\n", <answer> S4 
<token> frame->pf); <answer> lapb->dev, 
lapb_send_control(lapb, LAPB_DM, <token> <answer> frame->pf, 
<token> lapb_data_input(struct lapb_cb *lapb, struct sk_buff *skb) <answer> void 
<token> lapb_frame frame; <answer> struct 
if (lapb_decode(lapb, skb, &frame) < 0) <token> <answer> { 
switch (lapb->state) <token> <answer> { 
<token> LAPB_STATE_0: <answer> case 
lapb_state0_machine(lapb, skb, <token> break; <answer> &frame); 
<token> LAPB_STATE_1: <answer> case 
<token> skb, &frame); break; <answer> lapb_state1_machine(lapb, 
case <token> <answer> LAPB_STATE_2: 
lapb_state2_machine(lapb, <token> &frame); break; <answer> skb, 
<token> LAPB_STATE_3: <answer> case 
lapb_state3_machine(lapb, skb, <token> break; <answer> &frame); 
<token> LAPB_STATE_4: <answer> case 
lapb_state4_machine(lapb, <token> &frame); break; <answer> skb, 
#include <token> <answer> <linux/fs.h> 
#include <token> <answer> <linux/capability.h> 
<token> <linux/time.h> <answer> #include 
<token> <linux/compat.h> <answer> #include 
<token> <linux/mount.h> <answer> #include 
#include <token> <answer> <linux/file.h> 
<token> <linux/quotaops.h> <answer> #include 
#include <token> <answer> <linux/random.h> 
#include <token> <answer> <linux/uaccess.h> 
<token> <linux/delay.h> <answer> #include 
<token> <linux/iversion.h> <answer> #include 
<token> <linux/fileattr.h> <answer> #include 
<token> <linux/uuid.h> <answer> #include 
<token> "ext4_jbd2.h" <answer> #include 
#include <token> <answer> "ext4.h" 
#include <token> <answer> <linux/fsmap.h> 
#include <token> <answer> "fsmap.h" 
<token> <trace/events/ext4.h> <answer> #include 
typedef void ext4_update_sb_callback(struct <token> *es, <answer> ext4_super_block 
<token> void *arg); <answer> const 
static void ext4_sb_setlabel(struct ext4_super_block *es, const <token> *arg) <answer> void 
<token> void ext4_sb_setuuid(struct ext4_super_block *es, const void *arg) <answer> static 
memcpy(es->s_uuid, (__u8 <token> UUID_SIZE); <answer> *)arg, 
int <token> super_block *sb, handle_t *handle, <answer> ext4_update_primary_sb(struct 
<token> func, <answer> ext4_update_sb_callback 
const <token> *arg) <answer> void 
int err <token> 0; <answer> = 
struct ext4_sb_info <token> = EXT4_SB(sb); <answer> *sbi 
struct buffer_head *bh = <token> <answer> sbi->s_sbh; 
struct ext4_super_block <token> = sbi->s_es; <answer> *es 
trace_ext4_update_sb(sb, bh->b_blocknr, <token> <answer> 1); 
<token> "get_write_access"); <answer> BUFFER_TRACE(bh, 
err <token> ext4_journal_get_write_access(handle, sb, <answer> = 
if <token> <answer> (err) 
<token> out_err; <answer> goto 
<token> arg); <answer> func(es, 
if <token> || !buffer_uptodate(bh)) { <answer> (buffer_write_io_error(bh) 
ext4_msg(sbi->s_sb, KERN_ERR, "previous I/O error <token> " <answer> to 
<token> detected"); <answer> "superblock 
err = ext4_handle_dirty_metadata(handle, <token> bh); <answer> NULL, 
<token> (err) <answer> if 
<token> out_err; <answer> goto 
err <token> sync_dirty_buffer(bh); <answer> = 
ext4_std_error(sb, <token> <answer> err); 
return <token> <answer> err; 
static <token> ext4_update_backup_sb(struct super_block *sb, <answer> int 
handle_t <token> ext4_group_t grp, <answer> *handle, 
ext4_update_sb_callback func, const void <token> <answer> *arg) 
int err = <token> <answer> 0; 
ext4_fsblk_t <token> <answer> sb_block; 
<token> buffer_head *bh; <answer> struct 
unsigned long offset <token> 0; <answer> = 
struct <token> *es; <answer> ext4_super_block 
<token> (!ext4_bg_has_super(sb, grp)) <answer> if 
<token> 0; <answer> return 
if (grp == 0) <token> <answer> { 
sb_block <token> 1 * EXT4_MIN_BLOCK_SIZE; <answer> = 
offset = do_div(sb_block, <token> <answer> sb->s_blocksize); 
} <token> { <answer> else 
<token> = ext4_group_first_block_no(sb, grp); <answer> sb_block 
offset <token> 0; <answer> = 
trace_ext4_update_sb(sb, sb_block, handle ? 1 : <token> <answer> 0); 
bh = ext4_sb_bread(sb, <token> 0); <answer> sb_block, 
if <token> <answer> (IS_ERR(bh)) 
<token> PTR_ERR(bh); <answer> return 
<token> (handle) { <answer> if 
BUFFER_TRACE(bh, <token> <answer> "get_write_access"); 
err = ext4_journal_get_write_access(handle, <token> <answer> sb, 
<token> (err) <answer> if 
<token> out_bh; <answer> goto 
es = (struct ext4_super_block <token> (bh->b_data + offset); <answer> *) 
if <token> && <answer> (ext4_has_metadata_csum(sb) 
es->s_checksum != ext4_superblock_csum(sb, es)) <token> <answer> { 
ext4_msg(sb, KERN_ERR, "Invalid checksum <token> backup " <answer> for 
"superblock %llu", <token> <answer> sb_block); 
goto <token> <answer> out_bh; 
func(es, <token> <answer> arg); 
<token> (ext4_has_metadata_csum(sb)) <answer> if 
es->s_checksum = <token> es); <answer> ext4_superblock_csum(sb, 
<token> (handle) { <answer> if 
err = <token> NULL, bh); <answer> ext4_handle_dirty_metadata(handle, 
if <token> <answer> (err) 
goto <token> <answer> out_bh; 
} else <token> <answer> { 
<token> "marking dirty"); <answer> BUFFER_TRACE(bh, 
err <token> sync_dirty_buffer(bh); <answer> = 
<token> err); <answer> ext4_std_error(sb, 
return (err) ? <token> : 1; <answer> err 
int ext4_update_superblocks_fn(struct super_block <token> <answer> *sb, 
<token> func, <answer> ext4_update_sb_callback 
<token> void *arg) <answer> const 
handle_t <token> <answer> *handle; 
<token> ngroups; <answer> ext4_group_t 
unsigned int three <token> 1; <answer> = 
unsigned <token> five = 5; <answer> int 
unsigned int seven = <token> <answer> 7; 
int <token> = 0, ret, i; <answer> err 
ext4_group_t grp, <token> <answer> primary_grp; 
struct ext4_sb_info <token> = EXT4_SB(sb); <answer> *sbi 
<token> (test_and_set_bit_lock(EXT4_FLAGS_RESIZING, <answer> if 
&sbi->s_ext4_flags)) <token> <answer> { 
<token> KERN_ERR, "Can't modify superblock while" <answer> ext4_msg(sb, 
"performing online <token> <answer> resize"); 
return <token> <answer> -EBUSY; 
handle = ext4_journal_start_sb(sb, <token> 3); <answer> EXT4_HT_MISC, 
if <token> { <answer> (IS_ERR(handle)) 
err = <token> <answer> PTR_ERR(handle); 
<token> out; <answer> goto 
i <token> 0; <answer> = 
grp <token> 0; <answer> = 
while (grp < ngroups) <token> <answer> { 
<token> = ext4_journal_stop(handle); <answer> err 
if <token> <answer> (err) 
<token> out; <answer> goto 
handle = <token> <answer> NULL; 
grp = ext4_list_backups(sb, <token> &five, &seven); <answer> &three, 
<token> (handle) { <answer> if 
ret = <token> <answer> ext4_journal_stop(handle); 
if (ret && <token> <answer> !err) 
err <token> ret; <answer> = 
clear_bit_unlock(EXT4_FLAGS_RESIZING, <token> <answer> &sbi->s_ext4_flags); 
return err ? err <token> 0; <answer> : 
static void memswap(void *a, <token> *b, size_t len) <answer> void 
unsigned char <token> *bp; <answer> *ap, 
ap = (unsigned <token> *)a; <answer> char 
bp <token> (unsigned char *)b; <answer> = 
<token> (len-- > 0) { <answer> while 
<token> *bp); <answer> swap(*ap, 
static void swap_inode_data(struct inode *inode1, struct inode <token> <answer> *inode2) 
loff_t <token> <answer> isize; 
<token> ext4_inode_info *ei1; <answer> struct 
struct ext4_inode_info <token> <answer> *ei2; 
unsigned <token> tmp; <answer> long 
<token> timespec64 ts1, ts2; <answer> struct 
ei1 = <token> <answer> EXT4_I(inode1); 
ei2 = <token> <answer> EXT4_I(inode2); 
swap(inode1->i_version, <token> <answer> inode2->i_version); 
<token> = inode_get_atime(inode1); <answer> ts1 
<token> = inode_get_atime(inode2); <answer> ts2 
<token> ts2); <answer> inode_set_atime_to_ts(inode1, 
inode_set_atime_to_ts(inode2, <token> <answer> ts1); 
ts1 = <token> <answer> inode_get_mtime(inode1); 
<token> = inode_get_mtime(inode2); <answer> ts2 
<token> ts2); <answer> inode_set_mtime_to_ts(inode1, 
<token> ts1); <answer> inode_set_mtime_to_ts(inode2, 
memswap(ei1->i_data, ei2->i_data, <token> <answer> sizeof(ei1->i_data)); 
<token> = ei1->i_flags & EXT4_FL_SHOULD_SWAP; <answer> tmp 
ei1->i_flags = (ei2->i_flags & EXT4_FL_SHOULD_SWAP) <token> <answer> | 
<token> & ~EXT4_FL_SHOULD_SWAP); <answer> (ei1->i_flags 
<token> = tmp | (ei2->i_flags & ~EXT4_FL_SHOULD_SWAP); <answer> ei2->i_flags 
<token> ei2->i_disksize); <answer> swap(ei1->i_disksize, 
ext4_es_remove_extent(inode1, <token> EXT_MAX_BLOCKS); <answer> 0, 
<token> 0, EXT_MAX_BLOCKS); <answer> ext4_es_remove_extent(inode2, 
isize <token> i_size_read(inode1); <answer> = 
i_size_write(inode1, <token> <answer> i_size_read(inode2)); 
<token> isize); <answer> i_size_write(inode2, 
void ext4_reset_inode_seed(struct <token> *inode) <answer> inode 
struct ext4_inode_info *ei = <token> <answer> EXT4_I(inode); 
struct <token> *sbi = EXT4_SB(inode->i_sb); <answer> ext4_sb_info 
__le32 inum = <token> <answer> cpu_to_le32(inode->i_ino); 
__le32 <token> = cpu_to_le32(inode->i_generation); <answer> gen 
<token> csum; <answer> __u32 
if <token> <answer> (!ext4_has_metadata_csum(inode->i_sb)) 
csum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&inum, <token> <answer> sizeof(inum)); 
ei->i_csum_seed = ext4_chksum(sbi, csum, <token> *)&gen, sizeof(gen)); <answer> (__u8 
static long swap_inode_boot_loader(struct super_block <token> <answer> *sb, 
<token> mnt_idmap *idmap, <answer> struct 
<token> inode *inode) <answer> struct 
handle_t <token> <answer> *handle; 
<token> err; <answer> int 
struct <token> *inode_bl; <answer> inode 
struct ext4_inode_info <token> <answer> *ei_bl; 
qsize_t <token> size_bl, diff; <answer> size, 
<token> blocks; <answer> blkcnt_t 
unsigned short <token> <answer> bytes; 
inode_bl = ext4_iget(sb, <token> <answer> EXT4_BOOT_LOADER_INO, 
EXT4_IGET_SPECIAL <token> EXT4_IGET_BAD); <answer> | 
if <token> <answer> (IS_ERR(inode_bl)) 
<token> PTR_ERR(inode_bl); <answer> return 
ei_bl <token> EXT4_I(inode_bl); <answer> = 
lock_two_nondirectories(inode, <token> <answer> inode_bl); 
if <token> != 1 || !S_ISREG(inode->i_mode) || <answer> (inode->i_nlink 
IS_SWAPFILE(inode) || IS_ENCRYPTED(inode) <token> <answer> || 
(EXT4_I(inode)->i_flags & <token> || <answer> EXT4_JOURNAL_DATA_FL) 
ext4_has_inline_data(inode)) <token> <answer> { 
err = <token> <answer> -EINVAL; 
goto <token> <answer> journal_err_out; 
if (IS_RDONLY(inode) || IS_APPEND(inode) <token> IS_IMMUTABLE(inode) || <answer> || 
<token> inode) || <answer> !inode_owner_or_capable(idmap, 
<token> { <answer> !capable(CAP_SYS_ADMIN)) 
err <token> -EPERM; <answer> = 
goto <token> <answer> journal_err_out; 
err <token> filemap_write_and_wait(inode->i_mapping); <answer> = 
if <token> <answer> (err) 
goto <token> <answer> err_out; 
err <token> filemap_write_and_wait(inode_bl->i_mapping); <answer> = 
<token> (err) <answer> if 
goto <token> <answer> err_out; 
<token> int ext4_ioctl_check_immutable(struct inode *inode, __u32 new_projid, <answer> static 
unsigned <token> flags) <answer> int 
struct ext4_inode_info *ei <token> EXT4_I(inode); <answer> = 
unsigned <token> oldflags = ei->i_flags; <answer> int 
if (!(oldflags & EXT4_IMMUTABLE_FL) || <token> & EXT4_IMMUTABLE_FL)) <answer> !(flags 
<token> 0; <answer> return 
if ((oldflags & ~EXT4_IMMUTABLE_FL) != <token> & ~EXT4_IMMUTABLE_FL)) <answer> (flags 
<token> -EPERM; <answer> return 
if (ext4_has_feature_project(inode->i_sb) <token> <answer> && 
__kprojid_val(ei->i_projid) != <token> <answer> new_projid) 
return <token> <answer> -EPERM; 
return <token> <answer> 0; 
static void ext4_dax_dontcache(struct <token> *inode, unsigned int flags) <answer> inode 
struct ext4_inode_info *ei <token> EXT4_I(inode); <answer> = 
if <token> <answer> (S_ISDIR(inode->i_mode)) 
if (test_opt2(inode->i_sb, DAX_NEVER) <token> <answer> || 
<token> DAX_ALWAYS)) <answer> test_opt(inode->i_sb, 
if ((ei->i_flags <token> flags) & EXT4_DAX_FL) <answer> ^ 
static bool dax_compatible(struct inode *inode, unsigned int <token> <answer> oldflags, 
unsigned int <token> <answer> flags) 
if ((flags ^ oldflags) & (EXT4_JOURNAL_DATA_FL)) <token> <answer> { 
if <token> <answer> (!capable(CAP_SYS_RESOURCE)) 
<token> flags_out; <answer> goto 
if <token> oldflags, flags)) { <answer> (!dax_compatible(inode, 
<token> = -EOPNOTSUPP; <answer> err 
<token> flags_out; <answer> goto 
if ((flags ^ oldflags) & <token> <answer> EXT4_EXTENTS_FL) 
migrate <token> 1; <answer> = 
if ((flags <token> oldflags) & EXT4_CASEFOLD_FL) { <answer> ^ 
if (!ext4_has_feature_casefold(sb)) <token> <answer> { 
err <token> -EOPNOTSUPP; <answer> = 
goto <token> <answer> flags_out; 
<token> (!S_ISDIR(inode->i_mode)) { <answer> if 
err <token> -ENOTDIR; <answer> = 
goto <token> <answer> flags_out; 
<token> (!ext4_empty_dir(inode)) { <answer> if 
err <token> -ENOTEMPTY; <answer> = 
<token> flags_out; <answer> goto 
<token> (S_ISREG(inode->i_mode) && !IS_IMMUTABLE(inode) && <answer> if 
(flags & <token> { <answer> EXT4_IMMUTABLE_FL)) 
<token> = filemap_write_and_wait(inode->i_mapping); <answer> err 
if <token> <answer> (err) 
<token> flags_out; <answer> goto 
handle = <token> EXT4_HT_INODE, 1); <answer> ext4_journal_start(inode, 
<token> (IS_ERR(handle)) { <answer> if 
err = <token> <answer> PTR_ERR(handle); 
goto <token> <answer> flags_out; 
if <token> <answer> (IS_SYNC(inode)) 
err = ext4_reserve_inode_write(handle, inode, <token> <answer> &iloc); 
<token> (err) <answer> if 
<token> flags_err; <answer> goto 
ext4_dax_dontcache(inode, <token> <answer> flags); 
for (i = 0, mask = 1; i < 32; i++, <token> <<= 1) { <answer> mask 
if (!(mask <token> EXT4_FL_USER_MODIFIABLE)) <answer> & 
if <token> { <answer> (IS_DAX(inode)) 
err = <token> <answer> -EBUSY; 
<token> flags_out; <answer> goto 
<token> = ext4_change_inode_journal_flag(inode, <answer> err 
flags & <token> <answer> EXT4_JOURNAL_DATA_FL); 
<token> (err) <answer> if 
<token> flags_out; <answer> goto 
if <token> { <answer> (migrate) 
<token> (flags & EXT4_EXTENTS_FL) <answer> if 
err <token> ext4_ext_migrate(inode); <answer> = 
err <token> ext4_ind_migrate(inode); <answer> = 
<token> err; <answer> return 
<token> CONFIG_QUOTA <answer> #ifdef 
<token> int ext4_ioctl_setproject(struct inode *inode, __u32 projid) <answer> static 
<token> super_block *sb = inode->i_sb; <answer> struct 
struct ext4_inode_info *ei <token> EXT4_I(inode); <answer> = 
int <token> rc; <answer> err, 
<token> *handle; <answer> handle_t 
<token> kprojid; <answer> kprojid_t 
struct ext4_iloc <token> <answer> iloc; 
struct ext4_inode <token> <answer> *raw_inode; 
struct dquot *transfer_to[MAXQUOTAS] <token> { }; <answer> = 
if <token> { <answer> (!ext4_has_feature_project(sb)) 
<token> (projid != EXT4_DEF_PROJID) <answer> if 
<token> -EOPNOTSUPP; <answer> return 
<token> 0; <answer> return 
if (EXT4_INODE_SIZE(sb) <= <token> <answer> EXT4_GOOD_OLD_INODE_SIZE) 
return <token> <answer> -EOPNOTSUPP; 
kprojid <token> make_kprojid(&init_user_ns, (projid_t)projid); <answer> = 
if <token> EXT4_I(inode)->i_projid)) <answer> (projid_eq(kprojid, 
return <token> <answer> 0; 
err = <token> <answer> -EPERM; 
err <token> __dquot_transfer(inode, transfer_to); <answer> = 
<token> (err) <answer> if 
<token> out_dirty; <answer> goto 
<token> = kprojid; <answer> EXT4_I(inode)->i_projid 
<token> = ext4_mark_iloc_dirty(handle, inode, &iloc); <answer> rc 
if <token> <answer> (!err) 
err <token> rc; <answer> = 
return <token> <answer> err; 
static int ext4_ioctl_setproject(struct inode <token> __u32 projid) <answer> *inode, 
<token> (projid != EXT4_DEF_PROJID) <answer> if 
<token> -EOPNOTSUPP; <answer> return 
<token> 0; <answer> return 
int ext4_force_shutdown(struct <token> *sb, u32 flags) <answer> super_block 
struct <token> *sbi = EXT4_SB(sb); <answer> ext4_sb_info 
<token> ret; <answer> int 
if <token> > EXT4_GOING_FLAGS_NOLOGFLUSH) <answer> (flags 
return <token> <answer> -EINVAL; 
<token> (ext4_forced_shutdown(sb)) <answer> if 
<token> 0; <answer> return 
ext4_msg(sb, KERN_ALERT, "shut down requested <token> flags); <answer> (%d)", 
trace_ext4_shutdown(sb, <token> <answer> flags); 
<token> (flags) { <answer> switch 
case <token> <answer> EXT4_GOING_FLAGS_DEFAULT: 
ret = <token> <answer> bdev_freeze(sb->s_bdev); 
if <token> <answer> (ret) 
<token> ret; <answer> return 
set_bit(EXT4_FLAGS_SHUTDOWN, <token> <answer> &sbi->s_ext4_flags); 
case <token> <answer> EXT4_GOING_FLAGS_LOGFLUSH: 
set_bit(EXT4_FLAGS_SHUTDOWN, <token> <answer> &sbi->s_ext4_flags); 
<token> (sbi->s_journal && !is_journal_aborted(sbi->s_journal)) { <answer> if 
<token> ext4_force_commit(sb); <answer> (void) 
jbd2_journal_abort(sbi->s_journal, <token> <answer> -ESHUTDOWN); 
case <token> <answer> EXT4_GOING_FLAGS_NOLOGFLUSH: 
<token> &sbi->s_ext4_flags); <answer> set_bit(EXT4_FLAGS_SHUTDOWN, 
<token> (sbi->s_journal && !is_journal_aborted(sbi->s_journal)) <answer> if 
<token> -ESHUTDOWN); <answer> jbd2_journal_abort(sbi->s_journal, 
<token> -EINVAL; <answer> return 
clear_opt(sb, <token> <answer> DISCARD); 
<token> 0; <answer> return 
static int <token> super_block *sb, unsigned long arg) <answer> ext4_ioctl_shutdown(struct 
u32 <token> <answer> flags; 
<token> (!capable(CAP_SYS_ADMIN)) <answer> if 
<token> -EPERM; <answer> return 
if (get_user(flags, (__u32 __user <token> <answer> *)arg)) 
return <token> <answer> -EFAULT; 
return ext4_force_shutdown(sb, <token> <answer> flags); 
struct <token> { <answer> getfsmap_info 
<token> super_block *gi_sb; <answer> struct 
struct fsmap_head __user <token> <answer> *gi_data; 
unsigned <token> gi_idx; <answer> int 
<token> gi_last_flags; <answer> __u32 
static int ext4_getfsmap_format(struct ext4_fsmap *xfm, void <token> <answer> *priv) 
struct getfsmap_info *info <token> priv; <answer> = 
<token> fsmap fm; <answer> struct 
<token> xfm); <answer> trace_ext4_getfsmap_mapping(info->gi_sb, 
info->gi_last_flags <token> xfm->fmr_flags; <answer> = 
<token> &fm, xfm); <answer> ext4_fsmap_from_internal(info->gi_sb, 
if (copy_to_user(&info->gi_data->fmh_recs[info->gi_idx++], <token> <answer> &fm, 
<token> fsmap))) <answer> sizeof(struct 
<token> -EFAULT; <answer> return 
return <token> <answer> 0; 
static <token> ext4_ioc_getfsmap(struct super_block *sb, <answer> int 
struct fsmap_head <token> *arg) <answer> __user 
struct getfsmap_info info = { <token> }; <answer> NULL 
struct <token> xhead = {0}; <answer> ext4_fsmap_head 
struct <token> head; <answer> fsmap_head 
<token> aborted = false; <answer> bool 
<token> error; <answer> int 
if <token> arg, sizeof(struct fsmap_head))) <answer> (copy_from_user(&head, 
<token> -EFAULT; <answer> return 
if (memchr_inv(head.fmh_reserved, 0, sizeof(head.fmh_reserved)) <token> <answer> || 
<token> 0, <answer> memchr_inv(head.fmh_keys[0].fmr_reserved, 
<token> || <answer> sizeof(head.fmh_keys[0].fmr_reserved)) 
memchr_inv(head.fmh_keys[1].fmr_reserved, <token> <answer> 0, 
<token> -EINVAL; <answer> return 
if (head.fmh_keys[0].fmr_offset <token> <answer> || 
<token> != 0 && <answer> (head.fmh_keys[1].fmr_offset 
head.fmh_keys[1].fmr_offset != <token> <answer> -1ULL)) 
<token> -EINVAL; <answer> return 
xhead.fmh_iflags = <token> <answer> head.fmh_iflags; 
xhead.fmh_count <token> head.fmh_count; <answer> = 
ext4_fsmap_to_internal(sb, &xhead.fmh_keys[0], <token> <answer> &head.fmh_keys[0]); 
ext4_fsmap_to_internal(sb, &xhead.fmh_keys[1], <token> <answer> &head.fmh_keys[1]); 
<token> &xhead.fmh_keys[0]); <answer> trace_ext4_getfsmap_low_key(sb, 
<token> &xhead.fmh_keys[1]); <answer> trace_ext4_getfsmap_high_key(sb, 
<token> = sb; <answer> info.gi_sb 
info.gi_data = <token> <answer> arg; 
error = <token> &xhead, ext4_getfsmap_format, &info); <answer> ext4_getfsmap(sb, 
if (error <token> EXT4_QUERY_RANGE_ABORT) <answer> == 
aborted <token> true; <answer> = 
else if <token> <answer> (error) 
return <token> <answer> error; 
<token> &= EXT4_FL_USER_MODIFIABLE; <answer> flags 
<token> (ext4_mask_flags(inode->i_mode, flags) != flags) <answer> if 
<token> out; <answer> goto 
err = ext4_ioctl_check_immutable(inode, <token> flags); <answer> fa->fsx_projid, 
if <token> <answer> (err) 
<token> out; <answer> goto 
err = <token> flags); <answer> ext4_ioctl_setflags(inode, 
if <token> <answer> (err) 
goto <token> <answer> out; 
err <token> ext4_ioctl_setproject(inode, fa->fsx_projid); <answer> = 
<token> err; <answer> return 
if (copy_from_user(new_label, user_label, EXT4_LABEL_MAX <token> 1)) <answer> + 
<token> -EFAULT; <answer> return 
len = <token> EXT4_LABEL_MAX + 1); <answer> strnlen(new_label, 
<token> (len > EXT4_LABEL_MAX) <answer> if 
return <token> <answer> -EINVAL; 
<token> + len, 0, EXT4_LABEL_MAX - len); <answer> memset(new_label 
ret = <token> <answer> mnt_want_write_file(filp); 
if <token> <answer> (ret) 
<token> ret; <answer> return 
ret = ext4_update_superblocks_fn(sb, <token> new_label); <answer> ext4_sb_setlabel, 
return <token> <answer> ret; 
static int ext4_ioctl_getlabel(struct <token> *sbi, char __user *user_label) <answer> ext4_sb_info 
<token> label[EXT4_LABEL_MAX + 1]; <answer> char 
BUILD_BUG_ON(EXT4_LABEL_MAX <token> FSLABEL_MAX); <answer> >= 
<token> 0, sizeof(label)); <answer> memset(label, 
strncpy(label, sbi->s_es->s_volume_name, <token> <answer> EXT4_LABEL_MAX); 
if (copy_to_user(user_label, label, <token> <answer> sizeof(label))) 
return <token> <answer> -EFAULT; 
return <token> <answer> 0; 
static <token> ext4_ioctl_getuuid(struct ext4_sb_info *sbi, <answer> int 
struct fsuuid __user <token> <answer> *ufsuuid) 
struct fsuuid <token> <answer> fsuuid; 
__u8 <token> <answer> uuid[UUID_SIZE]; 
if (copy_from_user(&fsuuid, <token> sizeof(fsuuid))) <answer> ufsuuid, 
<token> -EFAULT; <answer> return 
if <token> == 0) { <answer> (fsuuid.fsu_len 
fsuuid.fsu_len <token> UUID_SIZE; <answer> = 
if (copy_to_user(&ufsuuid->fsu_len, <token> <answer> &fsuuid.fsu_len, 
<token> -EFAULT; <answer> return 
<token> 0; <answer> return 
if (fsuuid.fsu_len <token> UUID_SIZE || fsuuid.fsu_flags != 0) <answer> < 
return <token> <answer> -EINVAL; 
<token> sbi->s_es->s_uuid, UUID_SIZE); <answer> memcpy(uuid, 
fsuuid.fsu_len = <token> <answer> UUID_SIZE; 
<token> (copy_to_user(ufsuuid, &fsuuid, sizeof(fsuuid)) || <answer> if 
copy_to_user(&ufsuuid->fsu_uuid[0], <token> UUID_SIZE)) <answer> uuid, 
return <token> <answer> -EFAULT; 
<token> 0; <answer> return 
<token> int ext4_ioctl_setuuid(struct file *filp, <answer> static 
const struct fsuuid <token> *ufsuuid) <answer> __user 
int ret <token> 0; <answer> = 
struct super_block *sb <token> file_inode(filp)->i_sb; <answer> = 
<token> fsuuid fsuuid; <answer> struct 
__u8 <token> <answer> uuid[UUID_SIZE]; 
<token> (!capable(CAP_SYS_ADMIN)) <answer> if 
<token> -EPERM; <answer> return 
<token> (((ext4_has_feature_gdt_csum(sb) || ext4_has_metadata_csum(sb)) <answer> if 
&& <token> <answer> !ext4_has_feature_csum_seed(sb)) 
|| <token> <answer> ext4_has_feature_stable_inodes(sb)) 
<token> -EOPNOTSUPP; <answer> return 
if (copy_from_user(&fsuuid, ufsuuid, <token> <answer> sizeof(fsuuid))) 
return <token> <answer> -EFAULT; 
if <token> != UUID_SIZE || fsuuid.fsu_flags != 0) <answer> (fsuuid.fsu_len 
<token> -EINVAL; <answer> return 
if (copy_from_user(uuid, &ufsuuid->fsu_uuid[0], <token> <answer> UUID_SIZE)) 
return <token> <answer> -EFAULT; 
<token> = mnt_want_write_file(filp); <answer> ret 
<token> (ret) <answer> if 
return <token> <answer> ret; 
ret = ext4_update_superblocks_fn(sb, <token> &uuid); <answer> ext4_sb_setuuid, 
return <token> <answer> ret; 
<token> long __ext4_ioctl(struct file *filp, unsigned int cmd, unsigned long arg) <answer> static 
<token> inode *inode = file_inode(filp); <answer> struct 
struct <token> *sb = inode->i_sb; <answer> super_block 
struct mnt_idmap *idmap = <token> <answer> file_mnt_idmap(filp); 
ext4_debug("cmd <token> %u, arg = %lu\n", cmd, arg); <answer> = 
<token> (cmd) { <answer> switch 
<token> FS_IOC_GETFSMAP: <answer> case 
return <token> (void __user *)arg); <answer> ext4_ioc_getfsmap(sb, 
<token> EXT4_IOC_GETVERSION: <answer> case 
case <token> <answer> EXT4_IOC_GETVERSION_OLD: 
return <token> (int __user *) arg); <answer> put_user(inode->i_generation, 
<token> EXT4_IOC_SETVERSION: <answer> case 
case EXT4_IOC_SETVERSION_OLD: <token> <answer> { 
<token> *handle; <answer> handle_t 
struct ext4_iloc <token> <answer> iloc; 
<token> generation; <answer> __u32 
int <token> <answer> err; 
<token> (!inode_owner_or_capable(idmap, inode)) <answer> if 
return <token> <answer> -EPERM; 
<token> (ext4_has_metadata_csum(inode->i_sb)) { <answer> if 
ext4_warning(sb, "Setting <token> version is not " <answer> inode 
<token> with metadata_csum enabled."); <answer> "supported 
<token> -ENOTTY; <answer> return 
err = <token> <answer> mnt_want_write_file(filp); 
if <token> <answer> (err) 
<token> err; <answer> return 
if (get_user(generation, (int <token> *) arg)) { <answer> __user 
<token> = -EFAULT; <answer> err 
<token> setversion_out; <answer> goto 
handle = <token> EXT4_HT_INODE, 1); <answer> ext4_journal_start(inode, 
if (IS_ERR(handle)) <token> <answer> { 
err <token> PTR_ERR(handle); <answer> = 
<token> unlock_out; <answer> goto 
err = ext4_reserve_inode_write(handle, <token> &iloc); <answer> inode, 
if (err == 0) <token> <answer> { 
inode->i_generation <token> generation; <answer> = 
<token> = ext4_mark_iloc_dirty(handle, inode, &iloc); <answer> err 
<token> err; <answer> return 
<token> EXT4_IOC_GROUP_EXTEND: { <answer> case 
ext4_fsblk_t <token> <answer> n_blocks_count; 
int <token> err2=0; <answer> err, 
err = <token> <answer> ext4_resize_begin(sb); 
<token> (err) <answer> if 
<token> err; <answer> return 
if <token> (__u32 __user *)arg)) { <answer> (get_user(n_blocks_count, 
err = <token> <answer> -EFAULT; 
goto <token> <answer> group_extend_out; 
<token> (ext4_has_feature_bigalloc(sb)) { <answer> if 
<token> KERN_ERR, <answer> ext4_msg(sb, 
"Online resizing not <token> with bigalloc"); <answer> supported 
<token> = -EOPNOTSUPP; <answer> err 
goto <token> <answer> group_extend_out; 
err <token> mnt_want_write_file(filp); <answer> = 
<token> (err) <answer> if 
<token> group_extend_out; <answer> goto 
<token> = ext4_group_extend(sb, EXT4_SB(sb)->s_es, n_blocks_count); <answer> err 
<token> (EXT4_SB(sb)->s_journal) { <answer> if 
err2 <token> jbd2_journal_flush(EXT4_SB(sb)->s_journal, 0); <answer> = 
if (err == <token> <answer> 0) 
<token> = err2; <answer> err 
err2 = ext4_resize_end(sb, <token> <answer> false); 
if (err == <token> <answer> 0) 
err = <token> <answer> err2; 
<token> err; <answer> return 
case <token> { <answer> EXT4_IOC_MOVE_EXT: 
struct <token> me; <answer> move_extent 
struct fd <token> <answer> donor; 
<token> err; <answer> int 
if <token> & FMODE_READ) || <answer> (!(filp->f_mode 
<token> & FMODE_WRITE)) <answer> !(filp->f_mode 
return <token> <answer> -EBADF; 
<token> (copy_from_user(&me, <answer> if 
(struct move_extent __user *)arg, <token> <answer> sizeof(me))) 
<token> -EFAULT; <answer> return 
me.moved_len <token> 0; <answer> = 
donor <token> fdget(me.donor_fd); <answer> = 
<token> (!donor.file) <answer> if 
<token> -EBADF; <answer> return 
if (!(donor.file->f_mode & FMODE_WRITE)) <token> <answer> { 
<token> = -EBADF; <answer> err 
<token> mext_out; <answer> goto 
if (ext4_has_feature_bigalloc(sb)) <token> <answer> { 
ext4_msg(sb, <token> <answer> KERN_ERR, 
"Online defrag not <token> with bigalloc"); <answer> supported 
err <token> -EOPNOTSUPP; <answer> = 
<token> mext_out; <answer> goto 
} <token> if (IS_DAX(inode)) { <answer> else 
ext4_msg(sb, <token> <answer> KERN_ERR, 
"Online defrag not supported <token> DAX"); <answer> with 
err <token> -EOPNOTSUPP; <answer> = 
goto <token> <answer> mext_out; 
err <token> mnt_want_write_file(filp); <answer> = 
<token> (err) <answer> if 
goto <token> <answer> mext_out; 
err = ext4_move_extents(filp, <token> me.orig_start, <answer> donor.file, 
me.donor_start, <token> &me.moved_len); <answer> me.len, 
if (copy_to_user((struct <token> __user *)arg, <answer> move_extent 
&me, <token> <answer> sizeof(me))) 
<token> = -EFAULT; <answer> err 
<token> err; <answer> return 
case EXT4_IOC_GROUP_ADD: <token> <answer> { 
struct <token> input; <answer> ext4_new_group_data 
if <token> (struct ext4_new_group_input __user *)arg, <answer> (copy_from_user(&input, 
return <token> <answer> -EFAULT; 
return <token> &input); <answer> ext4_ioctl_group_add(filp, 
<token> EXT4_IOC_MIGRATE: <answer> case 
<token> err; <answer> int 
<token> (!inode_owner_or_capable(idmap, inode)) <answer> if 
<token> -EACCES; <answer> return 
err <token> mnt_want_write_file(filp); <answer> = 
<token> (err) <answer> if 
return <token> <answer> err; 
err = <token> <answer> ext4_ext_migrate(inode); 
<token> err; <answer> return 
case <token> <answer> EXT4_IOC_ALLOC_DA_BLKS: 
int <token> <answer> err; 
if (!inode_owner_or_capable(idmap, <token> <answer> inode)) 
<token> -EACCES; <answer> return 
err <token> mnt_want_write_file(filp); <answer> = 
<token> (err) <answer> if 
return <token> <answer> err; 
<token> = ext4_alloc_da_blocks(inode); <answer> err 
<token> err; <answer> return 
<token> EXT4_IOC_SWAP_BOOT: <answer> case 
int <token> <answer> err; 
<token> (!(filp->f_mode & FMODE_WRITE)) <answer> if 
return <token> <answer> -EBADF; 
err <token> mnt_want_write_file(filp); <answer> = 
<token> (err) <answer> if 
<token> err; <answer> return 
err = swap_inode_boot_loader(sb, <token> inode); <answer> idmap, 
return <token> <answer> err; 
case EXT4_IOC_RESIZE_FS: <token> <answer> { 
<token> n_blocks_count; <answer> ext4_fsblk_t 
int err <token> 0, err2 = 0; <answer> = 
ext4_group_t o_group = <token> <answer> EXT4_SB(sb)->s_groups_count; 
if <token> (__u64 __user *)arg, <answer> (copy_from_user(&n_blocks_count, 
sizeof(__u64))) <token> <answer> { 
<token> -EFAULT; <answer> return 
err = <token> <answer> ext4_resize_begin(sb); 
<token> (err) <answer> if 
<token> err; <answer> return 
err = <token> <answer> mnt_want_write_file(filp); 
<token> (err) <answer> if 
<token> resizefs_out; <answer> goto 
err = ext4_resize_fs(sb, <token> <answer> n_blocks_count); 
if (EXT4_SB(sb)->s_journal) <token> <answer> { 
ext4_fc_mark_ineligible(sb, EXT4_FC_REASON_RESIZE, <token> <answer> NULL); 
err2 <token> jbd2_journal_flush(EXT4_SB(sb)->s_journal, 0); <answer> = 
if <token> == 0) <answer> (err 
err = <token> <answer> err2; 
if (!err && (o_group < <token> && <answer> EXT4_SB(sb)->s_groups_count) 
<token> && <answer> ext4_has_group_desc_csum(sb) 
<token> INIT_INODE_TABLE)) <answer> test_opt(sb, 
err = <token> o_group); <answer> ext4_register_li_request(sb, 
err2 = <token> true); <answer> ext4_resize_end(sb, 
if (err <token> 0) <answer> == 
err = <token> <answer> err2; 
<token> err; <answer> return 
<token> FITRIM: <answer> case 
struct fstrim_range <token> <answer> range; 
<token> ret = 0; <answer> int 
if <token> <answer> (!capable(CAP_SYS_ADMIN)) 
return <token> <answer> -EPERM; 
<token> (!bdev_max_discard_sectors(sb->s_bdev)) <answer> if 
return <token> <answer> -EOPNOTSUPP; 
if <token> NOLOAD) && ext4_has_feature_journal(sb)) <answer> (test_opt(sb, 
return <token> <answer> -EROFS; 
if (copy_from_user(&range, (struct fstrim_range <token> *)arg, <answer> __user 
return <token> <answer> -EFAULT; 
<token> = ext4_trim_fs(sb, &range); <answer> ret 
<token> (ret < 0) <answer> if 
return <token> <answer> ret; 
if (copy_to_user((struct fstrim_range <token> *)arg, &range, <answer> __user 
<token> -EFAULT; <answer> return 
<token> 0; <answer> return 
<token> EXT4_IOC_PRECACHE_EXTENTS: <answer> case 
<token> ext4_ext_precache(inode); <answer> return 
case <token> <answer> FS_IOC_SET_ENCRYPTION_POLICY: 
if <token> <answer> (!ext4_has_feature_encrypt(sb)) 
<token> -EOPNOTSUPP; <answer> return 
return <token> (const void __user *)arg); <answer> fscrypt_ioctl_set_policy(filp, 
case <token> <answer> FS_IOC_GET_ENCRYPTION_PWSALT: 
return ext4_ioctl_get_encryption_pwsalt(filp, <token> __user *)arg); <answer> (void 
case <token> <answer> FS_IOC_GET_ENCRYPTION_POLICY: 
if <token> <answer> (!ext4_has_feature_encrypt(sb)) 
return <token> <answer> -EOPNOTSUPP; 
return fscrypt_ioctl_get_policy(filp, (void <token> *)arg); <answer> __user 
<token> FS_IOC_GET_ENCRYPTION_POLICY_EX: <answer> case 
<token> (!ext4_has_feature_encrypt(sb)) <answer> if 
<token> -EOPNOTSUPP; <answer> return 
return <token> (void __user *)arg); <answer> fscrypt_ioctl_get_policy_ex(filp, 
case <token> <answer> FS_IOC_ADD_ENCRYPTION_KEY: 
if <token> <answer> (!ext4_has_feature_encrypt(sb)) 
return <token> <answer> -EOPNOTSUPP; 
return <token> (void __user *)arg); <answer> fscrypt_ioctl_add_key(filp, 
case <token> <answer> FS_IOC_REMOVE_ENCRYPTION_KEY: 
<token> (!ext4_has_feature_encrypt(sb)) <answer> if 
<token> -EOPNOTSUPP; <answer> return 
<token> fscrypt_ioctl_remove_key(filp, (void __user *)arg); <answer> return 
case <token> <answer> FS_IOC_REMOVE_ENCRYPTION_KEY_ALL_USERS: 
<token> (!ext4_has_feature_encrypt(sb)) <answer> if 
return <token> <answer> -EOPNOTSUPP; 
<token> fscrypt_ioctl_remove_key_all_users(filp, <answer> return 
<token> __user *)arg); <answer> (void 
<token> FS_IOC_GET_ENCRYPTION_KEY_STATUS: <answer> case 
if <token> <answer> (!ext4_has_feature_encrypt(sb)) 
<token> -EOPNOTSUPP; <answer> return 
return fscrypt_ioctl_get_key_status(filp, (void __user <token> <answer> *)arg); 
<token> FS_IOC_GET_ENCRYPTION_NONCE: <answer> case 
<token> (!ext4_has_feature_encrypt(sb)) <answer> if 
return <token> <answer> -EOPNOTSUPP; 
return fscrypt_ioctl_get_nonce(filp, <token> __user *)arg); <answer> (void 
<token> EXT4_IOC_CLEAR_ES_CACHE: <answer> case 
<token> (!inode_owner_or_capable(idmap, inode)) <answer> if 
<token> -EACCES; <answer> return 
return <token> <answer> 0; 
<token> EXT4_IOC_GETSTATE: <answer> case 
__u32 <token> = 0; <answer> state 
if <token> EXT4_STATE_EXT_PRECACHED)) <answer> (ext4_test_inode_state(inode, 
state |= <token> <answer> EXT4_STATE_FLAG_EXT_PRECACHED; 
if <token> EXT4_STATE_NEW)) <answer> (ext4_test_inode_state(inode, 
state |= <token> <answer> EXT4_STATE_FLAG_NEW; 
if (ext4_test_inode_state(inode, <token> <answer> EXT4_STATE_NEWENTRY)) 
<token> |= EXT4_STATE_FLAG_NEWENTRY; <answer> state 
if <token> EXT4_STATE_DA_ALLOC_CLOSE)) <answer> (ext4_test_inode_state(inode, 
<token> |= EXT4_STATE_FLAG_DA_ALLOC_CLOSE; <answer> state 
return <token> (__u32 __user *) arg); <answer> put_user(state, 
<token> EXT4_IOC_GET_ES_CACHE: <answer> case 
return ext4_ioctl_get_es_cache(filp, <token> <answer> arg); 
<token> EXT4_IOC_SHUTDOWN: <answer> case 
return <token> arg); <answer> ext4_ioctl_shutdown(sb, 
case <token> <answer> FS_IOC_ENABLE_VERITY: 
if <token> <answer> (!ext4_has_feature_verity(sb)) 
return <token> <answer> -EOPNOTSUPP; 
<token> fsverity_ioctl_enable(filp, (const void __user *)arg); <answer> return 
case <token> <answer> FS_IOC_MEASURE_VERITY: 
if <token> <answer> (!ext4_has_feature_verity(sb)) 
<token> -EOPNOTSUPP; <answer> return 
return <token> (void __user *)arg); <answer> fsverity_ioctl_measure(filp, 
case <token> <answer> FS_IOC_READ_VERITY_METADATA: 
<token> (!ext4_has_feature_verity(sb)) <answer> if 
return <token> <answer> -EOPNOTSUPP; 
<token> fsverity_ioctl_read_metadata(filp, <answer> return 
(const void __user <token> <answer> *)arg); 
case <token> <answer> EXT4_IOC_CHECKPOINT: 
return ext4_ioctl_checkpoint(filp, <token> <answer> arg); 
case <token> <answer> FS_IOC_GETFSLABEL: 
return ext4_ioctl_getlabel(EXT4_SB(sb), <token> __user *)arg); <answer> (void 
case <token> <answer> FS_IOC_SETFSLABEL: 
<token> ext4_ioctl_setlabel(filp, <answer> return 
<token> void __user *)arg); <answer> (const 
<token> EXT4_IOC_GETFSUUID: <answer> case 
return ext4_ioctl_getuuid(EXT4_SB(sb), (void <token> *)arg); <answer> __user 
<token> EXT4_IOC_SETFSUUID: <answer> case 
<token> ext4_ioctl_setuuid(filp, (const void __user *)arg); <answer> return 
<token> -ENOTTY; <answer> return 
long <token> file *filp, unsigned int cmd, unsigned long arg) <answer> ext4_ioctl(struct 
<token> __ext4_ioctl(filp, cmd, arg); <answer> return 
<token> CONFIG_COMPAT <answer> #ifdef 
long ext4_compat_ioctl(struct file *file, unsigned <token> cmd, unsigned long arg) <answer> int 
<token> <linux/clk.h> <answer> #include 
#include <token> <answer> <linux/regmap.h> 
#include <token> <answer> <video/videomode.h> 
<token> <drm/drm_atomic.h> <answer> #include 
<token> <drm/drm_atomic_helper.h> <answer> #include 
<token> <drm/drm_crtc.h> <answer> #include 
#include <token> <answer> <drm/drm_probe_helper.h> 
#include <token> <answer> <drm/drm_vblank.h> 
<token> "fsl_dcu_drm_crtc.h" <answer> #include 
#include <token> <answer> "fsl_dcu_drm_drv.h" 
#include <token> <answer> "fsl_dcu_drm_plane.h" 
static void fsl_dcu_drm_crtc_atomic_flush(struct drm_crtc <token> <answer> *crtc, 
<token> drm_atomic_state *state) <answer> struct 
<token> drm_device *dev = crtc->dev; <answer> struct 
struct fsl_dcu_drm_device *fsl_dev <token> dev->dev_private; <answer> = 
struct drm_pending_vblank_event <token> = crtc->state->event; <answer> *event 
<token> DCU_UPDATE_MODE_READREG); <answer> DCU_UPDATE_MODE, 
if (event) <token> <answer> { 
crtc->state->event = <token> <answer> NULL; 
if <token> == 0) <answer> (drm_crtc_vblank_get(crtc) 
<token> event); <answer> drm_crtc_arm_vblank_event(crtc, 
drm_crtc_send_vblank_event(crtc, <token> <answer> event); 
<token> void fsl_dcu_drm_crtc_atomic_disable(struct drm_crtc *crtc, <answer> static 
<token> drm_atomic_state *state) <answer> struct 
struct drm_crtc_state *old_crtc_state <token> drm_atomic_get_old_crtc_state(state, <answer> = 
struct <token> *dev = crtc->dev; <answer> drm_device 
struct fsl_dcu_drm_device *fsl_dev <token> dev->dev_private; <answer> = 
<token> <linux/errno.h> <answer> #include 
#include <token> <answer> <linux/init.h> 
#include <token> <answer> <linux/module.h> 
#include <token> <answer> <linux/kernel.h> 
#include <token> <answer> <linux/kmod.h> 
<token> <linux/ktime.h> <answer> #include 
#include <token> <answer> <linux/slab.h> 
<token> <linux/mm.h> <answer> #include 
<token> <linux/string.h> <answer> #include 
<token> <linux/types.h> <answer> #include 
<token> <linux/uaccess.h> <answer> #include 
#include <token> <answer> <linux/version.h> 
#include <token> <answer> <media/cec-pin.h> 
<token> "cec-priv.h" <answer> #include 
<token> "cec-pin-priv.h" <answer> #include 
static inline struct cec_devnode *cec_devnode_data(struct file <token> <answer> *filp) 
struct cec_fh *fh <token> filp->private_data; <answer> = 
return <token> <answer> &fh->adap->devnode; 
<token> (valid_initiator || valid_follower) <answer> if 
<token> false; <answer> return 
<token> adap->cec_initiator || <answer> return 
fh->mode_initiator <token> CEC_MODE_NO_INITIATOR; <answer> == 
static long cec_adap_g_caps(struct <token> *adap, <answer> cec_adapter 
struct cec_caps <token> *parg) <answer> __user 
struct cec_caps <token> = {}; <answer> caps 
<token> adap->devnode.dev.parent->driver->name, <answer> strscpy(caps.driver, 
<token> adap->name, sizeof(caps.name)); <answer> strscpy(caps.name, 
caps.available_log_addrs = <token> <answer> adap->available_log_addrs; 
caps.capabilities <token> adap->capabilities; <answer> = 
caps.version <token> LINUX_VERSION_CODE; <answer> = 
if <token> &caps, sizeof(caps))) <answer> (copy_to_user(parg, 
<token> -EFAULT; <answer> return 
return <token> <answer> 0; 
<token> long cec_adap_g_phys_addr(struct cec_adapter *adap, <answer> static 
__u16 __user <token> <answer> *parg) 
<token> phys_addr; <answer> u16 
<token> = adap->phys_addr; <answer> phys_addr 
if (copy_to_user(parg, <token> sizeof(phys_addr))) <answer> &phys_addr, 
<token> -EFAULT; <answer> return 
return <token> <answer> 0; 
static int cec_validate_phys_addr(u16 <token> <answer> phys_addr) 
int <token> <answer> i; 
if <token> == CEC_PHYS_ADDR_INVALID) <answer> (phys_addr 
return <token> <answer> 0; 
for (i <token> 0; i < 16; i += 4) <answer> = 
if (phys_addr <token> (0xf << i)) <answer> & 
<token> (i == 16) <answer> if 
<token> 0; <answer> return 
for (i <token> 4; i < 16; i += 4) <answer> += 
if ((phys_addr & (0xf <token> i)) == 0) <answer> << 
return <token> <answer> -EINVAL; 
<token> 0; <answer> return 
static long cec_adap_s_phys_addr(struct cec_adapter <token> struct cec_fh *fh, <answer> *adap, 
bool block, __u16 __user <token> <answer> *parg) 
<token> phys_addr; <answer> u16 
<token> err; <answer> long 
if (!(adap->capabilities & <token> <answer> CEC_CAP_PHYS_ADDR)) 
<token> -ENOTTY; <answer> return 
<token> (copy_from_user(&phys_addr, parg, sizeof(phys_addr))) <answer> if 
return <token> <answer> -EFAULT; 
<token> = cec_validate_phys_addr(phys_addr); <answer> err 
<token> (err) <answer> if 
<token> err; <answer> return 
<token> (cec_is_busy(adap, fh)) <answer> if 
err = <token> <answer> -EBUSY; 
__cec_s_phys_addr(adap, <token> block); <answer> phys_addr, 
<token> err; <answer> return 
static long <token> cec_adapter *adap, <answer> cec_adap_g_log_addrs(struct 
struct cec_log_addrs __user <token> <answer> *parg) 
struct <token> log_addrs; <answer> cec_log_addrs 
memcpy(&log_addrs, <token> sizeof(log_addrs)); <answer> &adap->log_addrs, 
if <token> <answer> (!adap->is_configured) 
memset(log_addrs.log_addr, <token> <answer> CEC_LOG_ADDR_INVALID, 
if (copy_to_user(parg, <token> sizeof(log_addrs))) <answer> &log_addrs, 
<token> -EFAULT; <answer> return 
<token> 0; <answer> return 
static long cec_adap_s_log_addrs(struct cec_adapter *adap, struct cec_fh <token> <answer> *fh, 
bool block, struct cec_log_addrs __user <token> <answer> *parg) 
<token> cec_log_addrs log_addrs; <answer> struct 
<token> err = -EBUSY; <answer> long 
if (!(adap->capabilities & <token> <answer> CEC_CAP_LOG_ADDRS)) 
<token> -ENOTTY; <answer> return 
<token> (copy_from_user(&log_addrs, parg, sizeof(log_addrs))) <answer> if 
<token> -EFAULT; <answer> return 
log_addrs.flags <token> CEC_LOG_ADDRS_FL_ALLOW_UNREG_FALLBACK | <answer> &= 
<token> | <answer> CEC_LOG_ADDRS_FL_ALLOW_RC_PASSTHRU 
<token> (!adap->is_configuring && <answer> if 
<token> || !adap->is_configured) && <answer> (!log_addrs.num_log_addrs 
<token> fh)) { <answer> !cec_is_busy(adap, 
err = <token> &log_addrs, block); <answer> __cec_s_log_addrs(adap, 
<token> (!err) <answer> if 
log_addrs = <token> <answer> adap->log_addrs; 
<token> (err) <answer> if 
<token> err; <answer> return 
if (copy_to_user(parg, <token> sizeof(log_addrs))) <answer> &log_addrs, 
<token> -EFAULT; <answer> return 
<token> 0; <answer> return 
static long cec_adap_g_connector_info(struct <token> *adap, <answer> cec_adapter 
struct cec_log_addrs <token> *parg) <answer> __user 
<token> ret = 0; <answer> int 
<token> (!(adap->capabilities & CEC_CAP_CONNECTOR_INFO)) <answer> if 
return <token> <answer> -ENOTTY; 
if (copy_to_user(parg, <token> sizeof(adap->conn_info))) <answer> &adap->conn_info, 
ret <token> -EFAULT; <answer> = 
<token> ret; <answer> return 
<token> long cec_transmit(struct cec_adapter *adap, struct cec_fh *fh, <answer> static 
bool <token> struct cec_msg __user *parg) <answer> block, 
struct <token> msg = {}; <answer> cec_msg 
long <token> = 0; <answer> err 
<token> (!(adap->capabilities & CEC_CAP_TRANSMIT)) <answer> if 
<token> -ENOTTY; <answer> return 
if (copy_from_user(&msg, parg, <token> <answer> sizeof(msg))) 
<token> -EFAULT; <answer> return 
if (adap->log_addrs.num_log_addrs <token> 0) <answer> == 
err <token> -EPERM; <answer> = 
else <token> (adap->is_configuring) <answer> if 
<token> = -ENONET; <answer> err 
else <token> (cec_is_busy(adap, fh)) <answer> if 
<token> = -EBUSY; <answer> err 
<token> = cec_transmit_msg_fh(adap, &msg, fh, block); <answer> err 
if <token> <answer> (err) 
return <token> <answer> err; 
if (copy_to_user(parg, <token> sizeof(msg))) <answer> &msg, 
return <token> <answer> -EFAULT; 
return <token> <answer> 0; 
if ((mode_follower <token> CEC_MODE_EXCL_FOLLOWER || <answer> == 
mode_follower <token> CEC_MODE_EXCL_FOLLOWER_PASSTHRU) && <answer> == 
adap->cec_follower && adap->cec_follower <token> fh) <answer> != 
err <token> -EBUSY; <answer> = 
if (mode_initiator <token> CEC_MODE_EXCL_INITIATOR && <answer> == 
adap->cec_initiator <token> adap->cec_initiator != fh) <answer> && 
err <token> -EBUSY; <answer> = 
if (!err) <token> <answer> { 
bool old_mon_all = fh->mode_follower <token> CEC_MODE_MONITOR_ALL; <answer> == 
bool new_mon_all = mode_follower <token> CEC_MODE_MONITOR_ALL; <answer> == 
if (old_mon_all != new_mon_all) <token> <answer> { 
if <token> <answer> (new_mon_all) 
err = <token> <answer> cec_monitor_all_cnt_inc(adap); 
<token> (!err) { <answer> if 
bool old_mon_pin = fh->mode_follower == <token> <answer> CEC_MODE_MONITOR_PIN; 
<token> new_mon_pin = mode_follower == CEC_MODE_MONITOR_PIN; <answer> bool 
if <token> != new_mon_pin) { <answer> (old_mon_pin 
<token> = new_mon_pin; <answer> send_pin_event 
<token> (new_mon_pin) <answer> if 
err <token> cec_monitor_pin_cnt_inc(adap); <answer> = 
if (err) <token> <answer> { 
return <token> <answer> err; 
if (fh->mode_follower <token> CEC_MODE_FOLLOWER) <answer> == 
if <token> == CEC_MODE_FOLLOWER) <answer> (mode_follower 
if <token> { <answer> (send_pin_event) 
struct <token> ev = { <answer> cec_event 
<token> = CEC_EVENT_FL_INITIAL_STATE, <answer> .flags 
ev.event <token> adap->cec_pin_is_high ? CEC_EVENT_PIN_CEC_HIGH : <answer> = 
cec_queue_event_fh(fh, <token> 0); <answer> &ev, 
<token> (mode_follower == CEC_MODE_EXCL_FOLLOWER || <answer> if 
mode_follower <token> CEC_MODE_EXCL_FOLLOWER_PASSTHRU) { <answer> == 
<token> = <answer> adap->passthrough 
mode_follower == <token> <answer> CEC_MODE_EXCL_FOLLOWER_PASSTHRU; 
<token> = fh; <answer> adap->cec_follower 
} else if (adap->cec_follower == <token> { <answer> fh) 
adap->passthrough <token> false; <answer> = 
adap->cec_follower = <token> <answer> NULL; 
if (mode_initiator <token> CEC_MODE_EXCL_INITIATOR) <answer> == 
adap->cec_initiator = <token> <answer> fh; 
else <token> (adap->cec_initiator == fh) <answer> if 
adap->cec_initiator <token> NULL; <answer> = 
<token> = mode_initiator; <answer> fh->mode_initiator 
fh->mode_follower <token> mode_follower; <answer> = 
<token> 0; <answer> return 
static long cec_ioctl(struct file *filp, unsigned int cmd, unsigned <token> arg) <answer> long 
struct cec_fh *fh = <token> <answer> filp->private_data; 
struct cec_adapter *adap = <token> <answer> fh->adap; 
bool block <token> !(filp->f_flags & O_NONBLOCK); <answer> = 
void __user <token> = (void __user *)arg; <answer> *parg 
if <token> <answer> (!cec_is_registered(adap)) 
<token> -ENODEV; <answer> return 
<token> (cmd) { <answer> switch 
case <token> <answer> CEC_ADAP_G_CAPS: 
return cec_adap_g_caps(adap, <token> <answer> parg); 
case <token> <answer> CEC_ADAP_G_PHYS_ADDR: 
<token> cec_adap_g_phys_addr(adap, parg); <answer> return 
case <token> <answer> CEC_ADAP_S_PHYS_ADDR: 
return cec_adap_s_phys_addr(adap, fh, <token> parg); <answer> block, 
<token> CEC_ADAP_G_LOG_ADDRS: <answer> case 
return cec_adap_g_log_addrs(adap, <token> <answer> parg); 
case <token> <answer> CEC_ADAP_S_LOG_ADDRS: 
return cec_adap_s_log_addrs(adap, fh, <token> parg); <answer> block, 
<token> CEC_ADAP_G_CONNECTOR_INFO: <answer> case 
return <token> parg); <answer> cec_adap_g_connector_info(adap, 
<token> CEC_TRANSMIT: <answer> case 
<token> cec_transmit(adap, fh, block, parg); <answer> return 
case <token> <answer> CEC_RECEIVE: 
return cec_receive(adap, fh, <token> parg); <answer> block, 
<token> CEC_DQEVENT: <answer> case 
return cec_dqevent(adap, fh, <token> parg); <answer> block, 
<token> CEC_G_MODE: <answer> case 
return <token> fh, parg); <answer> cec_g_mode(adap, 
<token> CEC_S_MODE: <answer> case 
return <token> fh, parg); <answer> cec_s_mode(adap, 
return <token> <answer> -ENOTTY; 
static int cec_open(struct inode *inode, <token> file *filp) <answer> struct 
struct cec_devnode *devnode <token> <answer> = 
container_of(inode->i_cdev, struct <token> cdev); <answer> cec_devnode, 
struct cec_adapter *adap <token> to_cec_adapter(devnode); <answer> = 
struct cec_fh <token> = kzalloc(sizeof(*fh), GFP_KERNEL); <answer> *fh 
struct cec_event ev = <token> <answer> { 
.event = <token> <answer> CEC_EVENT_STATE_CHANGE, 
.flags <token> CEC_EVENT_FL_INITIAL_STATE, <answer> = 
unsigned int <token> <answer> i; 
int <token> <answer> err; 
if <token> <answer> (!fh) 
return <token> <answer> -ENOMEM; 
for (i = <token> i < CEC_NUM_EVENTS; i++) <answer> 0; 
fh->mode_initiator = <token> <answer> CEC_MODE_INITIATOR; 
fh->adap <token> adap; <answer> = 
<token> = cec_get_device(devnode); <answer> err 
<token> (err) { <answer> if 
return <token> <answer> err; 
filp->private_data = <token> <answer> fh; 
<token> <acpi/acpi.h> <answer> #include 
#include <token> <answer> "accommon.h" 
#include <token> <answer> "acparser.h" 
<token> "amlcode.h" <answer> #include 
#include <token> <answer> "acdispat.h" 
<token> "acinterp.h" <answer> #include 
<token> "acnamesp.h" <answer> #include 
<token> "acevents.h" <answer> #include 
#include <token> <answer> "actables.h" 
<token> _COMPONENT ACPI_DISPATCHER <answer> #define 
acpi_status <token> obj_handle) <answer> acpi_ds_initialize_region(acpi_handle 
union acpi_operand_object <token> <answer> *obj_desc; 
acpi_status <token> <answer> status; 
<token> = acpi_ns_get_attached_object(obj_handle); <answer> obj_desc 
<token> acpi_status <answer> static 
acpi_ds_init_buffer_field(u16 <token> <answer> aml_opcode, 
union acpi_operand_object <token> <answer> *obj_desc, 
<token> acpi_operand_object *buffer_desc, <answer> union 
union acpi_operand_object <token> <answer> *offset_desc, 
<token> acpi_operand_object *length_desc, <answer> union 
union acpi_operand_object <token> <answer> *result_desc) 
<token> offset; <answer> u32 
u32 <token> <answer> bit_offset; 
<token> bit_count; <answer> u32 
<token> field_flags; <answer> u8 
acpi_status <token> <answer> status; 
ACPI_FUNCTION_TRACE_PTR(ds_init_buffer_field, <token> <answer> obj_desc); 
if (ACPI_GET_DESCRIPTOR_TYPE(result_desc) <token> ACPI_DESC_TYPE_NAMED) { <answer> != 
"(%s) destination not a NS <token> [%s]", <answer> Node 
status = <token> <answer> AE_AML_OPERAND_TYPE; 
<token> cleanup; <answer> goto 
offset <token> (u32) offset_desc->integer.value; <answer> = 
switch <token> { <answer> (aml_opcode) 
<token> AML_CREATE_FIELD_OP: <answer> case 
status <token> <answer> = 
<token> field_flags, 0, <answer> acpi_ex_prep_common_field_object(obj_desc, 
bit_offset, <token> <answer> bit_count); 
if (ACPI_FAILURE(status)) <token> <answer> { 
goto <token> <answer> cleanup; 
obj_desc->buffer_field.buffer_obj = <token> <answer> buffer_desc; 
obj_desc->buffer_field.is_create_field <token> <answer> = 
<token> == AML_CREATE_FIELD_OP; <answer> aml_opcode 
<token> acpi_walk_state *walk_state, <answer> acpi_ds_eval_buffer_field_operands(struct 
<token> acpi_parse_object *op) <answer> union 
<token> status; <answer> acpi_status 
union <token> *obj_desc; <answer> acpi_operand_object 
struct <token> *node; <answer> acpi_namespace_node 
<token> acpi_parse_object *next_op; <answer> union 
ACPI_FUNCTION_TRACE_PTR(ds_eval_buffer_field_operands, <token> <answer> op); 
node = <token> <answer> op->common.node; 
<token> acpi_walk_state *walk_state, <answer> acpi_ds_eval_region_operands(struct 
union <token> *op) <answer> acpi_parse_object 
acpi_status <token> <answer> status; 
<token> acpi_operand_object *obj_desc; <answer> union 
<token> acpi_operand_object *operand_desc; <answer> union 
struct <token> *node; <answer> acpi_namespace_node 
<token> acpi_parse_object *next_op; <answer> union 
<token> space_id; <answer> acpi_adr_space_type 
ACPI_FUNCTION_TRACE_PTR(ds_eval_region_operands, <token> <answer> op); 
<token> = op->common.node; <answer> node 
operand_desc <token> walk_state->operands[walk_state->num_operands - 1]; <answer> = 
obj_desc->region.length = <token> operand_desc->integer.value; <answer> (u32) 
operand_desc = walk_state->operands[walk_state->num_operands - <token> <answer> 2]; 
<token> = (acpi_physical_address) <answer> obj_desc->region.address 
ACPI_DEBUG_PRINT((ACPI_DB_EXEC, "RgnObj %p Addr %8.8X%8.8X Len <token> <answer> %X\n", 
status <token> acpi_ut_add_address_range(obj_desc->region.space_id, <answer> = 
<token> node); <answer> obj_desc->region.length, 
acpi_ds_eval_table_region_operands(struct <token> *walk_state, <answer> acpi_walk_state 
<token> acpi_parse_object *op) <answer> union 
acpi_status <token> <answer> status; 
<token> acpi_operand_object *obj_desc; <answer> union 
<token> acpi_operand_object **operand; <answer> union 
struct <token> *node; <answer> acpi_namespace_node 
union <token> *next_op; <answer> acpi_parse_object 
<token> acpi_table_header *table; <answer> struct 
<token> table_index; <answer> u32 
ACPI_FUNCTION_TRACE_PTR(ds_eval_table_region_operands, <token> <answer> op); 
<token> = op->common.node; <answer> node 
status = acpi_ds_create_operands(walk_state, <token> <answer> next_op); 
if <token> { <answer> (ACPI_FAILURE(status)) 
operand = <token> <answer> &walk_state->operands[0]; 
<token> = <answer> status 
acpi_ex_resolve_operands(op->common.aml_opcode, <token> <answer> ACPI_WALK_OPERANDS, 
<token> (ACPI_FAILURE(status)) { <answer> if 
goto <token> <answer> cleanup; 
acpi_ds_eval_data_object_operands(struct acpi_walk_state <token> <answer> *walk_state, 
<token> acpi_parse_object *op, <answer> union 
union <token> *obj_desc) <answer> acpi_operand_object 
acpi_status <token> <answer> status; 
union acpi_operand_object <token> <answer> *arg_desc; 
u32 <token> <answer> length; 
<token> = walk_state->num_operands; <answer> walk_state->operand_index 
switch (op->common.aml_opcode) <token> <answer> { 
case <token> <answer> AML_BUFFER_OP: 
status <token> <answer> = 
acpi_ds_build_internal_buffer_obj(walk_state, op, <token> <answer> length, 
case <token> <answer> AML_PACKAGE_OP: 
case <token> <answer> AML_VARIABLE_PACKAGE_OP: 
status <token> <answer> = 
<token> op, length, <answer> acpi_ds_build_internal_package_obj(walk_state, 
<token> (ACPI_SUCCESS(status)) { <answer> if 
if ((!op->common.parent) <token> <answer> || 
((op->common.parent->common.aml_opcode != AML_PACKAGE_OP) <token> <answer> && 
(op->common.parent->common.aml_opcode <token> <answer> != 
&& <token> != <answer> (op->common.parent->common.aml_opcode 
<token> { <answer> AML_NAME_OP))) 
<token> = obj_desc; <answer> walk_state->result_obj 
acpi_ds_eval_bank_field_operands(struct acpi_walk_state <token> <answer> *walk_state, 
union acpi_parse_object <token> <answer> *op) 
acpi_status <token> <answer> status; 
<token> acpi_operand_object *obj_desc; <answer> union 
union acpi_operand_object <token> <answer> *operand_desc; 
struct acpi_namespace_node <token> <answer> *node; 
<token> acpi_parse_object *next_op; <answer> union 
union <token> *arg; <answer> acpi_parse_object 
<token> op); <answer> ACPI_FUNCTION_TRACE_PTR(ds_eval_bank_field_operands, 
walk_state->operand_index <token> 0; <answer> = 
status <token> acpi_ds_create_operand(walk_state, next_op, 0); <answer> = 
if (ACPI_FAILURE(status)) <token> <answer> { 
status = acpi_ex_resolve_to_value(&walk_state->operands[0], <token> <answer> walk_state); 
if (ACPI_FAILURE(status)) <token> <answer> { 
acpi_ps_get_opcode_name(op->common.aml_opcode), <token> <answer> 1); 
operand_desc = <token> <answer> walk_state->operands[0]; 
#include <token> <answer> <linux/ftrace.h> 
<token> <linux/uaccess.h> <answer> #include 
<token> <linux/module.h> <answer> #include 
#include <token> <answer> <linux/stop_machine.h> 
<token> <asm/cacheflush.h> <answer> #include 
#include <token> <answer> <asm/opcodes.h> 
<token> <asm/ftrace.h> <answer> #include 
#include <token> <answer> <asm/insn.h> 
#include <token> <answer> <asm/set_memory.h> 
<token> <asm/stacktrace.h> <answer> #include 
#include <token> <answer> <asm/patch.h> 
#ifdef <token> <answer> CONFIG_THUMB2_KERNEL 
ret = ftrace_modify_code(ip, old, new, <token> <answer> !is_kernel_inittext(ip)); 
<token> ret; <answer> return 
<token> <rdma/ib_mad.h> <answer> #include 
#include <token> <answer> <rdma/ib_user_verbs.h> 
<token> <linux/io.h> <answer> #include 
#include <token> <answer> <linux/module.h> 
<token> <linux/utsname.h> <answer> #include 
#include <token> <answer> <linux/rculist.h> 
<token> <linux/mm.h> <answer> #include 
<token> <linux/vmalloc.h> <answer> #include 
#include <token> <answer> <rdma/opa_addr.h> 
<token> <linux/nospec.h> <answer> #include 
<token> "hfi.h" <answer> #include 
<token> "common.h" <answer> #include 
#include <token> <answer> "device.h" 
#include <token> <answer> "trace.h" 
#include <token> <answer> "qp.h" 
#include <token> <answer> "verbs_txreq.h" 
#include <token> <answer> "debugfs.h" 
#include <token> <answer> "vnic.h" 
#include <token> <answer> "fault.h" 
<token> "affinity.h" <answer> #include 
<token> "ipoib.h" <answer> #include 
static unsigned int <token> = 16; <answer> hfi1_lkey_table_size 
module_param_named(lkey_table_size, <token> uint, <answer> hfi1_lkey_table_size, 
"LKEY table size in bits (2^n, <token> <= n <= 23)"); <answer> 1 
static unsigned int <token> = 0xFFFF; <answer> hfi1_max_pds 
module_param_named(max_pds, <token> uint, S_IRUGO); <answer> hfi1_max_pds, 
"Maximum number <token> protection domains to support"); <answer> of 
static <token> int hfi1_max_ahs = 0xFFFF; <answer> unsigned 
module_param_named(max_ahs, hfi1_max_ahs, uint, <token> <answer> S_IRUGO); 
MODULE_PARM_DESC(max_ahs, "Maximum number of <token> handles to support"); <answer> address 
<token> int hfi1_max_cqes = 0x2FFFFF; <answer> unsigned 
module_param_named(max_cqes, hfi1_max_cqes, <token> S_IRUGO); <answer> uint, 
"Maximum number of completion queue <token> to support"); <answer> entries 
unsigned <token> hfi1_max_cqs = 0x1FFFF; <answer> int 
module_param_named(max_cqs, hfi1_max_cqs, <token> S_IRUGO); <answer> uint, 
MODULE_PARM_DESC(max_cqs, "Maximum number of completion <token> to support"); <answer> queues 
unsigned int <token> = 0x3FFF; <answer> hfi1_max_qp_wrs 
<token> hfi1_max_qp_wrs, uint, S_IRUGO); <answer> module_param_named(max_qp_wrs, 
<token> "Maximum number of QP WRs to support"); <answer> MODULE_PARM_DESC(max_qp_wrs, 
unsigned <token> hfi1_max_qps = 32768; <answer> int 
<token> hfi1_max_qps, uint, S_IRUGO); <answer> module_param_named(max_qps, 
MODULE_PARM_DESC(max_qps, "Maximum number <token> QPs to support"); <answer> of 
unsigned <token> hfi1_max_sges = 0x60; <answer> int 
module_param_named(max_sges, hfi1_max_sges, <token> S_IRUGO); <answer> uint, 
MODULE_PARM_DESC(max_sges, "Maximum number of <token> to support"); <answer> SGEs 
unsigned int hfi1_max_mcast_grps = <token> <answer> 16384; 
module_param_named(max_mcast_grps, hfi1_max_mcast_grps, uint, <token> <answer> S_IRUGO); 
"Maximum number <token> multicast groups to support"); <answer> of 
unsigned int hfi1_max_mcast_qp_attached <token> 16; <answer> = 
module_param_named(max_mcast_qp_attached, <token> <answer> hfi1_max_mcast_qp_attached, 
uint, <token> <answer> S_IRUGO); 
"Maximum <token> of attached QPs to support"); <answer> number 
unsigned int <token> = 1024; <answer> hfi1_max_srqs 
module_param_named(max_srqs, <token> uint, S_IRUGO); <answer> hfi1_max_srqs, 
MODULE_PARM_DESC(max_srqs, <token> number of SRQs to support"); <answer> "Maximum 
unsigned int hfi1_max_srq_sges = <token> <answer> 128; 
<token> hfi1_max_srq_sges, uint, S_IRUGO); <answer> module_param_named(max_srq_sges, 
MODULE_PARM_DESC(max_srq_sges, "Maximum <token> of SRQ SGEs to support"); <answer> number 
unsigned int <token> = 0x1FFFF; <answer> hfi1_max_srq_wrs 
module_param_named(max_srq_wrs, hfi1_max_srq_wrs, <token> S_IRUGO); <answer> uint, 
MODULE_PARM_DESC(max_srq_wrs, "Maximum <token> of SRQ WRs support"); <answer> number 
unsigned short piothreshold <token> 256; <answer> = 
module_param(piothreshold, ushort, <token> <answer> S_IRUGO); 
MODULE_PARM_DESC(piothreshold, "size used to determine <token> vs. pio"); <answer> sdma 
static unsigned <token> sge_copy_mode; <answer> int 
module_param(sge_copy_mode, uint, <token> <answer> S_IRUGO); 
"Verbs copy mode: 0 use <token> 1 use cacheless copy, 2 adapt based on WSS"); <answer> memcpy, 
<token> void verbs_sdma_complete( <answer> static 
struct sdma_txreq <token> <answer> *cookie, 
<token> status); <answer> int 
static <token> pio_wait(struct rvt_qp *qp, <answer> int 
struct <token> *sc, <answer> send_context 
struct hfi1_pkt_state <token> <answer> *ps, 
u32 <token> <answer> flag); 
const enum ib_wc_opcode <token> = { <answer> ib_hfi1_wc_opcode[] 
<token> = IB_WC_RDMA_WRITE, <answer> [IB_WR_RDMA_WRITE] 
[IB_WR_TID_RDMA_WRITE] <token> IB_WC_RDMA_WRITE, <answer> = 
[IB_WR_RDMA_WRITE_WITH_IMM] <token> IB_WC_RDMA_WRITE, <answer> = 
<token> = IB_WC_SEND, <answer> [IB_WR_SEND] 
[IB_WR_SEND_WITH_IMM] <token> IB_WC_SEND, <answer> = 
[IB_WR_RDMA_READ] <token> IB_WC_RDMA_READ, <answer> = 
<token> = IB_WC_RDMA_READ, <answer> [IB_WR_TID_RDMA_READ] 
[IB_WR_ATOMIC_CMP_AND_SWP] = <token> <answer> IB_WC_COMP_SWAP, 
[IB_WR_ATOMIC_FETCH_AND_ADD] = <token> <answer> IB_WC_FETCH_ADD, 
<token> = IB_WC_SEND, <answer> [IB_WR_SEND_WITH_INV] 
<token> = IB_WC_LOCAL_INV, <answer> [IB_WR_LOCAL_INV] 
[IB_WR_REG_MR] = <token> <answer> IB_WC_REG_MR 
const u8 <token> = { <answer> hdr_len_by_opcode[256] 
<token> ib_hfi1_sys_image_guid; <answer> __be64 
static inline opcode_handler qp_ok(struct <token> *packet) <answer> hfi1_packet 
if (!(ib_rvt_state_ops[packet->qp->state] & <token> <answer> RVT_PROCESS_RECV_OK)) 
return <token> <answer> NULL; 
if (((packet->opcode & RVT_OPCODE_QP_MASK) <token> <answer> == 
packet->qp->allowed_ops) <token> <answer> || 
<token> == IB_OPCODE_CNP)) <answer> (packet->opcode 
return <token> <answer> opcode_handler_tbl[packet->opcode]; 
return <token> <answer> NULL; 
static u64 hfi1_fault_tx(struct rvt_qp *qp, u8 <token> u64 pbc) <answer> opcode, 
<token> CONFIG_FAULT_INJECTION <answer> #ifdef 
<token> ((opcode & IB_OPCODE_MSP) == IB_OPCODE_MSP) { <answer> if 
<token> &= ~PBC_INSERT_HCRC_SMASK; <answer> pbc 
pbc |= (u64)PBC_IHCRC_NONE << <token> <answer> PBC_INSERT_HCRC_SHIFT; 
<token> else { <answer> } 
pbc <token> PBC_TEST_EBP; <answer> |= 
return <token> <answer> pbc; 
static opcode_handler tid_qp_ok(int opcode, struct <token> *packet) <answer> hfi1_packet 
if (packet->qp->ibqp.qp_type != IB_QPT_RC <token> <answer> || 
!(ib_rvt_state_ops[packet->qp->state] & <token> <answer> RVT_PROCESS_RECV_OK)) 
return <token> <answer> NULL; 
if ((opcode & RVT_OPCODE_QP_MASK) == <token> <answer> IB_OPCODE_TID_RDMA) 
return <token> <answer> opcode_handler_tbl[opcode]; 
return <token> <answer> NULL; 
void <token> hfi1_packet *packet) <answer> hfi1_kdeth_eager_rcv(struct 
struct hfi1_ctxtdata <token> = packet->rcd; <answer> *rcd 
struct <token> *hdr = packet->hdr; <answer> ib_header 
u32 tlen = <token> <answer> packet->tlen; 
struct hfi1_pportdata *ppd <token> rcd->ppd; <answer> = 
struct hfi1_ibport <token> = &ppd->ibport_data; <answer> *ibp 
struct rvt_dev_info *rdi <token> &ppd->dd->verbs_dev.rdi; <answer> = 
<token> opcode_handler; <answer> opcode_handler 
unsigned long <token> <answer> flags; 
u32 <token> <answer> qp_num; 
<token> lnh; <answer> int 
<token> opcode; <answer> u8 
if (atomic_dec_return(&mcast->refcount) <= <token> <answer> 1) 
} else <token> <answer> { 
void hfi1_ib_rcv(struct hfi1_packet <token> <answer> *packet) 
struct hfi1_ctxtdata *rcd = <token> <answer> packet->rcd; 
trace_input_ibhdr(rcd->dd, <token> !!(rhf_dc_info(packet->rhf))); <answer> packet, 
<token> hfi1_check_mcast(packet->dlid)); <answer> hfi1_handle_packet(packet, 
void hfi1_16B_rcv(struct <token> *packet) <answer> hfi1_packet 
<token> hfi1_ctxtdata *rcd = packet->rcd; <answer> struct 
trace_input_ibhdr(rcd->dd, packet, <token> <answer> false); 
<token> hfi1_check_mcast(packet->dlid)); <answer> hfi1_handle_packet(packet, 
<token> void mem_timer(struct timer_list *t) <answer> static 
struct hfi1_ibdev *dev = <token> t, mem_timer); <answer> from_timer(dev, 
struct list_head *list = <token> <answer> &dev->memwait; 
struct <token> *qp = NULL; <answer> rvt_qp 
struct <token> *wait; <answer> iowait 
unsigned <token> flags; <answer> long 
struct <token> *priv; <answer> hfi1_qp_priv 
write_seqlock_irqsave(&dev->iowait_lock, <token> <answer> flags); 
<token> (!list_empty(list)) { <answer> if 
wait = <token> struct iowait, list); <answer> list_first_entry(list, 
<token> = iowait_to_qp(wait); <answer> qp 
priv <token> qp->priv; <answer> = 
priv->s_iowait.lock = <token> <answer> NULL; 
static noinline <token> build_verbs_ulp_payload( <answer> int 
<token> sdma_engine *sde, <answer> struct 
<token> length, <answer> u32 
struct <token> *tx) <answer> verbs_txreq 
struct <token> *ss = tx->ss; <answer> rvt_sge_state 
struct <token> *sg_list = ss->sg_list; <answer> rvt_sge 
struct rvt_sge sge = <token> <answer> ss->sge; 
<token> num_sge = ss->num_sge; <answer> u8 
<token> len; <answer> u32 
<token> ret = 0; <answer> int 
while <token> { <answer> (length) 
len = rvt_get_sge_length(&ss->sge, <token> <answer> length); 
<token> == 0); <answer> WARN_ON_ONCE(len 
ret = <token> <answer> sdma_txadd_kvaddr( 
if <token> <answer> (ret) 
<token> bail_txadd; <answer> goto 
rvt_update_sge(ss, len, <token> <answer> false); 
length <token> len; <answer> -= 
return <token> <answer> ret; 
static void <token> rvt_qp *qp, struct hfi1_pkt_state *ps, <answer> update_tx_opstats(struct 
<token> plen) <answer> u32 
#ifdef <token> <answer> CONFIG_DEBUG_FS 
<token> hfi1_devdata *dd = dd_from_ibdev(qp->ibqp.device); <answer> struct 
<token> hfi1_opcode_stats_perctx *s = get_cpu_ptr(dd->tx_opstats); <answer> struct 
inc_opstats(plen * <token> &s->stats[ps->opcode]); <answer> 4, 
<token> = hfi1_get_16b_padding(hdrbytes - 8, length) + <answer> extra_bytes 
(SIZE_OF_CRC <token> 2) + SIZE_OF_LT; <answer> << 
if (!ahg_info->ahgcount) <token> <answer> { 
ret <token> sdma_txinit_ahg( <answer> = 
hdrbytes + length <token> <answer> + 
if <token> <answer> (ret) 
goto <token> <answer> bail_txadd; 
phdr->pbc <token> cpu_to_le64(pbc); <answer> = 
<token> = sdma_txadd_kvaddr( <answer> ret 
<token> (ret) <answer> if 
goto <token> <answer> bail_txadd; 
<token> else { <answer> } 
<token> = sdma_txinit_ahg( <answer> ret 
if <token> <answer> (ret) 
goto <token> <answer> bail_txadd; 
static int pio_wait(struct <token> *qp, <answer> rvt_qp 
<token> send_context *sc, <answer> struct 
struct <token> *ps, <answer> hfi1_pkt_state 
<token> flag) <answer> u32 
struct hfi1_qp_priv <token> = qp->priv; <answer> *priv 
struct hfi1_devdata <token> = sc->dd; <answer> *dd 
<token> long flags; <answer> unsigned 
int <token> = 0; <answer> ret 
<token> flags); <answer> spin_lock_irqsave(&qp->s_lock, 
if (ib_rvt_state_ops[qp->state] & <token> { <answer> RVT_PROCESS_RECV_OK) 
<token> (list_empty(&priv->s_iowait.list)) { <answer> if 
struct <token> *dev = &dd->verbs_dev; <answer> hfi1_ibdev 
<token> was_empty; <answer> int 
<token> += !!(flag & RVT_S_WAIT_PIO); <answer> dev->n_piowait 
dev->n_piodrain <token> !!(flag & HFI1_S_WAIT_PIO_DRAIN); <answer> += 
qp->s_flags <token> flag; <answer> |= 
was_empty = <token> <answer> list_empty(&sc->piowait); 
<token> &priv->s_iowait, <answer> iowait_queue(ps->pkts_sent, 
priv->s_iowait.lock = <token> <answer> &sc->waitlock; 
<token> RVT_S_WAIT_PIO); <answer> trace_hfi1_qpsleep(qp, 
"alloc failed. state not <token> completing"); <answer> active, 
<token> = IB_WC_GENERAL_ERR; <answer> wc_status 
goto <token> <answer> pio_bail; 
<token> else { <answer> } 
<token> "alloc failed. state active, queuing"); <answer> hfi1_cdbg(PIO, 
ret = pio_wait(qp, <token> ps, RVT_S_WAIT_PIO); <answer> sc, 
<token> (!ret) <answer> if 
static inline <token> egress_pkey_matches_entry(u16 pkey, u16 ent) <answer> int 
u16 mkey <token> pkey & PKEY_LOW_15_MASK; <answer> = 
u16 <token> = ent & PKEY_LOW_15_MASK; <answer> mentry 
<token> (mkey == mentry) { <answer> if 
<token> (pkey & PKEY_MEMBER_MASK) <answer> if 
return <token> & PKEY_MEMBER_MASK); <answer> !!(ent 
return <token> <answer> 1; 
<token> 0; <answer> return 
int egress_pkey_check(struct hfi1_pportdata *ppd, <token> slid, u16 pkey, <answer> u32 
<token> sc5, int8_t s_pkey_index) <answer> u8 
struct hfi1_devdata <token> <answer> *dd; 
<token> i; <answer> int 
int is_user_ctxt_mechanism = (s_pkey_index < <token> <answer> 0); 
if (!(ppd->part_enforce & <token> <answer> HFI1_PART_ENFORCE_OUT)) 
return <token> <answer> 0; 
if (!is_user_ctxt_mechanism <token> <answer> && 
egress_pkey_matches_entry(pkey, <token> { <answer> ppd->pkeys[s_pkey_index])) 
return <token> <answer> 0; 
for (i = 0; i < MAX_PKEY_VALUES; i++) <token> <answer> { 
<token> (egress_pkey_matches_entry(pkey, ppd->pkeys[i])) <answer> if 
<token> 0; <answer> return 
if <token> { <answer> (!is_user_ctxt_mechanism) 
<token> = ppd->dd; <answer> dd 
if <token> & <answer> (!(dd->err_info_xmit_constraint.status 
<token> { <answer> OPA_EI_STATUS_SMASK)) 
dd->err_info_xmit_constraint.status <token> <answer> |= 
dd->err_info_xmit_constraint.slid = <token> <answer> slid; 
<token> = pkey; <answer> dd->err_info_xmit_constraint.pkey 
<token> 1; <answer> return 
static inline send_routine get_send_routine(struct rvt_qp <token> <answer> *qp, 
struct <token> *ps) <answer> hfi1_pkt_state 
<token> hfi1_devdata *dd = dd_from_ibdev(qp->ibqp.device); <answer> struct 
struct <token> *priv = qp->priv; <answer> hfi1_qp_priv 
struct verbs_txreq *tx <token> ps->s_txreq; <answer> = 
if <token> & HFI1_HAS_SEND_DMA))) <answer> (unlikely(!(dd->flags 
<token> dd->process_pio_send; <answer> return 
switch (qp->ibqp.qp_type) <token> <answer> { 
case <token> <answer> IB_QPT_SMI: 
return <token> <answer> dd->process_pio_send; 
case <token> <answer> IB_QPT_GSI: 
<token> IB_QPT_UD: <answer> case 
<token> IB_QPT_UC: <answer> case 
<token> IB_QPT_RC: <answer> case 
priv->s_running_pkt_size <token> <answer> = 
<token> + priv->s_running_pkt_size) / 2; <answer> (tx->s_cur_size 
<token> (piothreshold && <answer> if 
priv->s_running_pkt_size <token> min(piothreshold, qp->pmtu) && <answer> <= 
(BIT(ps->opcode & <token> & pio_opmask[ps->opcode >> 5]) && <answer> OPMASK) 
iowait_sdma_pending(&priv->s_iowait) <token> 0 && <answer> == 
<token> dd->process_pio_send; <answer> return 
return <token> <answer> dd->process_dma_send; 
int hfi1_verbs_send(struct rvt_qp <token> struct hfi1_pkt_state *ps) <answer> *qp, 
struct hfi1_devdata <token> = dd_from_ibdev(qp->ibqp.device); <answer> *dd 
struct hfi1_qp_priv *priv <token> qp->priv; <answer> = 
struct ib_other_headers *ohdr = <token> <answer> NULL; 
send_routine <token> <answer> sr; 
int <token> <answer> ret; 
u16 <token> <answer> pkey; 
u32 <token> <answer> slid; 
u8 l4 <token> 0; <answer> = 
<token> (sr == dd->process_pio_send) { <answer> if 
unsigned long <token> <answer> flags; 
hfi1_cdbg(PIO, <token> Failed. Completing with err", <answer> "%s() 
<token> flags); <answer> spin_lock_irqsave(&qp->s_lock, 
rvt_send_complete(qp, qp->s_wqe, <token> <answer> IB_WC_GENERAL_ERR); 
<token> flags); <answer> spin_unlock_irqrestore(&qp->s_lock, 
return <token> <answer> -EINVAL; 
if (sr == <token> && iowait_pio_pending(&priv->s_iowait)) <answer> dd->process_dma_send 
return <token> <answer> pio_wait(qp, 
return sr(qp, <token> 0); <answer> ps, 
static void hfi1_fill_device_attr(struct <token> *dd) <answer> hfi1_devdata 
struct <token> *rdi = &dd->verbs_dev.rdi; <answer> rvt_dev_info 
<token> ver = dd->dc8051_ver; <answer> u32 
<token> 0, sizeof(rdi->dparms.props)); <answer> memset(&rdi->dparms.props, 
<token> = ((u64)(dc8051_ver_maj(ver)) << 32) | <answer> rdi->dparms.props.fw_ver 
<token> << 16) | <answer> ((u64)(dc8051_ver_min(ver)) 
rdi->dparms.props.device_cap_flags <token> IB_DEVICE_BAD_PKEY_CNTR | <answer> = 
IB_DEVICE_BAD_QKEY_CNTR | IB_DEVICE_SHUTDOWN_PORT <token> <answer> | 
IB_DEVICE_SYS_IMAGE_GUID <token> IB_DEVICE_RC_RNR_NAK_GEN | <answer> | 
IB_DEVICE_PORT_ACTIVE_EVENT | IB_DEVICE_SRQ_RESIZE <token> <answer> | 
<token> = IBK_RDMA_NETDEV_OPA; <answer> rdi->dparms.props.kernel_cap_flags 
rdi->dparms.props.page_size_cap <token> PAGE_SIZE; <answer> = 
rdi->dparms.props.vendor_id = dd->oui1 << 16 <token> dd->oui2 << 8 | dd->oui3; <answer> | 
rdi->dparms.props.vendor_part_id <token> dd->pcidev->device; <answer> = 
<token> = dd->minrev; <answer> rdi->dparms.props.hw_ver 
rdi->dparms.props.sys_image_guid <token> ib_hfi1_sys_image_guid; <answer> = 
rdi->dparms.props.max_mr_size = <token> <answer> U64_MAX; 
rdi->dparms.props.max_fast_reg_page_list_len = <token> <answer> UINT_MAX; 
<token> = hfi1_max_qps; <answer> rdi->dparms.props.max_qp 
rdi->dparms.props.max_qp_wr <token> <answer> = 
(hfi1_max_qp_wrs <token> HFI1_QP_WQE_INVALID ? <answer> >= 
HFI1_QP_WQE_INVALID <token> 1 : hfi1_max_qp_wrs); <answer> - 
<token> = hfi1_max_sges; <answer> rdi->dparms.props.max_send_sge 
rdi->dparms.props.max_recv_sge <token> hfi1_max_sges; <answer> = 
rdi->dparms.props.max_sge_rd <token> hfi1_max_sges; <answer> = 
rdi->dparms.props.max_cq <token> hfi1_max_cqs; <answer> = 
rdi->dparms.props.max_ah <token> hfi1_max_ahs; <answer> = 
rdi->dparms.props.max_cqe <token> hfi1_max_cqes; <answer> = 
rdi->dparms.props.max_pd = <token> <answer> hfi1_max_pds; 
<token> = HFI1_MAX_RDMA_ATOMIC; <answer> rdi->dparms.props.max_qp_rd_atom 
<token> = 255; <answer> rdi->dparms.props.max_qp_init_rd_atom 
<token> = hfi1_max_srqs; <answer> rdi->dparms.props.max_srq 
rdi->dparms.props.max_srq_wr = <token> <answer> hfi1_max_srq_wrs; 
rdi->dparms.props.max_srq_sge = <token> <answer> hfi1_max_srq_sges; 
<token> = IB_ATOMIC_GLOB; <answer> rdi->dparms.props.atomic_cap 
<token> = hfi1_get_npkeys(dd); <answer> rdi->dparms.props.max_pkeys 
<token> = hfi1_max_mcast_grps; <answer> rdi->dparms.props.max_mcast_grp 
rdi->dparms.props.max_mcast_qp_attach <token> hfi1_max_mcast_qp_attached; <answer> = 
rdi->dparms.props.max_total_mcast_qp_attach <token> <answer> = 
rdi->dparms.props.max_mcast_qp_attach <token> <answer> * 
<token> inline u16 opa_speed_to_ib(u16 in) <answer> static 
u16 <token> = 0; <answer> out 
if <token> & OPA_LINK_SPEED_25G) <answer> (in 
<token> |= IB_SPEED_EDR; <answer> out 
if (in & <token> <answer> OPA_LINK_SPEED_12_5G) 
<token> |= IB_SPEED_FDR; <answer> out 
return <token> <answer> out; 
static inline u16 opa_width_to_ib(u16 <token> <answer> in) 
switch <token> { <answer> (in) 
case <token> <answer> OPA_LINK_WIDTH_1X: 
<token> = mtu_to_enum((!valid_ib_mtu(hfi1_max_mtu) ? <answer> props->max_mtu 
4096 <token> hfi1_max_mtu), IB_MTU_4096); <answer> : 
props->active_mtu = !valid_ib_mtu(ppd->ibmtu) ? <token> : <answer> props->max_mtu 
<token> IB_MTU_4096); <answer> mtu_to_enum(ppd->ibmtu, 
<token> = hfi1_max_mtu; <answer> props->phys_mtu 
<token> 0; <answer> return 
static int modify_device(struct ib_device <token> <answer> *device, 
<token> device_modify_mask, <answer> int 
struct <token> *device_modify) <answer> ib_device_modify 
struct hfi1_devdata *dd = <token> <answer> dd_from_ibdev(device); 
unsigned <token> <answer> i; 
<token> ret; <answer> int 
if (device_modify_mask & <token> | <answer> ~(IB_DEVICE_MODIFY_SYS_IMAGE_GUID 
<token> { <answer> IB_DEVICE_MODIFY_NODE_DESC)) 
ret = <token> <answer> -EOPNOTSUPP; 
<token> bail; <answer> goto 
if (device_modify_mask & <token> { <answer> IB_DEVICE_MODIFY_NODE_DESC) 
memcpy(device->node_desc, <token> <answer> device_modify->node_desc, 
for (i = 0; i < <token> i++) { <answer> dd->num_pports; 
<token> hfi1_ibport *ibp = &dd->pport[i].ibport_data; <answer> struct 
if (device_modify_mask & IB_DEVICE_MODIFY_SYS_IMAGE_GUID) <token> <answer> { 
ib_hfi1_sys_image_guid <token> <answer> = 
for (i = 0; <token> < dd->num_pports; i++) { <answer> i 
struct <token> *ibp = &dd->pport[i].ibport_data; <answer> hfi1_ibport 
<token> = 0; <answer> ret 
return <token> <answer> ret; 
static int <token> rvt_dev_info *rdi, u32 port_num) <answer> shut_down_port(struct 
struct <token> *verbs_dev = dev_from_rdi(rdi); <answer> hfi1_ibdev 
struct hfi1_devdata *dd = <token> <answer> dd_from_dev(verbs_dev); 
struct hfi1_pportdata *ppd = &dd->pport[port_num - <token> <answer> 1]; 
set_link_down_reason(ppd, <token> 0, <answer> OPA_LINKDOWN_REASON_UNKNOWN, 
return set_link_state(ppd, <token> <answer> HLS_DN_DOWNDEF); 
static int <token> rvt_dev_info *rdi, struct rvt_ibport *rvp, <answer> hfi1_get_guid_be(struct 
int guid_index, __be64 <token> <answer> *guid) 
struct hfi1_ibport <token> = container_of(rvp, struct hfi1_ibport, rvp); <answer> *ibp 
if (guid_index <token> HFI1_GUIDS_PER_PORT) <answer> >= 
<token> -EINVAL; <answer> return 
<token> = get_sguid(ibp, guid_index); <answer> *guid 
<token> 0; <answer> return 
u8 ah_to_sc(struct ib_device <token> struct rdma_ah_attr *ah) <answer> *ibdev, 
struct hfi1_ibport <token> = to_iport(ibdev, rdma_ah_get_port_num(ah)); <answer> *ibp 
return <token> <answer> ibp->sl_to_sc[rdma_ah_get_sl(ah)]; 
static int hfi1_check_ah(struct ib_device *ibdev, struct <token> *ah_attr) <answer> rdma_ah_attr 
<token> hfi1_ibport *ibp; <answer> struct 
struct <token> *ppd; <answer> hfi1_pportdata 
struct <token> *dd; <answer> hfi1_devdata 
<token> sc5; <answer> u8 
<token> sl; <answer> u8 
if (hfi1_check_mcast(rdma_ah_get_dlid(ah_attr)) <token> <answer> && 
!(rdma_ah_get_ah_flags(ah_attr) & <token> <answer> IB_AH_GRH)) 
<token> -EINVAL; <answer> return 
ibp = <token> rdma_ah_get_port_num(ah_attr)); <answer> to_iport(ibdev, 
<token> = ppd_from_ibp(ibp); <answer> ppd 
sc5 = <token> <answer> ibp->sl_to_sc[rdma_ah_get_sl(&ah->attr)]; 
<token> attr); <answer> hfi1_update_ah_attr(ibdev, 
<token> = dd_from_ppd(ppd); <answer> dd 
<token> = sc_to_vlt(dd, sc5); <answer> ah->vl 
if (ah->vl < num_vls || ah->vl <token> 15) <answer> == 
ah->log_pmtu <token> ilog2(dd->vld[ah->vl].mtu); <answer> = 
unsigned <token> hfi1_devdata *dd) <answer> hfi1_get_npkeys(struct 
return <token> <answer> ARRAY_SIZE(dd->pport[0].pkeys); 
static void init_ibport(struct <token> *ppd) <answer> hfi1_pportdata 
struct hfi1_ibport *ibp = <token> <answer> &ppd->ibport_data; 
size_t sz = <token> <answer> ARRAY_SIZE(ibp->sl_to_sc); 
int <token> <answer> i; 
for <token> = 0; i < sz; i++) { <answer> (i 
ibp->sl_to_sc[i] = <token> <answer> i; 
ibp->sc_to_sl[i] <token> i; <answer> = 
for (i = 0; i <token> RVT_MAX_TRAP_LISTS ; i++) <answer> < 
timer_setup(&ibp->rvp.trap_timer, hfi1_handle_trap_timer, <token> <answer> 0); 
ibp->rvp.port_cap_flags <token> IB_PORT_AUTO_MIGR_SUP | <answer> = 
ibp->rvp.port_cap3_flags = <token> <answer> OPA_CAP_MASK3_IsSharedSpaceSupported; 
ibp->rvp.pma_counter_select[0] = <token> <answer> IB_PMA_PORT_XMIT_DATA; 
ibp->rvp.pma_counter_select[1] = <token> <answer> IB_PMA_PORT_RCV_DATA; 
ibp->rvp.pma_counter_select[2] <token> IB_PMA_PORT_XMIT_PKTS; <answer> = 
ibp->rvp.pma_counter_select[3] <token> IB_PMA_PORT_RCV_PKTS; <answer> = 
<token> = IB_PMA_PORT_XMIT_WAIT; <answer> ibp->rvp.pma_counter_select[4] 
RCU_INIT_POINTER(ibp->rvp.qp[0], <token> <answer> NULL); 
<token> NULL); <answer> RCU_INIT_POINTER(ibp->rvp.qp[1], 
static <token> hfi1_get_dev_fw_str(struct ib_device *ibdev, char *str) <answer> void 
struct rvt_dev_info *rdi = <token> <answer> ib_to_rvt(ibdev); 
struct hfi1_ibdev <token> = dev_from_rdi(rdi); <answer> *dev 
u32 ver <token> dd_from_dev(dev)->dc8051_ver; <answer> = 
snprintf(str, <token> "%u.%u.%u", dc8051_ver_maj(ver), <answer> IB_FW_VERSION_NAME_MAX, 
<token> dc8051_ver_patch(ver)); <answer> dc8051_ver_min(ver), 
static const char * <token> driver_cntr_names[] = { <answer> const 
static int init_cntr_names(const char *names_in, const size_t <token> <answer> names_len, 
<token> num_extra_names, int *num_cntrs, <answer> int 
struct <token> **cntr_descs) <answer> rdma_stat_desc 
struct <token> *names_out; <answer> rdma_stat_desc 
<token> *p; <answer> char 
int i, <token> <answer> n; 
<token> = 0; <answer> n 
for (i = 0; i <token> names_len; i++) <answer> < 
if (names_in[i] <token> '\n') <answer> == 
names_out = kzalloc((n + <token> * sizeof(*names_out) <answer> num_extra_names) 
<token> names_len, <answer> + 
<token> (!names_out) { <answer> if 
*num_cntrs <token> 0; <answer> = 
*cntr_descs = <token> <answer> NULL; 
<token> -ENOMEM; <answer> return 
p = (char *)&names_out[n <token> num_extra_names]; <answer> + 
<token> names_in, names_len); <answer> memcpy(p, 
<token> (i = 0; i < n; i++) { <answer> for 
names_out[i].name <token> p; <answer> = 
p <token> strchr(p, '\n'); <answer> = 
*p++ <token> '\0'; <answer> = 
<token> = n; <answer> *num_cntrs 
*cntr_descs = <token> <answer> names_out; 
<token> 0; <answer> return 
static struct <token> *hfi1_alloc_hw_device_stats(struct ib_device *ibdev) <answer> rdma_hw_stats 
if (!dev_cntr_descs) <token> <answer> { 
struct hfi1_devdata <token> = dd_from_ibdev(ibdev); <answer> *dd 
<token> i, err; <answer> int 
<token> = init_cntr_names(dd->cntrnames, dd->cntrnameslen, <answer> err 
<token> &dev_cntr_descs); <answer> &num_dev_cntrs, 
<token> (err) <answer> if 
<token> NULL; <answer> return 
for (i = <token> i < num_driver_cntrs; i++) <answer> 0; 
dev_cntr_descs[num_dev_cntrs <token> i].name = <answer> + 
return <token> <answer> rdma_alloc_hw_stats_struct(dev_cntr_descs, 
num_dev_cntrs + <token> <answer> num_driver_cntrs, 
static <token> rdma_hw_stats *hfi_alloc_hw_port_stats(struct ib_device *ibdev, <answer> struct 
u32 <token> <answer> port_num) 
<token> (!port_cntr_descs) { <answer> if 
struct <token> *dd = dd_from_ibdev(ibdev); <answer> hfi1_devdata 
int <token> <answer> err; 
err <token> init_cntr_names(dd->portcntrnames, dd->portcntrnameslen, <answer> = 
<token> &port_cntr_descs); <answer> &num_port_cntrs, 
<token> (err) <answer> if 
return <token> <answer> NULL; 
return rdma_alloc_hw_stats_struct(port_cntr_descs, <token> <answer> num_port_cntrs, 
static <token> hfi1_sps_ints(void) <answer> u64 
unsigned long index, <token> <answer> flags; 
struct hfi1_devdata <token> <answer> *dd; 
<token> sps_ints = 0; <answer> u64 
<token> flags); <answer> xa_lock_irqsave(&hfi1_dev_table, 
xa_for_each(&hfi1_dev_table, <token> dd) { <answer> index, 
<token> += get_all_cpu_total(dd->int_counter); <answer> sps_ints 
<token> flags); <answer> xa_unlock_irqrestore(&hfi1_dev_table, 
<token> sps_ints; <answer> return 
static int get_hw_stats(struct ib_device *ibdev, struct rdma_hw_stats <token> <answer> *stats, 
u32 <token> int index) <answer> port, 
<token> *values; <answer> u64 
<token> count; <answer> int 
if (!port) <token> <answer> { 
u64 *stats = (u64 <token> <answer> *)&hfi1_stats; 
int <token> <answer> i; 
<token> NULL, &values); <answer> hfi1_read_cntrs(dd_from_ibdev(ibdev), 
values[num_dev_cntrs] = <token> <answer> hfi1_sps_ints(); 
<token> (i = 1; i < num_driver_cntrs; i++) <answer> for 
values[num_dev_cntrs + i] = <token> <answer> stats[i]; 
count <token> num_dev_cntrs + num_driver_cntrs; <answer> = 
<token> else { <answer> } 
struct hfi1_ibport *ibp = <token> port); <answer> to_iport(ibdev, 
hfi1_read_portcntrs(ppd_from_ibp(ibp), <token> &values); <answer> NULL, 
count = <token> <answer> num_port_cntrs; 
memcpy(stats->value, values, count * <token> <answer> sizeof(u64)); 
<token> count; <answer> return 
static <token> struct ib_device_ops hfi1_dev_ops = { <answer> const 
.owner <token> THIS_MODULE, <answer> = 
.driver_id <token> RDMA_DRIVER_HFI1, <answer> = 
.alloc_hw_device_stats <token> hfi1_alloc_hw_device_stats, <answer> = 
.alloc_hw_port_stats = <token> <answer> hfi_alloc_hw_port_stats, 
.alloc_rdma_netdev = <token> <answer> hfi1_vnic_alloc_rn, 
.device_group <token> &ib_hfi1_attr_group, <answer> = 
.get_dev_fw_str = <token> <answer> hfi1_get_dev_fw_str, 
.get_hw_stats = <token> <answer> get_hw_stats, 
.modify_device <token> modify_device, <answer> = 
.port_groups = <token> <answer> hfi1_attr_port_groups, 
int hfi1_register_ib_device(struct hfi1_devdata <token> <answer> *dd) 
struct <token> *dev = &dd->verbs_dev; <answer> hfi1_ibdev 
struct ib_device <token> = &dev->rdi.ibdev; <answer> *ibdev 
struct hfi1_pportdata *ppd = <token> <answer> dd->pport; 
struct hfi1_ibport *ibp = <token> <answer> &ppd->ibport_data; 
<token> i; <answer> unsigned 
int <token> <answer> ret; 
<token> (i = 0; i < dd->num_pports; i++) <answer> for 
<token> + i); <answer> init_ibport(ppd 
if <token> <answer> (!ib_hfi1_sys_image_guid) 
<token> = ibdev->node_guid; <answer> ib_hfi1_sys_image_guid 
<token> = dd->num_pports; <answer> ibdev->phys_port_cnt 
<token> = &dd->pcidev->dev; <answer> ibdev->dev.parent 
ib_set_device_ops(ibdev, <token> <answer> &hfi1_dev_ops); 
<token> init_utsname()->nodename, <answer> strscpy(ibdev->node_desc, 
dd->verbs_dev.rdi.driver_f.get_pci_dev = <token> <answer> get_pci_dev; 
dd->verbs_dev.rdi.driver_f.check_ah = <token> <answer> hfi1_check_ah; 
dd->verbs_dev.rdi.driver_f.notify_new_ah = <token> <answer> hfi1_notify_new_ah; 
dd->verbs_dev.rdi.driver_f.get_guid_be <token> hfi1_get_guid_be; <answer> = 
<token> = query_port; <answer> dd->verbs_dev.rdi.driver_f.query_port_state 
dd->verbs_dev.rdi.driver_f.shut_down_port = <token> <answer> shut_down_port; 
dd->verbs_dev.rdi.driver_f.cap_mask_chg <token> hfi1_cap_mask_chg; <answer> = 
<token> <linux/compiler.h> <answer> #include 
<token> <linux/proc_fs.h> <answer> #include 
#include <token> <answer> <linux/f2fs_fs.h> 
<token> <linux/seq_file.h> <answer> #include 
<token> <linux/unicode.h> <answer> #include 
<token> <linux/ioprio.h> <answer> #include 
<token> <linux/sysfs.h> <answer> #include 
<token> "f2fs.h" <answer> #include 
#include <token> <answer> "segment.h" 
#include <token> <answer> "gc.h" 
#include <token> <answer> "iostat.h" 
<token> <trace/events/f2fs.h> <answer> #include 
static struct proc_dir_entry <token> <answer> *f2fs_proc_root; 
static ssize_t f2fs_feature_show(struct f2fs_attr <token> <answer> *a, 
struct <token> *sbi, char *buf) <answer> f2fs_sb_info 
return <token> "supported\n"); <answer> sysfs_emit(buf, 
#define <token> \ <answer> F2FS_FEATURE_RO_ATTR(_name) 
static struct <token> f2fs_attr_##_name = { \ <answer> f2fs_attr 
.attr = {.name <token> __stringify(_name), .mode = 0444 }, \ <answer> = 
.show <token> f2fs_feature_show, \ <answer> = 
static <token> f2fs_sb_feature_show(struct f2fs_attr *a, <answer> ssize_t 
struct <token> *sbi, char *buf) <answer> f2fs_sb_info 
<token> (F2FS_HAS_FEATURE(sbi, a->id)) <answer> if 
<token> sysfs_emit(buf, "supported\n"); <answer> return 
<token> sysfs_emit(buf, "unsupported\n"); <answer> return 
#define F2FS_SB_FEATURE_RO_ATTR(_name, _feat) <token> <answer> \ 
static struct f2fs_attr f2fs_attr_sb_##_name <token> { \ <answer> = 
.attr <token> {.name = __stringify(_name), .mode = 0444 }, \ <answer> = 
<token> = f2fs_sb_feature_show, \ <answer> .show 
.id <token> F2FS_FEATURE_##_feat, \ <answer> = 
#define F2FS_ATTR_OFFSET(_struct_type, <token> _mode, _show, _store, _offset) \ <answer> _name, 
static struct <token> f2fs_attr_##_name = { \ <answer> f2fs_attr 
.attr <token> {.name = __stringify(_name), .mode = _mode }, \ <answer> = 
.show = _show, <token> <answer> \ 
<token> = _store, \ <answer> .store 
.struct_type <token> _struct_type, \ <answer> = 
.offset = _offset <token> <answer> \ 
#define <token> struct_name, name, elname) \ <answer> F2FS_RO_ATTR(struct_type, 
F2FS_ATTR_OFFSET(struct_type, <token> 0444, \ <answer> name, 
<token> NULL, \ <answer> f2fs_sbi_show, 
<token> struct_name, elname)) <answer> offsetof(struct 
<token> F2FS_RW_ATTR(struct_type, struct_name, name, elname) \ <answer> #define 
<token> name, 0644, \ <answer> F2FS_ATTR_OFFSET(struct_type, 
f2fs_sbi_show, <token> \ <answer> f2fs_sbi_store, 
offsetof(struct struct_name, <token> <answer> elname)) 
<token> F2FS_GENERAL_RO_ATTR(name) \ <answer> #define 
static struct f2fs_attr f2fs_attr_##name = __ATTR(name, 0444, name##_show, <token> <answer> NULL) 
<token> CONFIG_F2FS_STAT_FS <answer> #ifdef 
#define <token> elname) \ <answer> STAT_INFO_RO_ATTR(name, 
F2FS_RO_ATTR(STAT_INFO, f2fs_stat_info, name, <token> <answer> elname) 
#define GC_THREAD_RW_ATTR(name, elname) <token> <answer> \ 
<token> f2fs_gc_kthread, name, elname) <answer> F2FS_RW_ATTR(GC_THREAD, 
#define SM_INFO_RW_ATTR(name, elname) <token> <answer> \ 
F2FS_RW_ATTR(SM_INFO, f2fs_sm_info, name, <token> <answer> elname) 
#define <token> \ <answer> SM_INFO_GENERAL_RW_ATTR(elname) 
SM_INFO_RW_ATTR(elname, <token> <answer> elname) 
#define <token> elname) \ <answer> DCC_INFO_RW_ATTR(name, 
F2FS_RW_ATTR(DCC_INFO, <token> name, elname) <answer> discard_cmd_control, 
<token> DCC_INFO_GENERAL_RW_ATTR(elname) \ <answer> #define 
<token> elname) <answer> DCC_INFO_RW_ATTR(elname, 
<token> NM_INFO_RW_ATTR(name, elname) \ <answer> #define 
F2FS_RW_ATTR(NM_INFO, f2fs_nm_info, name, <token> <answer> elname) 
#define <token> \ <answer> NM_INFO_GENERAL_RW_ATTR(elname) 
NM_INFO_RW_ATTR(elname, <token> <answer> elname) 
#define F2FS_SBI_RW_ATTR(name, elname) <token> <answer> \ 
<token> f2fs_sb_info, name, elname) <answer> F2FS_RW_ATTR(F2FS_SBI, 
#define <token> \ <answer> F2FS_SBI_GENERAL_RW_ATTR(elname) 
<token> elname) <answer> F2FS_SBI_RW_ATTR(elname, 
<token> F2FS_SBI_GENERAL_RO_ATTR(elname) \ <answer> #define 
F2FS_RO_ATTR(F2FS_SBI, <token> elname, elname) <answer> f2fs_sb_info, 
#ifdef <token> <answer> CONFIG_F2FS_FAULT_INJECTION 
#define FAULT_INFO_GENERAL_RW_ATTR(type, <token> \ <answer> elname) 
<token> f2fs_fault_info, elname, elname) <answer> F2FS_RW_ATTR(type, 
#define RESERVED_BLOCKS_GENERAL_RW_ATTR(elname) <token> <answer> \ 
F2FS_RW_ATTR(RESERVED_BLOCKS, <token> elname, elname) <answer> f2fs_sb_info, 
<token> CPRC_INFO_GENERAL_RW_ATTR(elname) \ <answer> #define 
F2FS_RW_ATTR(CPRC_INFO, ckpt_req_control, <token> elname) <answer> elname, 
#define ATGC_INFO_RW_ATTR(name, <token> \ <answer> elname) 
F2FS_RW_ATTR(ATGC_INFO, <token> name, elname) <answer> atgc_management, 
#include <token> <answer> <linux/init.h> 
<token> <linux/interrupt.h> <answer> #include 
#include <token> <answer> <linux/slab.h> 
#include <token> <answer> <linux/errno.h> 
#include <token> <answer> <linux/module.h> 
#include <token> <answer> <linux/device.h> 
#include <token> <answer> <linux/platform_device.h> 
<token> <linux/resource.h> <answer> #include 
#include <token> <answer> <linux/spi/spi.h> 
#include <token> <answer> <linux/spi/spi_bitbang.h> 
<token> <linux/dma-mapping.h> <answer> #include 
<token> <linux/completion.h> <answer> #include 
<token> <asm/mach-au1x00/au1000.h> <answer> #include 
<token> <asm/mach-au1x00/au1xxx_psc.h> <answer> #include 
#include <token> <answer> <asm/mach-au1x00/au1xxx_dbdma.h> 
<token> <asm/mach-au1x00/au1550_spi.h> <answer> #include 
static unsigned int usedma <token> 1; <answer> = 
<token> uint, 0644); <answer> module_param(usedma, 
#define AU1550_SPI_DBDMA_DESCRIPTORS <token> <answer> 1 
#define <token> 2048U <answer> AU1550_SPI_DMA_RXTMP_MINSIZE 
struct au1550_spi <token> <answer> { 
struct spi_bitbang <token> <answer> bitbang; 
volatile psc_spi_t __iomem <token> <answer> *regs; 
<token> irq; <answer> int 
unsigned int <token> <answer> len; 
<token> int tx_count; <answer> unsigned 
unsigned int <token> <answer> rx_count; 
<token> u8 *tx; <answer> const 
<token> *rx; <answer> u8 
void <token> au1550_spi *hw); <answer> (*rx_word)(struct 
void (*tx_word)(struct <token> *hw); <answer> au1550_spi 
int (*txrx_bufs)(struct spi_device <token> struct spi_transfer *t); <answer> *spi, 
irqreturn_t (*irq_callback)(struct <token> *hw); <answer> au1550_spi 
struct completion <token> <answer> host_done; 
<token> int usedma; <answer> unsigned 
<token> dma_tx_id; <answer> u32 
<token> dma_rx_id; <answer> u32 
<token> dma_tx_ch; <answer> u32 
u32 <token> <answer> dma_rx_ch; 
u8 <token> <answer> *dma_rx_tmpbuf; 
<token> int dma_rx_tmpbuf_size; <answer> unsigned 
u32 <token> <answer> dma_rx_tmpbuf_addr; 
struct spi_controller <token> <answer> *host; 
<token> device *dev; <answer> struct 
struct au1550_spi_info <token> <answer> *pdata; 
<token> resource *ioarea; <answer> struct 
static <token> au1550_spi_baudcfg(struct au1550_spi *hw, unsigned int speed_hz) <answer> u32 
u32 <token> = hw->pdata->mainclk_hz; <answer> mainclk_hz 
u32 <token> brg; <answer> div, 
for (div = 0; div < 4; div++) <token> <answer> { 
brg = mainclk_hz / speed_hz / (4 << <token> <answer> div); 
static void au1550_spi_chipsel(struct <token> *spi, int value) <answer> spi_device 
struct au1550_spi *hw = <token> <answer> spi_controller_get_devdata(spi->controller); 
unsigned int cspol = spi->mode & SPI_CS_HIGH ? 1 <token> 0; <answer> : 
u32 cfg, <token> <answer> stat; 
<token> (value) { <answer> switch 
case <token> <answer> BITBANG_CS_INACTIVE: 
<token> (hw->pdata->deactivate_cs) <answer> if 
hw->pdata->deactivate_cs(hw->pdata, <token> 0), <answer> spi_get_chipselect(spi, 
case <token> <answer> BITBANG_CS_ACTIVE: 
<token> spi->bits_per_word); <answer> au1550_spi_bits_handlers_set(hw, 
<token> = hw->regs->psc_spicfg; <answer> cfg 
<token> int au1550_spi_dma_rxtmp_alloc(struct au1550_spi *hw, unsigned int size) <answer> static 
hw->dma_rx_tmpbuf = <token> GFP_KERNEL); <answer> kmalloc(size, 
<token> (!hw->dma_rx_tmpbuf) <answer> if 
return <token> <answer> -ENOMEM; 
hw->dma_rx_tmpbuf_size = <token> <answer> size; 
<token> = dma_map_single(hw->dev, hw->dma_rx_tmpbuf, <answer> hw->dma_rx_tmpbuf_addr 
size, <token> <answer> DMA_FROM_DEVICE); 
if (dma_mapping_error(hw->dev, <token> { <answer> hw->dma_rx_tmpbuf_addr)) 
hw->dma_rx_tmpbuf <token> 0; <answer> = 
<token> = 0; <answer> hw->dma_rx_tmpbuf_size 
<token> -EFAULT; <answer> return 
<token> 0; <answer> return 
static void au1550_spi_dma_rxtmp_free(struct au1550_spi <token> <answer> *hw) 
<token> hw->dma_rx_tmpbuf_addr, <answer> dma_unmap_single(hw->dev, 
<token> DMA_FROM_DEVICE); <answer> hw->dma_rx_tmpbuf_size, 
<token> = 0; <answer> hw->dma_rx_tmpbuf 
<token> = 0; <answer> hw->dma_rx_tmpbuf_size 
static int <token> spi_device *spi, struct spi_transfer *t) <answer> au1550_spi_dma_txrxb(struct 
struct au1550_spi <token> = spi_controller_get_devdata(spi->controller); <answer> *hw 
<token> dma_tx_addr; <answer> dma_addr_t 
<token> dma_rx_addr; <answer> dma_addr_t 
u32 <token> <answer> res; 
<token> = t->len; <answer> hw->len 
hw->tx_count <token> 0; <answer> = 
hw->rx_count = <token> <answer> 0; 
<token> = t->tx_buf; <answer> hw->tx 
hw->rx = <token> <answer> t->rx_buf; 
dma_tx_addr = <token> <answer> t->tx_dma; 
dma_rx_addr = <token> <answer> t->rx_dma; 
<token> (t->tx_buf) { <answer> if 
"pio transfer: unexpected SPI error (event=0x%x <token> <answer> stat=0x%x)!\n", 
<token> stat); <answer> evnt, 
return <token> <answer> IRQ_HANDLED; 
<token> { <answer> do 
<token> = 0; <answer> busy 
stat <token> hw->regs->psc_spistat; <answer> = 
if (!(stat & PSC_SPISTAT_RE) && hw->rx_count < <token> { <answer> hw->len) 
busy = <token> <answer> 1; 
if (!(stat & <token> && hw->tx_count < hw->len) <answer> PSC_SPISTAT_TF) 
<token> while (busy); <answer> } 
hw->regs->psc_spievent = <token> | PSC_SPIEVNT_TR; <answer> PSC_SPIEVNT_RR 
if (evnt <token> PSC_SPIEVNT_TU) { <answer> & 
hw->regs->psc_spievent = PSC_SPIEVNT_TU <token> PSC_SPIEVNT_MD; <answer> | 
int min_div = (2 << 0) * (2 * (4 <token> 1)); <answer> + 
<token> max_div = (2 << 3) * (2 * (63 + 1)); <answer> int 
<token> = hw->pdata->mainclk_hz / min_div; <answer> host->max_speed_hz 
host->min_speed_hz <token> <answer> = 
hw->pdata->mainclk_hz / (max_div + <token> + 1; <answer> 1) 
err = <token> <answer> spi_bitbang_start(&hw->bitbang); 
if <token> { <answer> (err) 
dev_err(&pdev->dev, "Failed <token> register SPI host\n"); <answer> to 
goto <token> <answer> err_register; 
"spi <token> registered: bus_num=%d num_chipselect=%d\n", <answer> host 
host->bus_num, <token> <answer> host->num_chipselect); 
<token> 0; <answer> return 
<token> hw); <answer> free_irq(hw->irq, 
<token> (hw->usedma) <answer> if 
if <token> <answer> (hw->usedma) 
iounmap((void __iomem <token> <answer> *)hw->regs); 
release_mem_region(r->start, <token> <answer> sizeof(psc_spi_t)); 
<token> err; <answer> return 
static void au1550_spi_remove(struct <token> *pdev) <answer> platform_device 
struct au1550_spi <token> = platform_get_drvdata(pdev); <answer> *hw 
dev_info(&pdev->dev, "spi <token> remove: bus_num=%d\n", <answer> host 
<token> hw); <answer> free_irq(hw->irq, 
iounmap((void __iomem <token> <answer> *)hw->regs); 
release_mem_region(hw->ioarea->start, <token> <answer> sizeof(psc_spi_t)); 
if <token> { <answer> (hw->usedma) 
switch (alchemy_get_cputype()) <token> <answer> { 
<token> ALCHEMY_CPU_AU1550: <answer> case 
case <token> <answer> ALCHEMY_CPU_AU1200: 
case <token> <answer> ALCHEMY_CPU_AU1300: 
return <token> <answer> -ENODEV; 
if (usedma) <token> <answer> { 
<token> = au1xxx_ddma_add_device(&au1550_spi_mem_dbdev); <answer> ddma_memid 
if <token> <answer> (!ddma_memid) 
<token> "au1550-spi: cannot add memory dbdma device\n"); <answer> printk(KERN_ERR 
<token> platform_driver_register(&au1550_spi_drv); <answer> return 
static void __exit <token> <answer> au1550_spi_exit(void) 
if (usedma && <token> <answer> ddma_memid) 
<token> PSC SPI Driver"); <answer> MODULE_DESCRIPTION("Au1550 
MODULE_AUTHOR("Jan <token> <jan.nikitenko@gmail.com>"); <answer> Nikitenko 
#define pr_fmt(fmt) KBUILD_MODNAME ": <token> fmt <answer> " 
#include <token> <answer> <linux/types.h> 
<token> <linux/module.h> <answer> #include 
<token> <net/ip.h> <answer> #include 
#include <token> <answer> <linux/ipv6.h> 
#include <token> <answer> <linux/icmp.h> 
#include <token> <answer> <net/ipv6.h> 
<token> <net/tcp.h> <answer> #include 
#include <token> <answer> <net/udp.h> 
#include <token> <answer> <linux/netfilter/x_tables.h> 
#include <token> <answer> <linux/netfilter/xt_tcpudp.h> 
#include <token> <answer> <linux/netfilter_ipv4/ip_tables.h> 
<token> <linux/netfilter_ipv6/ip6_tables.h> <answer> #include 
MODULE_DESCRIPTION("Xtables: TCP, UDP and <token> match"); <answer> UDP-Lite 
if (par->fragoff == 1) <token> <answer> { 
pr_debug("Dropping evil TCP <token> frag.\n"); <answer> offset=1 
<token> = true; <answer> par->hotdrop 
pr_debug("Dropping <token> TCP offset=0 tinygram.\n"); <answer> evil 
par->hotdrop = <token> <answer> true; 
<token> false; <answer> return 
if <token> tcpinfo->spts[1], <answer> (!port_match(tcpinfo->spts[0], 
!!(tcpinfo->invflags & <token> <answer> XT_TCP_INV_SRCPT))) 
<token> false; <answer> return 
<token> (!port_match(tcpinfo->dpts[0], tcpinfo->dpts[1], <answer> if 
!!(tcpinfo->invflags <token> XT_TCP_INV_DSTPT))) <answer> & 
<token> false; <answer> return 
if (!NF_INVF(tcpinfo, <token> <answer> XT_TCP_INV_FLAGS, 
(((unsigned char *)th)[13] & <token> == tcpinfo->flg_cmp)) <answer> tcpinfo->flg_mask) 
<token> false; <answer> return 
<token> (tcpinfo->option) { <answer> if 
if (th->doff * 4 < sizeof(_tcph)) <token> <answer> { 
par->hotdrop = <token> <answer> true; 
return <token> <answer> false; 
<token> (!tcp_find_option(tcpinfo->option, skb, par->thoff, <answer> if 
<token> - sizeof(_tcph), <answer> th->doff*4 
tcpinfo->invflags <token> XT_TCP_INV_OPTION, <answer> & 
<token> false; <answer> return 
return <token> <answer> true; 
static int <token> struct xt_mtchk_param *par) <answer> tcp_mt_check(const 
const struct xt_tcp <token> = par->matchinfo; <answer> *tcpinfo 
pr_debug("Dropping <token> UDP tinygram.\n"); <answer> evil 
par->hotdrop = <token> <answer> true; 
return <token> <answer> false; 
<token> port_match(udpinfo->spts[0], udpinfo->spts[1], <answer> return 
!!(udpinfo->invflags & <token> <answer> XT_UDP_INV_SRCPT)) 
&& <token> udpinfo->dpts[1], <answer> port_match(udpinfo->dpts[0], 
!!(udpinfo->invflags <token> XT_UDP_INV_DSTPT)); <answer> & 
static <token> udp_mt_check(const struct xt_mtchk_param *par) <answer> int 
const struct xt_udp *udpinfo <token> par->matchinfo; <answer> = 
<token> = true; <answer> par->hotdrop 
return <token> <answer> false; 
<token> icmp_type_code_match(icmpinfo->type, <answer> return 
<token> ic->code, <answer> ic->type, 
!!(icmpinfo->invflags <token> IPT_ICMP_INV)); <answer> & 
<token> bool <answer> static 
icmp6_match(const struct sk_buff *skb, struct xt_action_param <token> <answer> *par) 
const <token> icmp6hdr *ic; <answer> struct 
struct icmp6hdr <token> <answer> _icmph; 
const struct ip6t_icmp <token> = par->matchinfo; <answer> *icmpinfo 
<token> = true; <answer> par->hotdrop 
return <token> <answer> false; 
<token> icmp6_type_code_match(icmpinfo->type, <answer> return 
ic->icmp6_type, <token> <answer> ic->icmp6_code, 
<token> & IP6T_ICMP_INV)); <answer> !!(icmpinfo->invflags 
static int <token> struct xt_mtchk_param *par) <answer> icmp_checkentry(const 
const <token> ipt_icmp *icmpinfo = par->matchinfo; <answer> struct 
return (icmpinfo->invflags & ~IPT_ICMP_INV) <token> -EINVAL : 0; <answer> ? 
static int icmp6_checkentry(const <token> xt_mtchk_param *par) <answer> struct 
const struct ip6t_icmp <token> = par->matchinfo; <answer> *icmpinfo 
return (icmpinfo->invflags & ~IP6T_ICMP_INV) ? -EINVAL : <token> <answer> 0; 
static struct xt_match tcpudp_mt_reg[] <token> = { <answer> __read_mostly 
.name = <token> <answer> "tcp", 
<token> = NFPROTO_IPV4, <answer> .family 
<token> = tcp_mt_check, <answer> .checkentry 
.match = <token> <answer> tcp_mt, 
<token> = sizeof(struct xt_tcp), <answer> .matchsize 
.proto = <token> <answer> IPPROTO_TCP, 
.me = <token> <answer> THIS_MODULE, 
.name = <token> <answer> "tcp", 
.family <token> NFPROTO_IPV6, <answer> = 
<token> = tcp_mt_check, <answer> .checkentry 
<token> = tcp_mt, <answer> .match 
.matchsize <token> sizeof(struct xt_tcp), <answer> = 
.proto <token> IPPROTO_TCP, <answer> = 
.me <token> THIS_MODULE, <answer> = 
.name = <token> <answer> "udp", 
<token> = NFPROTO_IPV4, <answer> .family 
.checkentry = <token> <answer> udp_mt_check, 
.match = <token> <answer> udp_mt, 
.matchsize = <token> xt_udp), <answer> sizeof(struct 
.proto = <token> <answer> IPPROTO_UDP, 
<token> = THIS_MODULE, <answer> .me 
.name <token> "udp", <answer> = 
.family <token> NFPROTO_IPV6, <answer> = 
<token> = udp_mt_check, <answer> .checkentry 
<token> = udp_mt, <answer> .match 
<token> = sizeof(struct xt_udp), <answer> .matchsize 
.proto <token> IPPROTO_UDP, <answer> = 
<token> = THIS_MODULE, <answer> .me 
<token> = "udplite", <answer> .name 
<token> = NFPROTO_IPV4, <answer> .family 
.checkentry = <token> <answer> udp_mt_check, 
.match <token> udp_mt, <answer> = 
<token> = sizeof(struct xt_udp), <answer> .matchsize 
.proto <token> IPPROTO_UDPLITE, <answer> = 
.me <token> THIS_MODULE, <answer> = 
.name = <token> <answer> "udplite", 
.family = <token> <answer> NFPROTO_IPV6, 
.checkentry = <token> <answer> udp_mt_check, 
.match <token> udp_mt, <answer> = 
.matchsize = sizeof(struct <token> <answer> xt_udp), 
.proto = <token> <answer> IPPROTO_UDPLITE, 
.me <token> THIS_MODULE, <answer> = 
.name = <token> <answer> "icmp", 
.match = <token> <answer> icmp_match, 
.matchsize <token> sizeof(struct ipt_icmp), <answer> = 
<token> = icmp_checkentry, <answer> .checkentry 
<token> = IPPROTO_ICMP, <answer> .proto 
.family = <token> <answer> NFPROTO_IPV4, 
.me <token> THIS_MODULE, <answer> = 
.name <token> "icmp6", <answer> = 
.match = <token> <answer> icmp6_match, 
.matchsize = <token> ip6t_icmp), <answer> sizeof(struct 
.checkentry <token> icmp6_checkentry, <answer> = 
<token> = IPPROTO_ICMPV6, <answer> .proto 
.family = <token> <answer> NFPROTO_IPV6, 
.me = <token> <answer> THIS_MODULE, 
static int <token> tcpudp_mt_init(void) <answer> __init 
return xt_register_matches(tcpudp_mt_reg, <token> <answer> ARRAY_SIZE(tcpudp_mt_reg)); 
static void <token> tcpudp_mt_exit(void) <answer> __exit 
<token> ARRAY_SIZE(tcpudp_mt_reg)); <answer> xt_unregister_matches(tcpudp_mt_reg, 
<token> pr_fmt(fmt) KBUILD_MODNAME ": " fmt <answer> #define 
#include <token> <answer> <linux/module.h> 
#include <token> <answer> <linux/skbuff.h> 
<token> <linux/netfilter/x_tables.h> <answer> #include 
<token> <linux/netfilter/xt_CONNSECMARK.h> <answer> #include 
<token> <net/netfilter/nf_conntrack.h> <answer> #include 
<token> <net/netfilter/nf_conntrack_ecache.h> <answer> #include 
MODULE_AUTHOR("James <token> <jmorris@redhat.com>"); <answer> Morris 
MODULE_DESCRIPTION("Xtables: target for <token> between connection and security mark"); <answer> copying 
static void secmark_save(const struct <token> *skb) <answer> sk_buff 
if <token> { <answer> (skb->secmark) 
<token> nf_conn *ct; <answer> struct 
enum <token> ctinfo; <answer> ip_conntrack_info 
<token> = nf_ct_get(skb, &ctinfo); <answer> ct 
if <token> && !ct->secmark) { <answer> (ct 
ct->secmark <token> skb->secmark; <answer> = 
<token> ct); <answer> nf_conntrack_event_cache(IPCT_SECMARK, 
static <token> secmark_restore(struct sk_buff *skb) <answer> void 
if (!skb->secmark) <token> <answer> { 
const <token> nf_conn *ct; <answer> struct 
<token> ip_conntrack_info ctinfo; <answer> enum 
ct <token> nf_ct_get(skb, &ctinfo); <answer> = 
if <token> && ct->secmark) <answer> (ct 
<token> = ct->secmark; <answer> skb->secmark 
<token> unsigned int <answer> static 
<token> sk_buff *skb, const struct xt_action_param *par) <answer> connsecmark_tg(struct 
const struct <token> *info = par->targinfo; <answer> xt_connsecmark_target_info 
switch (info->mode) <token> <answer> { 
case <token> <answer> CONNSECMARK_SAVE: 
<token> CONNSECMARK_RESTORE: <answer> case 
return <token> <answer> XT_CONTINUE; 
static int <token> struct xt_tgchk_param *par) <answer> connsecmark_tg_check(const 
const struct xt_connsecmark_target_info <token> = par->targinfo; <answer> *info 
<token> ret; <answer> int 
if (strcmp(par->table, "mangle") <token> 0 && <answer> != 
strcmp(par->table, <token> != 0) { <answer> "security") 
<token> valid in \'mangle\' or \'security\' table, not \'%s\'\n", <answer> pr_info_ratelimited("only 
return <token> <answer> -EINVAL; 
switch <token> { <answer> (info->mode) 
<token> CONNSECMARK_SAVE: <answer> case 
<token> CONNSECMARK_RESTORE: <answer> case 
pr_info_ratelimited("invalid mode: <token> info->mode); <answer> %hu\n", 
return <token> <answer> -EINVAL; 
ret = nf_ct_netns_get(par->net, <token> <answer> par->family); 
if <token> < 0) <answer> (ret 
<token> load conntrack support for proto=%u\n", <answer> pr_info_ratelimited("cannot 
<token> ret; <answer> return 
static void connsecmark_tg_destroy(const <token> xt_tgdtor_param *par) <answer> struct 
<token> par->family); <answer> nf_ct_netns_put(par->net, 
static struct xt_target <token> __read_mostly = { <answer> connsecmark_tg_reg 
<token> = "CONNSECMARK", <answer> .name 
.revision <token> 0, <answer> = 
.family <token> NFPROTO_UNSPEC, <answer> = 
.checkentry <token> connsecmark_tg_check, <answer> = 
<token> = connsecmark_tg_destroy, <answer> .destroy 
.target = <token> <answer> connsecmark_tg, 
.targetsize = sizeof(struct <token> <answer> xt_connsecmark_target_info), 
.me <token> THIS_MODULE, <answer> = 
static int <token> connsecmark_tg_init(void) <answer> __init 
return <token> <answer> xt_register_target(&connsecmark_tg_reg); 
static void <token> connsecmark_tg_exit(void) <answer> __exit 
<token> <linux/clk.h> <answer> #include 
<token> <linux/of.h> <answer> #include 
<token> <linux/pm_runtime.h> <answer> #include 
<token> "mtk_vcodec_enc_drv.h" <answer> #include 
<token> "mtk_vcodec_enc_pm.h" <answer> #include 
int mtk_vcodec_init_enc_clk(struct mtk_vcodec_enc_dev <token> <answer> *mtkdev) 
struct platform_device <token> <answer> *pdev; 
<token> mtk_vcodec_pm *pm; <answer> struct 
struct <token> *enc_clk; <answer> mtk_vcodec_clk 
struct mtk_vcodec_clk_info <token> <answer> *clk_info; 
<token> ret, i; <answer> int 
<token> = mtkdev->plat_dev; <answer> pdev 
pm <token> &mtkdev->pm; <answer> = 
memset(pm, 0, <token> mtk_vcodec_pm)); <answer> sizeof(struct 
pm->dev <token> &pdev->dev; <answer> = 
enc_clk <token> &pm->venc_clk; <answer> = 
enc_clk->clk_num <token> of_property_count_strings(pdev->dev.of_node, <answer> = 
if (enc_clk->clk_num <token> 0) { <answer> > 
enc_clk->clk_info = <token> <answer> devm_kcalloc(&pdev->dev, 
enc_clk->clk_num, <token> <answer> sizeof(*clk_info), 
if <token> <answer> (!enc_clk->clk_info) 
return <token> <answer> -ENOMEM; 
} else <token> <answer> { 
dev_err(pm->dev, "[MTK VCODEC] Failed to get venc <token> count"); <answer> clock 
<token> -EINVAL; <answer> return 
for (i = 0; i < <token> i++) { <answer> enc_clk->clk_num; 
clk_info <token> &enc_clk->clk_info[i]; <answer> = 
ret <token> of_property_read_string_index(pdev->dev.of_node, <answer> = 
<token> i, &clk_info->clk_name); <answer> "clock-names", 
if (ret) <token> <answer> { 
dev_err(pm->dev, "[MTK VCODEC] venc failed to get clk <token> %d", i); <answer> name 
<token> ret; <answer> return 
clk_info->vcodec_clk = <token> <answer> devm_clk_get(&pdev->dev, 
if <token> { <answer> (IS_ERR(clk_info->vcodec_clk)) 
dev_err(pm->dev, <token> VCODEC] venc devm_clk_get (%d)%s fail", i, <answer> "[MTK 
<token> PTR_ERR(clk_info->vcodec_clk); <answer> return 
return <token> <answer> 0; 
void <token> mtk_vcodec_pm *pm) <answer> mtk_vcodec_enc_pw_on(struct 
<token> ret; <answer> int 
ret = <token> <answer> pm_runtime_resume_and_get(pm->dev); 
<token> (ret) <answer> if 
dev_err(pm->dev, "pm_runtime_resume_and_get fail: %d", <token> <answer> ret); 
void mtk_vcodec_enc_pw_off(struct mtk_vcodec_pm <token> <answer> *pm) 
int <token> <answer> ret; 
ret <token> pm_runtime_put(pm->dev); <answer> = 
if (ret <token> ret != -EAGAIN) <answer> && 
dev_err(pm->dev, "pm_runtime_put fail <token> ret); <answer> %d", 
void <token> mtk_vcodec_pm *pm) <answer> mtk_vcodec_enc_clock_on(struct 
struct mtk_vcodec_clk *enc_clk = <token> <answer> &pm->venc_clk; 
int <token> i = 0; <answer> ret, 
for (i = <token> i < enc_clk->clk_num; i++) { <answer> 0; 
ret = <token> <answer> clk_prepare_enable(enc_clk->clk_info[i].vcodec_clk); 
<token> (ret) { <answer> if 
dev_err(pm->dev, "[MTK <token> venc clk_prepare_enable %d %s fail %d", i, <answer> VCODEC] 
enc_clk->clk_info[i].clk_name, <token> <answer> ret); 
goto <token> <answer> clkerr; 
for (i -= 1; i >= 0; <token> <answer> i--) 
void <token> mtk_vcodec_pm *pm) <answer> mtk_vcodec_enc_clock_off(struct 
struct mtk_vcodec_clk *enc_clk = <token> <answer> &pm->venc_clk; 
<token> i = 0; <answer> int 
for (i = <token> - 1; i >= 0; i--) <answer> enc_clk->clk_num 
#include <token> <answer> <linux/atomic.h> 
<token> <linux/bitmap.h> <answer> #include 
<token> <linux/kvm_host.h> <answer> #include 
<token> <linux/math.h> <answer> #include 
#include <token> <answer> <linux/spinlock.h> 
#include <token> <answer> <linux/swab.h> 
<token> <kvm/iodev.h> <answer> #include 
#include <token> <answer> <asm/csr.h> 
<token> <asm/kvm_aia_imsic.h> <answer> #include 
#define IMSIC_MAX_EIX <token> / BITS_PER_TYPE(u64)) <answer> (IMSIC_MAX_ID 
struct imsic_mrif_eix <token> <answer> { 
unsigned long eip[BITS_PER_TYPE(u64) / <token> <answer> BITS_PER_LONG]; 
unsigned long eie[BITS_PER_TYPE(u64) <token> BITS_PER_LONG]; <answer> / 
<token> imsic_mrif { <answer> struct 
struct <token> eix[IMSIC_MAX_EIX]; <answer> imsic_mrif_eix 
unsigned <token> eithreshold; <answer> long 
unsigned long <token> <answer> eidelivery; 
struct <token> { <answer> imsic 
struct <token> iodev; <answer> kvm_io_device 
u32 <token> <answer> nr_msis; 
u32 <token> <answer> nr_eix; 
<token> nr_hw_eix; <answer> u32 
if <token> { <answer> (idata->clear) 
mrif->eidelivery <token> imsic_vs_csr_swap(IMSIC_EIDELIVERY, 0); <answer> = 
<token> = imsic_vs_csr_swap(IMSIC_EITHRESHOLD, 0); <answer> mrif->eithreshold 
for (i <token> 0; i < idata->nr_eix; i++) { <answer> = 
<token> = &mrif->eix[i]; <answer> eix 
eix->eip[0] = <token> + i * 2, 0); <answer> imsic_eix_swap(IMSIC_EIP0 
eix->eie[0] <token> imsic_eix_swap(IMSIC_EIE0 + i * 2, 0); <answer> = 
<token> CONFIG_32BIT <answer> #ifdef 
eix->eip[1] = imsic_eix_swap(IMSIC_EIP0 + i * 2 + 1, <token> <answer> 0); 
eix->eie[1] = imsic_eix_swap(IMSIC_EIE0 <token> i * 2 + 1, 0); <answer> + 
} else <token> <answer> { 
mrif->eidelivery = <token> <answer> imsic_vs_csr_read(IMSIC_EIDELIVERY); 
<token> = imsic_vs_csr_read(IMSIC_EITHRESHOLD); <answer> mrif->eithreshold 
for <token> = 0; i < idata->nr_eix; i++) { <answer> (i 
eix <token> &mrif->eix[i]; <answer> = 
eix->eip[0] <token> imsic_eix_read(IMSIC_EIP0 + i * 2); <answer> = 
eix->eie[0] = imsic_eix_read(IMSIC_EIE0 + i <token> 2); <answer> * 
<token> CONFIG_32BIT <answer> #ifdef 
<token> = imsic_eix_read(IMSIC_EIP0 + i * 2 + 1); <answer> eix->eip[1] 
eix->eie[1] = imsic_eix_read(IMSIC_EIE0 + i * <token> + 1); <answer> 2 
<token> old_hstatus); <answer> csr_write(CSR_HSTATUS, 
csr_write(CSR_VSISELECT, <token> <answer> old_vsiselect); 
static <token> imsic_vsfile_read(int vsfile_hgei, int vsfile_cpu, u32 nr_eix, <answer> void 
<token> clear, struct imsic_mrif *mrif) <answer> bool 
struct imsic_vsfile_read_data <token> <answer> idata; 
old_vsiselect <token> csr_read(CSR_VSISELECT); <answer> = 
old_hstatus <token> csr_read(CSR_HSTATUS); <answer> = 
new_hstatus <token> old_hstatus & ~HSTATUS_VGEIN; <answer> = 
new_hstatus <token> ((unsigned long)vsfile_hgei) << HSTATUS_VGEIN_SHIFT; <answer> |= 
csr_write(CSR_HSTATUS, <token> <answer> new_hstatus); 
for (i <token> 0; i < nr_eix; i++) { <answer> = 
eix = <token> <answer> &mrif->eix[i]; 
imsic_eix_set(IMSIC_EIP0 <token> i * 2, eix->eip[0]); <answer> + 
imsic_eix_set(IMSIC_EIE0 + i * <token> eix->eie[0]); <answer> 2, 
#ifdef <token> <answer> CONFIG_32BIT 
imsic_eix_set(IMSIC_EIP0 + i * 2 + <token> eix->eip[1]); <answer> 1, 
imsic_eix_set(IMSIC_EIE0 + i <token> 2 + 1, eix->eie[1]); <answer> * 
<token> mrif->eithreshold); <answer> imsic_vs_csr_write(IMSIC_EITHRESHOLD, 
imsic_vs_csr_write(IMSIC_EIDELIVERY, <token> <answer> mrif->eidelivery); 
<token> old_hstatus); <answer> csr_write(CSR_HSTATUS, 
csr_write(CSR_VSISELECT, <token> <answer> old_vsiselect); 
static void imsic_vsfile_cleanup(struct imsic <token> <answer> *imsic) 
int <token> old_vsfile_cpu; <answer> old_vsfile_hgei, 
<token> long flags; <answer> unsigned 
<token> flags); <answer> write_lock_irqsave(&imsic->vsfile_lock, 
old_vsfile_hgei = <token> <answer> imsic->vsfile_hgei; 
<token> = imsic->vsfile_cpu; <answer> old_vsfile_cpu 
imsic->vsfile_cpu = <token> = -1; <answer> imsic->vsfile_hgei 
<token> = NULL; <answer> imsic->vsfile_va 
<token> = 0; <answer> imsic->vsfile_pa 
<token> flags); <answer> write_unlock_irqrestore(&imsic->vsfile_lock, 
<token> 0, sizeof(*imsic->swfile)); <answer> memset(imsic->swfile, 
<token> (old_vsfile_cpu >= 0) <answer> if 
kvm_riscv_aia_free_hgei(old_vsfile_cpu, <token> <answer> old_vsfile_hgei); 
static void <token> kvm_vcpu *vcpu) <answer> imsic_swfile_extirq_update(struct 
struct imsic <token> = vcpu->arch.aia_context.imsic_state; <answer> *imsic 
struct imsic_mrif <token> = imsic->swfile; <answer> *mrif 
unsigned long <token> <answer> flags; 
spin_lock_irqsave(&imsic->swfile_extirq_lock, <token> <answer> flags); 
<token> (imsic_mrif_atomic_read(mrif, &mrif->eidelivery) && <answer> if 
<token> imsic->nr_eix, imsic->nr_msis)) <answer> imsic_mrif_topei(mrif, 
kvm_riscv_vcpu_set_interrupt(vcpu, <token> <answer> IRQ_VS_EXT); 
kvm_riscv_vcpu_unset_interrupt(vcpu, <token> <answer> IRQ_VS_EXT); 
spin_unlock_irqrestore(&imsic->swfile_extirq_lock, <token> <answer> flags); 
<token> void imsic_swfile_read(struct kvm_vcpu *vcpu, bool clear, <answer> static 
struct <token> *mrif) <answer> imsic_mrif 
struct <token> *imsic = vcpu->arch.aia_context.imsic_state; <answer> imsic 
memcpy(mrif, <token> sizeof(*mrif)); <answer> imsic->swfile, 
if (clear) <token> <answer> { 
memset(imsic->swfile, 0, <token> <answer> sizeof(*imsic->swfile)); 
<token> IRQ_VS_EXT); <answer> kvm_riscv_vcpu_unset_interrupt(vcpu, 
static void imsic_swfile_update(struct <token> *vcpu, <answer> kvm_vcpu 
struct imsic_mrif <token> <answer> *mrif) 
<token> i; <answer> u32 
struct imsic_mrif_eix <token> *eix; <answer> *seix, 
struct <token> *imsic = vcpu->arch.aia_context.imsic_state; <answer> imsic 
struct imsic_mrif <token> = imsic->swfile; <answer> *smrif 
imsic_mrif_atomic_write(smrif, <token> mrif->eidelivery); <answer> &smrif->eidelivery, 
imsic_mrif_atomic_write(smrif, &smrif->eithreshold, <token> <answer> mrif->eithreshold); 
for (i = 0; i < imsic->nr_eix; i++) <token> <answer> { 
<token> = &smrif->eix[i]; <answer> seix 
<token> = &mrif->eix[i]; <answer> eix 
imsic_mrif_atomic_or(smrif, <token> eix->eip[0]); <answer> &seix->eip[0], 
<token> &seix->eie[0], eix->eie[0]); <answer> imsic_mrif_atomic_or(smrif, 
#ifdef <token> <answer> CONFIG_32BIT 
<token> &seix->eip[1], eix->eip[1]); <answer> imsic_mrif_atomic_or(smrif, 
imsic_mrif_atomic_or(smrif, &seix->eie[1], <token> <answer> eix->eie[1]); 
<token> kvm_riscv_vcpu_aia_imsic_release(struct kvm_vcpu *vcpu) <answer> void 
unsigned long <token> <answer> flags; 
<token> imsic_mrif tmrif; <answer> struct 
int <token> old_vsfile_cpu; <answer> old_vsfile_hgei, 
struct imsic *imsic <token> vcpu->arch.aia_context.imsic_state; <answer> = 
memset(&tmrif, <token> sizeof(tmrif)); <answer> 0, 
if (old_vsfile_cpu >= <token> { <answer> 0) 
int <token> ixgbe_hw *hw, u16 *refill, <answer> ixgbe_dcb_config_rx_arbiter_82598(struct 
u16 *max, <token> *prio_type) <answer> u8 
u32 <token> = 0; <answer> reg 
u32 credit_refill <token> 0; <answer> = 
<token> credit_max = 0; <answer> u32 
u8 i <token> 0; <answer> = 
reg = IXGBE_READ_REG(hw, IXGBE_RUPPBMR) <token> IXGBE_RUPPBMR_MQA; <answer> | 
<token> IXGBE_RUPPBMR, reg); <answer> IXGBE_WRITE_REG(hw, 
reg = IXGBE_READ_REG(hw, <token> <answer> IXGBE_RMCS); 
int ixgbe_dcb_config_tx_desc_arbiter_82598(struct ixgbe_hw <token> u16 *refill, <answer> *hw, 
<token> *max, u8 *bwg_id, u8 *prio_type) <answer> u16 
u32 reg, <token> <answer> max_credits; 
u8 <token> <answer> i; 
reg = <token> IXGBE_DPMCS); <answer> IXGBE_READ_REG(hw, 
int ixgbe_dcb_config_tx_data_arbiter_82598(struct ixgbe_hw <token> u16 *refill, <answer> *hw, 
u16 *max, <token> *bwg_id, u8 *prio_type) <answer> u8 
u32 <token> <answer> reg; 
u8 <token> <answer> i; 
reg = IXGBE_READ_REG(hw, <token> <answer> IXGBE_PDPMCS); 
int <token> ixgbe_hw *hw, u8 pfc_en) <answer> ixgbe_dcb_config_pfc_82598(struct 
<token> fcrtl, reg; <answer> u32 
u8 <token> <answer> i; 
static <token> ixgbe_dcb_config_tc_stats_82598(struct ixgbe_hw *hw) <answer> int 
<token> reg = 0; <answer> u32 
u8 <token> = 0; <answer> i 
u8 <token> = 0; <answer> j 
int ixgbe_dcb_hw_config_82598(struct ixgbe_hw <token> u8 pfc_en, u16 *refill, <answer> *hw, 
u16 *max, <token> *bwg_id, u8 *prio_type) <answer> u8 
ixgbe_dcb_config_rx_arbiter_82598(hw, refill, <token> prio_type); <answer> max, 
<token> refill, max, <answer> ixgbe_dcb_config_tx_desc_arbiter_82598(hw, 
bwg_id, <token> <answer> prio_type); 
ixgbe_dcb_config_tx_data_arbiter_82598(hw, <token> max, <answer> refill, 
bwg_id, <token> <answer> prio_type); 
ixgbe_dcb_config_pfc_82598(hw, <token> <answer> pfc_en); 
<token> 0; <answer> return 
#define pr_fmt(fmt) KBUILD_MODNAME <token> " fmt, __func__ <answer> ":%s(): 
<token> <linux/string.h> <answer> #include 
#include <token> <answer> <linux/skbuff.h> 
<token> <linux/export.h> <answer> #include 
<token> <net/caif/cfpkt.h> <answer> #include 
<token> PKT_PREFIX 48 <answer> #define 
<token> PKT_POSTFIX 2 <answer> #define 
<token> PKT_LEN_WHEN_EXTENDING 128 <answer> #define 
#define PKT_ERROR(pkt, errmsg) <token> <answer> \ 
<token> { \ <answer> do 
cfpkt_priv(pkt)->erronous = true; <token> <answer> \ 
<token> \ <answer> skb_reset_tail_pointer(&pkt->skb); 
<token> \ <answer> pr_warn(errmsg); 
<token> while (0) <answer> } 
struct cfpktq <token> <answer> { 
struct <token> head; <answer> sk_buff_head 
atomic_t <token> <answer> count; 
<token> cfpkt { <answer> struct 
struct <token> skb; <answer> sk_buff 
if <token> <answer> (unlikely(is_erronous(pkt))) 
return <token> <answer> -EPROTO; 
if <token> != 0)) { <answer> (unlikely(skb_linearize(&pkt->skb) 
PKT_ERROR(pkt, "linearize <token> <answer> failed\n"); 
return <token> <answer> -EPROTO; 
return <token> pkt->skb.data, cfpkt_getlen(pkt)); <answer> iter_func(data, 
int cfpkt_setlen(struct cfpkt *pkt, <token> len) <answer> u16 
struct sk_buff *skb = <token> <answer> pkt_to_skb(pkt); 
<token> (unlikely(is_erronous(pkt))) <answer> if 
return <token> <answer> -EPROTO; 
if (likely(len <token> skb->len)) { <answer> <= 
<token> (unlikely(skb->data_len)) <answer> if 
<token> len); <answer> ___pskb_trim(skb, 
<token> len); <answer> skb_trim(skb, 
return <token> <answer> cfpkt_getlen(pkt); 
#include <token> <answer> <linux/bitfield.h> 
#include <token> <answer> <linux/cec.h> 
<token> <linux/hdmi.h> <answer> #include 
#include <token> <answer> <linux/i2c.h> 
<token> <linux/kernel.h> <answer> #include 
#include <token> <answer> <linux/module.h> 
#include <token> <answer> <linux/pci.h> 
<token> <linux/slab.h> <answer> #include 
<token> <linux/vga_switcheroo.h> <answer> #include 
<token> <drm/drm_displayid.h> <answer> #include 
#include <token> <answer> <drm/drm_drv.h> 
#include <token> <answer> <drm/drm_edid.h> 
<token> <drm/drm_eld.h> <answer> #include 
#include <token> <answer> <drm/drm_encoder.h> 
#include <token> <answer> <drm/drm_print.h> 
<token> "drm_crtc_internal.h" <answer> #include 
<token> "drm_internal.h" <answer> #include 
static int oui(u8 first, u8 <token> u8 third) <answer> second, 
return (first << 16) | (second << 8) <token> third; <answer> | 
#define EDID_EST_TIMINGS <token> <answer> 16 
#define EDID_STD_TIMINGS <token> <answer> 8 
#define EDID_DETAILED_TIMINGS <token> <answer> 4 
#define EDID_QUIRK_DETAILED_USE_MAXIMUM_SIZE (1 << <token> <answer> 4) 
static const struct drm_display_mode drm_dmt_modes[] <token> { <answer> = 
static const struct drm_display_mode edid_est_modes[] = <token> <answer> { 
{ DRM_MODE("800x600", DRM_MODE_TYPE_DRIVER, 40000, <token> 840, <answer> 800, 
968, 1056, <token> 600, 601, 605, 628, 0, <answer> 0, 
static const <token> drm_display_mode edid_cea_modes_1[] = { <answer> struct 
static const struct drm_display_mode edid_cea_modes_193[] <token> { <answer> = 
static const struct <token> edid_4k_modes[] = { <answer> drm_display_mode 
<token> drm_edid { <answer> struct 
eeodb <token> edid_hfeeodb_block_count(drm_edid->edid); <answer> = 
<token> (eeodb) <answer> if 
num_blocks = <token> <answer> eeodb; 
<token> num_blocks; <answer> return 
static const struct <token> *drm_edid_legacy_init(struct drm_edid *drm_edid, <answer> drm_edid 
const <token> edid *edid) <answer> struct 
<token> (!edid) <answer> if 
<token> NULL; <answer> return 
memset(drm_edid, 0, <token> <answer> sizeof(*drm_edid)); 
drm_edid->edid = <token> <answer> edid; 
<token> = edid_size(edid); <answer> drm_edid->size 
<token> drm_edid; <answer> return 
<token> drm_edid_iter { <answer> struct 
const struct drm_edid <token> <answer> *drm_edid; 
int <token> void *_edid) <answer> drm_edid_header_is_valid(const 
const struct edid *edid = <token> <answer> _edid; 
int i, <token> = 0; <answer> score 
for (i = 0; i < sizeof(edid_header); <token> { <answer> i++) 
if <token> == edid_header[i]) <answer> (edid->header[i] 
return <token> <answer> score; 
static <token> edid_fixup __read_mostly = 6; <answer> int 
module_param_named(edid_fixup, edid_fixup, <token> 0400); <answer> int, 
"Minimum number of valid EDID header <token> (0-8, default 6)"); <answer> bytes 
static int edid_block_compute_checksum(const <token> *_block) <answer> void 
const u8 <token> = _block; <answer> *block 
int <token> <answer> i; 
u8 <token> = 0, crc = 0; <answer> csum 
for (i = 0; i < EDID_LENGTH <token> 1; i++) <answer> - 
<token> += block[i]; <answer> csum 
crc = <token> - csum; <answer> 0x100 
<token> crc; <answer> return 
static <token> edid_block_get_checksum(const void *_block) <answer> int 
const struct edid *block <token> _block; <answer> = 
<token> block->checksum; <answer> return 
static int <token> void *_block) <answer> edid_block_tag(const 
const <token> *block = _block; <answer> u8 
return <token> <answer> block[0]; 
static bool edid_block_is_zero(const void <token> <answer> *edid) 
<token> !memchr_inv(edid, 0, EDID_LENGTH); <answer> return 
bool drm_edid_are_equal(const struct <token> *edid1, const struct edid *edid2) <answer> edid 
int edid1_len, <token> <answer> edid2_len; 
bool edid1_present = <token> != NULL; <answer> edid1 
bool edid2_present = edid2 != <token> <answer> NULL; 
if (edid1_present != <token> <answer> edid2_present) 
return <token> <answer> false; 
if (edid1) <token> <answer> { 
edid1_len <token> edid_size(edid1); <answer> = 
<token> = edid_size(edid2); <answer> edid2_len 
if <token> != edid2_len) <answer> (edid1_len 
<token> false; <answer> return 
<token> (memcmp(edid1, edid2, edid1_len)) <answer> if 
<token> false; <answer> return 
<token> true; <answer> return 
enum <token> { <answer> edid_block_status 
EDID_BLOCK_OK = <token> <answer> 0, 
static enum <token> edid_block_check(const void *_block, <answer> edid_block_status 
bool <token> <answer> is_base_block) 
<token> struct edid *block = _block; <answer> const 
if <token> <answer> (!block) 
return <token> <answer> EDID_BLOCK_NULL; 
<token> (is_base_block) { <answer> if 
int score <token> drm_edid_header_is_valid(block); <answer> = 
if (score < clamp(edid_fixup, 0, <token> { <answer> 8)) 
if <token> <answer> (edid_block_is_zero(block)) 
return <token> <answer> EDID_BLOCK_ZERO; 
return <token> <answer> EDID_BLOCK_HEADER_CORRUPT; 
if <token> < 8) <answer> (score 
return <token> <answer> EDID_BLOCK_HEADER_REPAIR; 
if (edid_block_compute_checksum(block) != edid_block_get_checksum(block)) <token> <answer> { 
if <token> <answer> (edid_block_is_zero(block)) 
return <token> <answer> EDID_BLOCK_ZERO; 
return <token> <answer> EDID_BLOCK_CHECKSUM; 
if <token> { <answer> (is_base_block) 
if <token> != 1) <answer> (block->version 
return <token> <answer> EDID_BLOCK_VERSION; 
return <token> <answer> EDID_BLOCK_OK; 
static bool <token> edid_block_status status, int tag) <answer> edid_block_status_valid(enum 
return <token> == EDID_BLOCK_OK || <answer> status 
status == <token> || <answer> EDID_BLOCK_HEADER_FIXED 
(status == <token> && tag == CEA_EXT); <answer> EDID_BLOCK_CHECKSUM 
<token> bool edid_block_valid(const void *block, bool base) <answer> static 
<token> edid_block_status_valid(edid_block_check(block, base), <answer> return 
<token> void edid_block_status_print(enum edid_block_status status, <answer> static 
const <token> edid *block, <answer> struct 
<token> block_num) <answer> int 
switch <token> { <answer> (status) 
<token> EDID_BLOCK_OK: <answer> case 
<token> EDID_BLOCK_READ_FAIL: <answer> case 
pr_debug("EDID block %d <token> failed\n", block_num); <answer> read 
<token> EDID_BLOCK_NULL: <answer> case 
pr_debug("EDID <token> %d pointer is NULL\n", block_num); <answer> block 
<token> EDID_BLOCK_ZERO: <answer> case 
pr_notice("EDID block %d is all zeroes\n", <token> <answer> block_num); 
case <token> <answer> EDID_BLOCK_HEADER_CORRUPT: 
pr_notice("EDID has corrupt <token> <answer> header\n"); 
<token> EDID_BLOCK_HEADER_REPAIR: <answer> case 
pr_debug("EDID corrupt header <token> repair\n"); <answer> needs 
<token> EDID_BLOCK_HEADER_FIXED: <answer> case 
<token> corrupt header fixed\n"); <answer> pr_debug("EDID 
case <token> <answer> EDID_BLOCK_CHECKSUM: 
if (edid_block_status_valid(status, edid_block_tag(block))) <token> <answer> { 
pr_debug("EDID block <token> (tag 0x%02x) checksum is invalid, remainder is %d, ignoring\n", <answer> %d 
block_num, <token> <answer> edid_block_tag(block), 
<token> else { <answer> } 
pr_notice("EDID <token> %d (tag 0x%02x) checksum is invalid, remainder is %d\n", <answer> block 
<token> edid_block_tag(block), <answer> block_num, 
case <token> <answer> EDID_BLOCK_VERSION: 
pr_notice("EDID has <token> version %d, instead of 1\n", <answer> major 
WARN(1, "EDID block %d unknown edid block <token> code %d\n", <answer> status 
block_num, <token> <answer> status); 
static void edid_block_dump(const char <token> const void *block, int block_num) <answer> *level, 
enum <token> status; <answer> edid_block_status 
<token> prefix[20]; <answer> char 
status <token> edid_block_check(block, block_num == 0); <answer> = 
<token> (status == EDID_BLOCK_ZERO) <answer> if 
sprintf(prefix, "\t[%02x] ZERO ", <token> <answer> block_num); 
<token> if (!edid_block_status_valid(status, edid_block_tag(block))) <answer> else 
sprintf(prefix, "\t[%02x] BAD <token> block_num); <answer> ", 
sprintf(prefix, "\t[%02x] GOOD ", <token> <answer> block_num); 
print_hex_dump(level, prefix, <token> 16, 1, <answer> DUMP_PREFIX_NONE, 
<token> EDID_LENGTH, false); <answer> block, 
bool drm_edid_block_valid(u8 <token> int block_num, bool print_bad_edid, <answer> *_block, 
<token> *edid_corrupt) <answer> bool 
struct edid *block = (struct <token> *)_block; <answer> edid 
enum <token> status; <answer> edid_block_status 
bool <token> = block_num == 0; <answer> is_base_block 
<token> valid; <answer> bool 
<token> (WARN_ON(!block)) <answer> if 
<token> false; <answer> return 
status <token> edid_block_check(block, is_base_block); <answer> = 
if (status == <token> { <answer> EDID_BLOCK_HEADER_REPAIR) 
DRM_DEBUG_KMS("Fixing EDID header, your hardware <token> be failing\n"); <answer> may 
<token> (is_base_block && <answer> if 
<token> == EDID_BLOCK_OK || status == EDID_BLOCK_VERSION)) <answer> (status 
<token> = false; <answer> *edid_corrupt 
else <token> (status != EDID_BLOCK_OK) <answer> if 
*edid_corrupt <token> true; <answer> = 
edid_block_status_print(status, <token> block_num); <answer> block, 
bool <token> edid *edid) <answer> drm_edid_is_valid(struct 
int <token> <answer> i; 
<token> (!edid) <answer> if 
<token> false; <answer> return 
for (i = 0; <token> < edid_block_count(edid); i++) { <answer> i 
<token> *block = (void *)edid_block_data(edid, i); <answer> void 
if (!drm_edid_block_valid(block, i, true, <token> <answer> NULL)) 
return <token> <answer> false; 
return <token> <answer> true; 
bool drm_edid_valid(const struct <token> *drm_edid) <answer> drm_edid 
int <token> <answer> i; 
if <token> <answer> (!drm_edid) 
<token> false; <answer> return 
<token> (edid_size_by_blocks(__drm_edid_block_count(drm_edid)) != drm_edid->size) <answer> if 
<token> false; <answer> return 
for (i = 0; i < <token> i++) { <answer> drm_edid_block_count(drm_edid); 
const void *block = drm_edid_block_data(drm_edid, <token> <answer> i); 
if <token> i == 0)) <answer> (!edid_block_valid(block, 
<token> false; <answer> return 
return <token> <answer> true; 
static struct <token> *edid_filter_invalid_blocks(struct edid *edid, <answer> edid 
size_t <token> <answer> *alloc_size) 
struct <token> *new; <answer> edid 
int i, <token> = 0; <answer> valid_blocks 
<token> (i = 0; i < edid_block_count(edid); i++) { <answer> for 
const <token> *src_block = edid_block_data(edid, i); <answer> void 
if (edid_block_valid(src_block, <token> == 0)) { <answer> i 
void *dst_block <token> (void *)edid_block_data(edid, valid_blocks); <answer> = 
memmove(dst_block, src_block, <token> <answer> EDID_LENGTH); 
<token> int <answer> static 
drm_do_probe_ddc_edid(void *data, u8 *buf, unsigned int <token> size_t len) <answer> block, 
<token> i2c_adapter *adapter = data; <answer> struct 
unsigned char start = block * <token> <answer> EDID_LENGTH; 
unsigned char segment = block <token> 1; <answer> >> 
unsigned char xfers = segment ? 3 : <token> <answer> 2; 
<token> ret, retries = 5; <answer> int 
<token> { <answer> do 
struct <token> msgs[] = { <answer> i2c_msg 
.addr <token> DDC_SEGMENT_ADDR, <answer> = 
<token> = 0, <answer> .flags 
.len = <token> <answer> 1, 
<token> = &segment, <answer> .buf 
}, <token> <answer> { 
.addr = <token> <answer> DDC_ADDR, 
.flags = <token> <answer> 0, 
<token> = 1, <answer> .len 
.buf = <token> <answer> &start, 
}, <token> <answer> { 
.addr = <token> <answer> DDC_ADDR, 
.flags <token> I2C_M_RD, <answer> = 
.len <token> len, <answer> = 
<token> = buf, <answer> .buf 
<token> = i2c_transfer(adapter, &msgs[3 - xfers], xfers); <answer> ret 
if (ret <token> -ENXIO) { <answer> == 
<token> skipping non-existent adapter %s\n", <answer> DRM_DEBUG_KMS("drm: 
} while (ret <token> xfers && --retries); <answer> != 
return ret == xfers ? 0 <token> -1; <answer> : 
static <token> connector_bad_edid(struct drm_connector *connector, <answer> void 
const <token> edid *edid, int num_blocks) <answer> struct 
<token> i; <answer> int 
u8 <token> <answer> last_block; 
last_block <token> edid->extensions; <answer> = 
int drm_edid_override_connector_update(struct drm_connector <token> <answer> *connector) 
const struct drm_edid <token> <answer> *override; 
int num_modes = <token> <answer> 0; 
override <token> drm_edid_override_get(connector); <answer> = 
if <token> { <answer> (override) 
<token> (drm_edid_connector_update(connector, override) == 0) <answer> if 
num_modes <token> drm_edid_connector_add_modes(connector); <answer> = 
"[CONNECTOR:%d:%s] adding %d modes via fallback override/firmware <token> <answer> EDID\n", 
connector->base.id, <token> num_modes); <answer> connector->name, 
<token> num_modes; <answer> return 
typedef <token> read_block_fn(void *context, u8 *buf, unsigned int block, size_t len); <answer> int 
static enum edid_block_status edid_block_read(void <token> unsigned int block_num, <answer> *block, 
<token> read_block, <answer> read_block_fn 
<token> *context) <answer> void 
enum edid_block_status <token> <answer> status; 
bool is_base_block <token> block_num == 0; <answer> = 
int <token> <answer> try; 
for (try = 0; <token> < 4; try++) { <answer> try 
if (read_block(context, <token> block_num, EDID_LENGTH)) <answer> block, 
return <token> <answer> EDID_BLOCK_READ_FAIL; 
status <token> edid_block_check(block, is_base_block); <answer> = 
<token> (status == EDID_BLOCK_HEADER_REPAIR) { <answer> if 
int eeodb = <token> <answer> edid_hfeeodb_block_count(edid); 
if (eeodb > <token> { <answer> num_blocks) 
<token> = eeodb; <answer> num_blocks 
<token> = edid_size_by_blocks(num_blocks); <answer> alloc_size 
new = krealloc(edid, alloc_size, <token> <answer> GFP_KERNEL); 
<token> (!new) <answer> if 
<token> fail; <answer> goto 
<token> = new; <answer> edid 
<token> (invalid_blocks) { <answer> if 
connector_bad_edid(connector, edid, <token> <answer> num_blocks); 
edid <token> edid_filter_invalid_blocks(edid, &alloc_size); <answer> = 
if <token> <answer> (size) 
*size <token> alloc_size; <answer> = 
return <token> <answer> edid; 
return <token> <answer> NULL; 
struct edid *drm_do_get_edid(struct drm_connector <token> <answer> *connector, 
<token> read_block, <answer> read_block_fn 
void <token> <answer> *context) 
return <token> read_block, context, NULL); <answer> _drm_do_get_edid(connector, 
const struct <token> *drm_edid_raw(const struct drm_edid *drm_edid) <answer> edid 
if <token> || !drm_edid->size) <answer> (!drm_edid 
return <token> <answer> NULL; 
if <token> > drm_edid->size)) <answer> (WARN_ON(edid_size(drm_edid->edid) 
return <token> <answer> NULL; 
<token> drm_edid->edid; <answer> return 
const struct drm_edid *drm_edid_alloc(const void *edid, <token> size) <answer> size_t 
const <token> drm_edid *drm_edid; <answer> struct 
<token> (!edid || !size || size < EDID_LENGTH) <answer> if 
return <token> <answer> NULL; 
edid = kmemdup(edid, size, <token> <answer> GFP_KERNEL); 
if <token> <answer> (!edid) 
<token> NULL; <answer> return 
drm_edid = <token> size); <answer> _drm_edid_alloc(edid, 
if <token> <answer> (!drm_edid) 
<token> drm_edid; <answer> return 
const struct drm_edid *drm_edid_dup(const struct drm_edid <token> <answer> *drm_edid) 
<token> (!drm_edid) <answer> if 
<token> NULL; <answer> return 
<token> drm_edid_alloc(drm_edid->edid, drm_edid->size); <answer> return 
void drm_edid_free(const struct drm_edid <token> <answer> *drm_edid) 
<token> (!drm_edid) <answer> if 
drm_probe_ddc(struct i2c_adapter <token> <answer> *adapter) 
unsigned <token> out; <answer> char 
return (drm_do_probe_ddc_edid(adapter, &out, 0, <token> == 0); <answer> 1) 
struct edid *drm_get_edid(struct <token> *connector, <answer> drm_connector 
<token> i2c_adapter *adapter) <answer> struct 
struct edid <token> <answer> *edid; 
<token> (connector->force == DRM_FORCE_OFF) <answer> if 
return <token> <answer> NULL; 
if (connector->force == DRM_FORCE_UNSPECIFIED <token> !drm_probe_ddc(adapter)) <answer> && 
<token> NULL; <answer> return 
edid = <token> drm_do_probe_ddc_edid, adapter, NULL); <answer> _drm_do_get_edid(connector, 
<token> edid); <answer> drm_connector_update_edid_property(connector, 
return <token> <answer> edid; 
const struct <token> *drm_edid_read_custom(struct drm_connector *connector, <answer> drm_edid 
<token> read_block, <answer> read_block_fn 
<token> *context) <answer> void 
const struct drm_edid <token> <answer> *drm_edid; 
struct <token> *edid; <answer> edid 
size_t <token> = 0; <answer> size 
<token> = _drm_do_get_edid(connector, read_block, context, &size); <answer> edid 
if <token> <answer> (!edid) 
return <token> <answer> NULL; 
<token> struct drm_edid *drm_edid_read_ddc(struct drm_connector *connector, <answer> const 
<token> i2c_adapter *adapter) <answer> struct 
const struct drm_edid <token> <answer> *drm_edid; 
<token> (connector->force == DRM_FORCE_OFF) <answer> if 
<token> NULL; <answer> return 
if (connector->force <token> DRM_FORCE_UNSPECIFIED && !drm_probe_ddc(adapter)) <answer> == 
return <token> <answer> NULL; 
drm_edid = drm_edid_read_custom(connector, drm_do_probe_ddc_edid, <token> <answer> adapter); 
<token> struct drm_edid *drm_edid_read(struct drm_connector *connector) <answer> const 
<token> (drm_WARN_ON(connector->dev, !connector->ddc)) <answer> if 
return <token> <answer> NULL; 
return <token> connector->ddc); <answer> drm_edid_read_ddc(connector, 
static u32 edid_extract_panel_id(const struct <token> *edid) <answer> edid 
<token> (u32)edid->mfg_id[0] << 24 | <answer> return 
(u32)edid->mfg_id[1] <token> 16 | <answer> << 
u32 drm_edid_get_panel_id(struct i2c_adapter <token> <answer> *adapter) 
enum edid_block_status <token> <answer> status; 
<token> *base_block; <answer> void 
<token> panel_id = 0; <answer> u32 
base_block = <token> GFP_KERNEL); <answer> kzalloc(EDID_LENGTH, 
<token> (!base_block) <answer> if 
return <token> <answer> 0; 
status <token> edid_block_read(base_block, 0, drm_do_probe_ddc_edid, adapter); <answer> = 
edid_block_status_print(status, <token> 0); <answer> base_block, 
if <token> edid_block_tag(base_block))) <answer> (edid_block_status_valid(status, 
panel_id = <token> <answer> edid_extract_panel_id(base_block); 
edid_block_dump(KERN_NOTICE, base_block, <token> <answer> 0); 
return <token> <answer> panel_id; 
struct edid <token> drm_connector *connector, <answer> *drm_get_edid_switcheroo(struct 
struct i2c_adapter <token> <answer> *adapter) 
struct drm_device *dev = <token> <answer> connector->dev; 
struct <token> *pdev = to_pci_dev(dev->dev); <answer> pci_dev 
struct <token> *edid; <answer> edid 
<token> (drm_WARN_ON_ONCE(dev, !dev_is_pci(dev->dev))) <answer> if 
return <token> <answer> NULL; 
edid = <token> adapter); <answer> drm_get_edid(connector, 
<token> edid; <answer> return 
<token> struct drm_edid *drm_edid_read_switcheroo(struct drm_connector *connector, <answer> const 
<token> i2c_adapter *adapter) <answer> struct 
struct drm_device *dev <token> connector->dev; <answer> = 
struct pci_dev <token> = to_pci_dev(dev->dev); <answer> *pdev 
const struct drm_edid <token> <answer> *drm_edid; 
<token> (drm_WARN_ON_ONCE(dev, !dev_is_pci(dev->dev))) <answer> if 
return <token> <answer> NULL; 
drm_edid = <token> adapter); <answer> drm_edid_read_ddc(connector, 
return <token> <answer> drm_edid; 
struct edid *drm_edid_duplicate(const struct edid <token> <answer> *edid) 
<token> (!edid) <answer> if 
<token> NULL; <answer> return 
<token> kmemdup(edid, edid_size(edid), GFP_KERNEL); <answer> return 
static u32 <token> struct drm_edid *drm_edid) <answer> edid_get_quirks(const 
<token> panel_id = edid_extract_panel_id(drm_edid->edid); <answer> u32 
const struct <token> *quirk; <answer> edid_quirk 
int <token> <answer> i; 
for (i = 0; i <token> ARRAY_SIZE(edid_quirk_list); i++) { <answer> < 
quirk = <token> <answer> &edid_quirk_list[i]; 
<token> (quirk->panel_id == panel_id) <answer> if 
return <token> <answer> quirk->quirks; 
return <token> <answer> 0; 
#define <token> ((m)->hdisplay * (m)->vdisplay) <answer> MODE_SIZE(m) 
#define <token> (abs((c) - (t))) <answer> MODE_REFRESH_DIFF(c,t) 
static void <token> drm_connector *connector) <answer> edid_fixup_preferred(struct 
const struct <token> *info = &connector->display_info; <answer> drm_display_info 
struct drm_display_mode *t, <token> *preferred_mode; <answer> *cur_mode, 
int target_refresh = <token> <answer> 0; 
int <token> preferred_vrefresh; <answer> cur_vrefresh, 
if <token> <answer> (list_empty(&connector->probed_modes)) 
if (info->quirks <token> EDID_QUIRK_PREFER_LARGE_60) <answer> & 
target_refresh = <token> <answer> 60; 
<token> (info->quirks & EDID_QUIRK_PREFER_LARGE_75) <answer> if 
target_refresh = <token> <answer> 75; 
preferred_mode = <token> <answer> list_first_entry(&connector->probed_modes, 
<token> drm_display_mode, head); <answer> struct 
list_for_each_entry_safe(cur_mode, <token> &connector->probed_modes, head) { <answer> t, 
cur_mode->type <token> ~DRM_MODE_TYPE_PREFERRED; <answer> &= 
if (cur_mode == <token> <answer> preferred_mode) 
struct drm_display_mode *drm_mode_find_dmt(struct drm_device <token> <answer> *dev, 
<token> hsize, int vsize, int fresh, <answer> int 
bool <token> <answer> rb) 
<token> i; <answer> int 
for (i = <token> i < ARRAY_SIZE(drm_dmt_modes); i++) { <answer> 0; 
const struct <token> *ptr = &drm_dmt_modes[i]; <answer> drm_display_mode 
if (hsize != <token> <answer> ptr->hdisplay) 
if (vsize <token> ptr->vdisplay) <answer> != 
if (fresh <token> drm_mode_vrefresh(ptr)) <answer> != 
if <token> != mode_is_rb(ptr)) <answer> (rb 
return drm_mode_duplicate(dev, <token> <answer> ptr); 
return <token> <answer> NULL; 
static bool is_display_descriptor(const struct detailed_timing <token> u8 type) <answer> *descriptor, 
BUILD_BUG_ON(offsetof(typeof(*descriptor), pixel_clock) <token> 0); <answer> != 
<token> data.other_data.pad1) != 2); <answer> BUILD_BUG_ON(offsetof(typeof(*descriptor), 
BUILD_BUG_ON(offsetof(typeof(*descriptor), <token> != 3); <answer> data.other_data.type) 
return descriptor->pixel_clock <token> 0 && <answer> == 
descriptor->data.other_data.pad1 == <token> && <answer> 0 
descriptor->data.other_data.type == <token> <answer> type; 
static bool is_detailed_timing_descriptor(const struct <token> *descriptor) <answer> detailed_timing 
<token> pixel_clock) != 0); <answer> BUILD_BUG_ON(offsetof(typeof(*descriptor), 
return <token> != 0; <answer> descriptor->pixel_clock 
typedef void detailed_cb(const struct detailed_timing *timing, <token> *closure); <answer> void 
static <token> <answer> void 
cea_for_each_detailed_block(const <token> *ext, detailed_cb *cb, void *closure) <answer> u8 
int i, <token> <answer> n; 
<token> d = ext[0x02]; <answer> u8 
const u8 *det_base = ext + <token> <answer> d; 
if (d <token> 4 || d > 127) <answer> < 
<token> = (127 - d) / 18; <answer> n 
for (i = 0; i < <token> i++) <answer> n; 
cb((const struct detailed_timing *)(det_base + 18 * i), <token> <answer> closure); 
static <token> <answer> void 
vtb_for_each_detailed_block(const <token> *ext, detailed_cb *cb, void *closure) <answer> u8 
unsigned int i, n = <token> 6); <answer> min((int)ext[0x02], 
const u8 *det_base = <token> + 5; <answer> ext 
if (ext[0x01] <token> 1) <answer> != 
int <token> = LEVEL_CVT; <answer> ret 
drm_for_each_detailed_block(drm_edid, get_timing_level, <token> <answer> &ret); 
<token> ret; <answer> return 
} else if (edid->revision <token> 3 && drm_gtf2_hbreak(drm_edid)) { <answer> >= 
return <token> <answer> LEVEL_GTF2; 
} <token> if (edid->revision >= 2) { <answer> else 
<token> LEVEL_GTF; <answer> return 
} <token> { <answer> else 
<token> LEVEL_DMT; <answer> return 
static <token> <answer> int 
bad_std_timing(u8 a, u8 <token> <answer> b) 
return <token> == 0x00 && b == 0x00) || <answer> (a 
(a == 0x01 && <token> == 0x01) || <answer> b 
(a == 0x20 && b == <token> <answer> 0x20); 
static <token> drm_mode_hsync(const struct drm_display_mode *mode) <answer> int 
if <token> <= 0) <answer> (mode->htotal 
return <token> <answer> 0; 
<token> DIV_ROUND_CLOSEST(mode->clock, mode->htotal); <answer> return 
<token> struct drm_display_mode * <answer> static 
<token> drm_device *dev, <answer> drm_gtf2_mode(struct 
<token> struct drm_edid *drm_edid, <answer> const 
int <token> int vsize, int vrefresh_rate) <answer> hsize, 
<token> drm_display_mode *mode; <answer> struct 
mode = drm_gtf_mode(dev, hsize, <token> vrefresh_rate, 0, 0); <answer> vsize, 
<token> (!mode) <answer> if 
<token> NULL; <answer> return 
if (drm_mode_hsync(mode) > <token> { <answer> drm_gtf2_hbreak(drm_edid)) 
drm_mode_destroy(dev, <token> <answer> mode); 
<token> = drm_gtf_mode_complex(dev, hsize, vsize, <answer> mode 
<token> 0, 0, <answer> vrefresh_rate, 
return <token> <answer> mode; 
static <token> drm_display_mode *drm_mode_std(struct drm_connector *connector, <answer> struct 
const <token> drm_edid *drm_edid, <answer> struct 
const struct std_timing <token> <answer> *t) 
struct drm_device <token> = connector->dev; <answer> *dev 
struct drm_display_mode <token> *mode = NULL; <answer> *m, 
int hsize, <token> <answer> vsize; 
<token> vrefresh_rate; <answer> int 
unsigned aspect_ratio <token> (t->vfreq_aspect & EDID_TIMING_ASPECT_MASK) <answer> = 
<token> EDID_TIMING_ASPECT_SHIFT; <answer> >> 
unsigned vfreq = <token> & EDID_TIMING_VFREQ_MASK) <answer> (t->vfreq_aspect 
>> <token> <answer> EDID_TIMING_VFREQ_SHIFT; 
int timing_level = <token> <answer> standard_timing_level(drm_edid); 
if (bad_std_timing(t->hsize, <token> <answer> t->vfreq_aspect)) 
<token> NULL; <answer> return 
<token> &connector->probed_modes, head) <answer> list_for_each_entry(m, 
if <token> == hsize && m->vdisplay == vsize && <answer> (m->hdisplay 
drm_mode_vrefresh(m) == <token> <answer> vrefresh_rate) 
return <token> <answer> NULL; 
static <token> <answer> void 
drm_mode_do_interlace_quirk(struct drm_display_mode <token> <answer> *mode, 
const struct detailed_pixel_timing <token> <answer> *pt) 
int <token> <answer> i; 
static <token> struct { <answer> const 
<token> w, h; <answer> int 
} cea_interlaced[] <token> { <answer> = 
{ 1920, 1080 <token> <answer> }, 
{ 720, 480 <token> <answer> }, 
{ 1440, <token> }, <answer> 480 
<token> 2880, 480 }, <answer> { 
{ <token> 576 }, <answer> 720, 
{ 1440, 576 <token> <answer> }, 
{ <token> 576 }, <answer> 2880, 
<token> (!(pt->misc & DRM_EDID_PT_INTERLACED)) <answer> if 
for (i = 0; i < ARRAY_SIZE(cea_interlaced); <token> { <answer> i++) 
<token> ((mode->hdisplay == cea_interlaced[i].w) && <answer> if 
(mode->vdisplay == cea_interlaced[i].h / 2)) <token> <answer> { 
mode->vdisplay *= <token> <answer> 2; 
<token> *= 2; <answer> mode->vsync_start 
mode->vsync_end <token> 2; <answer> *= 
mode->vtotal *= <token> <answer> 2; 
<token> |= 1; <answer> mode->vtotal 
mode->flags <token> DRM_MODE_FLAG_INTERLACE; <answer> |= 
<token> struct drm_display_mode *drm_mode_detailed(struct drm_connector *connector, <answer> static 
const <token> drm_edid *drm_edid, <answer> struct 
<token> struct detailed_timing *timing) <answer> const 
const struct drm_display_info *info = <token> <answer> &connector->display_info; 
struct drm_device *dev = <token> <answer> connector->dev; 
struct <token> *mode; <answer> drm_display_mode 
const struct detailed_pixel_timing *pt <token> &timing->data.pixel_data; <answer> = 
unsigned hactive = (pt->hactive_hblank_hi & 0xf0) << 4 | <token> <answer> pt->hactive_lo; 
unsigned vactive = (pt->vactive_vblank_hi & 0xf0) <token> 4 | pt->vactive_lo; <answer> << 
unsigned hblank = (pt->hactive_hblank_hi & 0xf) << <token> | pt->hblank_lo; <answer> 8 
unsigned vblank = (pt->vactive_vblank_hi & <token> << 8 | pt->vblank_lo; <answer> 0xf) 
unsigned hsync_offset = (pt->hsync_vsync_offset_pulse_width_hi <token> 0xc0) << 2 | pt->hsync_offset_lo; <answer> & 
unsigned <token> = (pt->hsync_vsync_offset_pulse_width_hi & 0x30) << 4 | pt->hsync_pulse_width_lo; <answer> hsync_pulse_width 
unsigned vsync_offset <token> (pt->hsync_vsync_offset_pulse_width_hi & 0xc) << 2 | pt->vsync_offset_pulse_width_lo >> 4; <answer> = 
unsigned vsync_pulse_width = (pt->hsync_vsync_offset_pulse_width_hi & 0x3) <token> 4 | (pt->vsync_offset_pulse_width_lo & 0xf); <answer> << 
void drm_mode_fixup_1366x768(struct <token> *mode) <answer> drm_display_mode 
if (mode->hdisplay == 1368 && mode->vdisplay <token> 768) { <answer> == 
mode->hdisplay = <token> <answer> 1366; 
static int <token> drm_connector *connector, <answer> drm_gtf_modes_for_range(struct 
const struct drm_edid <token> <answer> *drm_edid, 
const struct <token> *timing) <answer> detailed_timing 
int i, modes <token> 0; <answer> = 
struct <token> *newmode; <answer> drm_display_mode 
struct <token> *dev = connector->dev; <answer> drm_device 
<token> (i = 0; i < ARRAY_SIZE(extra_modes); i++) { <answer> for 
const struct minimode *m = <token> <answer> &extra_modes[i]; 
newmode = drm_gtf_mode(dev, <token> m->h, m->r, 0, 0); <answer> m->w, 
<token> (!newmode) <answer> if 
<token> modes; <answer> return 
if <token> drm_edid, timing) || <answer> (!mode_in_range(newmode, 
!valid_inferred_mode(connector, <token> { <answer> newmode)) 
drm_mode_destroy(dev, <token> <answer> newmode); 
drm_mode_probed_add(connector, <token> <answer> newmode); 
<token> modes; <answer> return 
static int drm_gtf2_modes_for_range(struct <token> *connector, <answer> drm_connector 
const <token> drm_edid *drm_edid, <answer> struct 
<token> struct detailed_timing *timing) <answer> const 
<token> i, modes = 0; <answer> int 
<token> drm_display_mode *newmode; <answer> struct 
<token> drm_device *dev = connector->dev; <answer> struct 
for (i = <token> i < ARRAY_SIZE(extra_modes); i++) { <answer> 0; 
const struct minimode *m = <token> <answer> &extra_modes[i]; 
newmode <token> drm_gtf2_mode(dev, drm_edid, m->w, m->h, m->r); <answer> = 
<token> (!newmode) <answer> if 
<token> modes; <answer> return 
<token> (!mode_in_range(newmode, drm_edid, timing) || <answer> if 
<token> newmode)) { <answer> !valid_inferred_mode(connector, 
<token> newmode); <answer> drm_mode_destroy(dev, 
<token> newmode); <answer> drm_mode_probed_add(connector, 
<token> modes; <answer> return 
static int drm_cvt_modes_for_range(struct <token> *connector, <answer> drm_connector 
const <token> drm_edid *drm_edid, <answer> struct 
<token> struct detailed_timing *timing) <answer> const 
int <token> modes = 0; <answer> i, 
struct drm_display_mode <token> <answer> *newmode; 
struct drm_device <token> = connector->dev; <answer> *dev 
bool <token> = drm_monitor_supports_rb(drm_edid); <answer> rb 
for (i = <token> i < ARRAY_SIZE(extra_modes); i++) { <answer> 0; 
const <token> minimode *m = &extra_modes[i]; <answer> struct 
newmode = drm_cvt_mode(dev, m->w, m->h, m->r, <token> 0, 0); <answer> rb, 
<token> (!newmode) <answer> if 
<token> modes; <answer> return 
if (!mode_in_range(newmode, <token> timing) || <answer> drm_edid, 
<token> newmode)) { <answer> !valid_inferred_mode(connector, 
<token> newmode); <answer> drm_mode_destroy(dev, 
<token> newmode); <answer> drm_mode_probed_add(connector, 
<token> modes; <answer> return 
static <token> <answer> void 
do_inferred_modes(const struct detailed_timing *timing, void <token> <answer> *c) 
<token> detailed_mode_closure *closure = c; <answer> struct 
const <token> detailed_non_pixel *data = &timing->data.other_data; <answer> struct 
const struct detailed_data_monitor_range *range = <token> <answer> &data->data.range; 
if (!is_display_descriptor(timing, <token> <answer> EDID_DETAIL_MONITOR_RANGE)) 
<token> += drm_dmt_modes_for_range(closure->connector, <answer> closure->modes 
<token> (closure->drm_edid->edid->revision < 2) <answer> if 
static <token> add_established_modes(struct drm_connector *connector, <answer> int 
const struct <token> *drm_edid) <answer> drm_edid 
struct drm_device <token> = connector->dev; <answer> *dev 
const struct edid *edid = <token> <answer> drm_edid->edid; 
<token> long est_bits = edid->established_timings.t1 | <answer> unsigned 
(edid->established_timings.t2 << 8) <token> <answer> | 
((edid->established_timings.mfg_rsvd & <token> << 9); <answer> 0x80) 
int i, modes = <token> <answer> 0; 
struct detailed_mode_closure closure <token> { <answer> = 
.connector <token> connector, <answer> = 
.drm_edid <token> drm_edid, <answer> = 
for (i = 0; i <= <token> i++) { <answer> EDID_EST_TIMINGS; 
if (est_bits & <token> { <answer> (1<<i)) 
<token> drm_display_mode *newmode; <answer> struct 
newmode = drm_mode_duplicate(dev, <token> <answer> &edid_est_modes[i]); 
if (newmode) <token> <answer> { 
drm_mode_probed_add(connector, <token> <answer> newmode); 
if <token> >= 1) <answer> (edid->revision 
drm_for_each_detailed_block(drm_edid, <token> <answer> do_established_modes, 
return modes + <token> <answer> closure.modes; 
<token> void <answer> static 
<token> struct detailed_timing *timing, void *c) <answer> do_standard_modes(const 
struct <token> *closure = c; <answer> detailed_mode_closure 
<token> struct detailed_non_pixel *data = &timing->data.other_data; <answer> const 
struct drm_connector <token> = closure->connector; <answer> *connector 
int <token> <answer> i; 
if (!is_display_descriptor(timing, <token> <answer> EDID_DETAIL_STD_MODES)) 
<token> (i = 0; i < 6; i++) { <answer> for 
<token> struct std_timing *std = &data->data.timings[i]; <answer> const 
<token> drm_display_mode *newmode; <answer> struct 
newmode <token> drm_mode_std(connector, closure->drm_edid, std); <answer> = 
<token> (newmode) { <answer> if 
<token> newmode); <answer> drm_mode_probed_add(connector, 
static int <token> drm_connector *connector, <answer> add_standard_modes(struct 
const <token> drm_edid *drm_edid) <answer> struct 
int <token> modes = 0; <answer> i, 
struct <token> closure = { <answer> detailed_mode_closure 
<token> = connector, <answer> .connector 
.drm_edid <token> drm_edid, <answer> = 
for (i = 0; i < <token> i++) { <answer> EDID_STD_TIMINGS; 
<token> drm_display_mode *newmode; <answer> struct 
newmode <token> drm_mode_std(connector, drm_edid, <answer> = 
if (newmode) <token> <answer> { 
drm_mode_probed_add(connector, <token> <answer> newmode); 
if (drm_edid->edid->revision >= <token> <answer> 1) 
drm_for_each_detailed_block(drm_edid, <token> <answer> do_standard_modes, 
fixup_detailed_cea_mode_clock(closure->connector, <token> <answer> newmode); 
drm_mode_probed_add(closure->connector, <token> <answer> newmode); 
closure->preferred <token> false; <answer> = 
static int add_detailed_modes(struct drm_connector <token> <answer> *connector, 
const struct <token> *drm_edid) <answer> drm_edid 
struct detailed_mode_closure closure = <token> <answer> { 
.connector = <token> <answer> connector, 
<token> = drm_edid, <answer> .drm_edid 
if (drm_edid->edid->revision <token> 4) <answer> >= 
const u8 *drm_find_edid_extension(const struct <token> *drm_edid, <answer> drm_edid 
<token> ext_id, int *ext_index) <answer> int 
<token> u8 *edid_ext = NULL; <answer> const 
<token> i; <answer> int 
static <token> int <answer> unsigned 
cea_mode_alternate_clock(const struct drm_display_mode <token> <answer> *cea_mode) 
unsigned int clock <token> cea_mode->clock; <answer> = 
if <token> % 6 != 0) <answer> (drm_mode_vrefresh(cea_mode) 
<token> clock; <answer> return 
if <token> == 240 || cea_mode->vdisplay == 480) <answer> (cea_mode->vdisplay 
clock = DIV_ROUND_CLOSEST(clock <token> 1001, 1000); <answer> * 
clock = DIV_ROUND_CLOSEST(clock * <token> 1001); <answer> 1000, 
<token> clock; <answer> return 
<token> bool <answer> static 
cea_mode_alternate_timings(u8 vic, struct <token> *mode) <answer> drm_display_mode 
<token> != 262 || <answer> BUILD_BUG_ON(cea_mode_for_vic(8)->vtotal 
<token> != 262 || <answer> cea_mode_for_vic(9)->vtotal 
cea_mode_for_vic(12)->vtotal != <token> || <answer> 262 
<token> != 262 || <answer> cea_mode_for_vic(13)->vtotal 
cea_mode_for_vic(23)->vtotal != 312 <token> <answer> || 
cea_mode_for_vic(24)->vtotal <token> 312 || <answer> != 
cea_mode_for_vic(27)->vtotal != 312 <token> <answer> || 
cea_mode_for_vic(28)->vtotal != <token> <answer> 312); 
if (((vic == <token> || vic == 9 || <answer> 8 
vic == 12 || vic <token> 13) && mode->vtotal < 263) || <answer> == 
((vic == 23 <token> vic == 24 || <answer> || 
vic == 27 || vic == 28) && <token> < 314)) { <answer> mode->vtotal 
<token> true; <answer> return 
return <token> <answer> false; 
static u8 drm_match_cea_mode_clock_tolerance(const <token> drm_display_mode *to_match, <answer> struct 
unsigned int <token> <answer> clock_tolerance) 
<token> int match_flags = DRM_MODE_MATCH_TIMINGS | DRM_MODE_MATCH_FLAGS; <answer> unsigned 
u8 <token> <answer> vic; 
if <token> <answer> (!to_match->clock) 
<token> 0; <answer> return 
<token> (to_match->picture_aspect_ratio) <answer> if 
match_flags |= <token> <answer> DRM_MODE_MATCH_ASPECT_RATIO; 
for <token> = 1; vic < cea_num_vics(); vic = cea_next_vic(vic)) { <answer> (vic 
struct drm_display_mode <token> <answer> cea_mode; 
unsigned int clock1, <token> <answer> clock2; 
drm_mode_init(&cea_mode, <token> <answer> cea_mode_for_vic(vic)); 
u8 drm_match_cea_mode(const struct drm_display_mode <token> <answer> *to_match) 
unsigned int <token> = DRM_MODE_MATCH_TIMINGS | DRM_MODE_MATCH_FLAGS; <answer> match_flags 
<token> vic; <answer> u8 
<token> (!to_match->clock) <answer> if 
<token> 0; <answer> return 
if <token> <answer> (to_match->picture_aspect_ratio) 
match_flags <token> DRM_MODE_MATCH_ASPECT_RATIO; <answer> |= 
for (vic = <token> vic < cea_num_vics(); vic = cea_next_vic(vic)) { <answer> 1; 
struct drm_display_mode <token> <answer> cea_mode; 
unsigned int clock1, <token> <answer> clock2; 
drm_mode_init(&cea_mode, <token> <answer> cea_mode_for_vic(vic)); 
static <token> int <answer> unsigned 
hdmi_mode_alternate_clock(const <token> drm_display_mode *hdmi_mode) <answer> struct 
<token> cea_mode_alternate_clock(hdmi_mode); <answer> return 
static u8 drm_match_hdmi_mode_clock_tolerance(const <token> drm_display_mode *to_match, <answer> struct 
unsigned int <token> <answer> clock_tolerance) 
unsigned int match_flags = DRM_MODE_MATCH_TIMINGS | <token> <answer> DRM_MODE_MATCH_FLAGS; 
u8 <token> <answer> vic; 
if <token> <answer> (!to_match->clock) 
<token> 0; <answer> return 
<token> (to_match->picture_aspect_ratio) <answer> if 
match_flags <token> DRM_MODE_MATCH_ASPECT_RATIO; <answer> |= 
<token> (vic = 1; vic < ARRAY_SIZE(edid_4k_modes); vic++) { <answer> for 
const struct drm_display_mode *hdmi_mode = <token> <answer> &edid_4k_modes[vic]; 
unsigned int clock1, <token> <answer> clock2; 
<token> u8 drm_match_hdmi_mode(const struct drm_display_mode *to_match) <answer> static 
unsigned int match_flags = <token> | DRM_MODE_MATCH_FLAGS; <answer> DRM_MODE_MATCH_TIMINGS 
u8 <token> <answer> vic; 
if <token> <answer> (!to_match->clock) 
return <token> <answer> 0; 
if <token> <answer> (to_match->picture_aspect_ratio) 
<token> |= DRM_MODE_MATCH_ASPECT_RATIO; <answer> match_flags 
for (vic = 1; vic < ARRAY_SIZE(edid_4k_modes); vic++) <token> <answer> { 
const <token> drm_display_mode *hdmi_mode = &edid_4k_modes[vic]; <answer> struct 
<token> int clock1, clock2; <answer> unsigned 
list_for_each_entry(mode, <token> head) { <answer> &connector->probed_modes, 
const struct drm_display_mode *cea_mode = <token> <answer> NULL; 
struct <token> *newmode; <answer> drm_display_mode 
u8 <token> = drm_match_cea_mode(mode); <answer> vic 
unsigned int clock1, <token> <answer> clock2; 
<token> (drm_valid_cea_vic(vic)) { <answer> if 
cea_mode = <token> <answer> cea_mode_for_vic(vic); 
<token> = cea_mode_alternate_clock(cea_mode); <answer> clock2 
} <token> { <answer> else 
vic <token> drm_match_hdmi_mode(mode); <answer> = 
if <token> { <answer> (drm_valid_hdmi_vic(vic)) 
<token> = &edid_4k_modes[vic]; <answer> cea_mode 
clock2 <token> hdmi_mode_alternate_clock(cea_mode); <answer> = 
<token> (!cea_mode) <answer> if 
clock1 <token> cea_mode->clock; <answer> = 
<token> (clock1 == clock2) <answer> if 
if (mode->clock != clock1 && mode->clock != <token> <answer> clock2) 
newmode = drm_mode_duplicate(dev, <token> <answer> cea_mode); 
if <token> <answer> (!newmode) 
if <token> != clock1) <answer> (mode->clock 
newmode->clock <token> clock1; <answer> = 
newmode->clock <token> clock2; <answer> = 
list_add_tail(&newmode->head, <token> <answer> &list); 
list_for_each_entry_safe(mode, tmp, &list, <token> { <answer> head) 
<token> mode); <answer> drm_mode_probed_add(connector, 
return <token> <answer> modes; 
static u8 <token> svd) <answer> svd_to_vic(u8 
static <token> drm_display_mode * <answer> struct 
drm_display_mode_from_vic_index(struct <token> *connector, int vic_index) <answer> drm_connector 
<token> struct drm_display_info *info = &connector->display_info; <answer> const 
<token> drm_device *dev = connector->dev; <answer> struct 
if (!info->vics || vic_index >= info->vics_len || <token> <answer> !info->vics[vic_index]) 
return <token> <answer> NULL; 
return drm_display_mode_from_cea_vic(dev, <token> <answer> info->vics[vic_index]); 
<token> int do_y420vdb_modes(struct drm_connector *connector, <answer> static 
const u8 *svds, u8 <token> <answer> svds_len) 
struct drm_device *dev = <token> <answer> connector->dev; 
int modes = <token> i; <answer> 0, 
<token> (i = 0; i < svds_len; i++) { <answer> for 
u8 vic = <token> <answer> svd_to_vic(svds[i]); 
<token> drm_display_mode *newmode; <answer> struct 
if <token> <answer> (!drm_valid_cea_vic(vic)) 
newmode <token> drm_mode_duplicate(dev, cea_mode_for_vic(vic)); <answer> = 
if <token> <answer> (!newmode) 
drm_mode_probed_add(connector, <token> <answer> newmode); 
return <token> <answer> modes; 
struct drm_display_mode <token> <answer> * 
drm_display_mode_from_cea_vic(struct <token> *dev, <answer> drm_device 
<token> video_code) <answer> u8 
const struct <token> *cea_mode; <answer> drm_display_mode 
struct drm_display_mode <token> <answer> *newmode; 
cea_mode <token> cea_mode_for_vic(video_code); <answer> = 
if <token> <answer> (!cea_mode) 
return <token> <answer> NULL; 
newmode = drm_mode_duplicate(dev, <token> <answer> cea_mode); 
if <token> <answer> (!newmode) 
<token> NULL; <answer> return 
return <token> <answer> newmode; 
<token> int <answer> static 
do_hdmi_vsdb_modes(struct <token> *connector, const u8 *db, u8 len) <answer> drm_connector 
int modes = 0, offset = 0, i, <token> = 0, multi_len; <answer> multi_present 
u8 vic_len, hdmi_3d_len <token> 0; <answer> = 
u16 <token> <answer> mask; 
<token> structure_all; <answer> u16 
if (len < <token> <answer> 8) 
goto <token> <answer> out; 
<token> (len < (8 + offset + 2)) <answer> if 
goto <token> <answer> out; 
return <token> <answer> cea[1]; 
<token> cea_db_iter { <answer> struct 
struct <token> edid_iter; <answer> drm_edid_iter 
<token> displayid_iter displayid_iter; <answer> struct 
static int <token> u8 *cta) <answer> cea_db_collection_size(const 
u8 d = <token> <answer> cta[2]; 
if <token> < 4 || d > 127) <answer> (d 
return <token> <answer> 0; 
return <token> - 4; <answer> d 
static <token> void *__cea_db_iter_edid_next(struct cea_db_iter *iter) <answer> const 
const <token> *ext; <answer> u8 
<token> &iter->edid_iter) { <answer> drm_edid_iter_for_each(ext, 
<token> size; <answer> int 
<token> const void *__cea_db_iter_displayid_next(struct cea_db_iter *iter) <answer> static 
const <token> displayid_block *block; <answer> struct 
<token> &iter->displayid_iter) { <answer> displayid_iter_for_each(block, 
if <token> != DATA_BLOCK_CTA) <answer> (block->tag 
iter->index = <token> <answer> sizeof(*block); 
iter->end = iter->index + <token> <answer> block->num_bytes; 
<token> block; <answer> return 
return <token> <answer> NULL; 
static const struct cea_db <token> cea_db_iter *iter) <answer> *__cea_db_iter_next(struct 
<token> struct cea_db *db; <answer> const 
if (iter->collection) <token> <answer> { 
iter->collection <token> __cea_db_iter_edid_next(iter); <answer> = 
if <token> <answer> (!iter->collection) 
iter->collection <token> __cea_db_iter_displayid_next(iter); <answer> = 
<token> (!iter->collection) <answer> if 
<token> NULL; <answer> return 
<token> = __cea_db_iter_current_block(iter); <answer> db 
if <token> <answer> (db) 
return <token> <answer> db; 
#define <token> __iter) \ <answer> cea_db_iter_for_each(__db, 
<token> (((__db) = __cea_db_iter_next(__iter))) <answer> while 
static <token> cea_db_iter_end(struct cea_db_iter *iter) <answer> void 
memset(iter, 0, <token> <answer> sizeof(*iter)); 
static bool cea_db_is_hdmi_vsdb(const struct cea_db <token> <answer> *db) 
return cea_db_is_vendor(db, <token> && <answer> HDMI_IEEE_OUI) 
cea_db_payload_len(db) >= <token> <answer> 5; 
static bool <token> struct cea_db *db) <answer> cea_db_is_hdmi_forum_vsdb(const 
return cea_db_is_vendor(db, HDMI_FORUM_IEEE_OUI) <token> <answer> && 
cea_db_payload_len(db) >= <token> <answer> 7; 
static bool cea_db_is_hdmi_forum_eeodb(const <token> *db) <answer> void 
<token> cea_db_is_extended_tag(db, CTA_EXT_DB_HF_EEODB) && <answer> return 
cea_db_payload_len(db) >= <token> <answer> 2; 
static bool cea_db_is_microsoft_vsdb(const struct cea_db <token> <answer> *db) 
return <token> MICROSOFT_IEEE_OUI) && <answer> cea_db_is_vendor(db, 
cea_db_payload_len(db) <token> 21; <answer> == 
static bool <token> struct cea_db *db) <answer> cea_db_is_vcdb(const 
return cea_db_is_extended_tag(db, <token> && <answer> CTA_EXT_DB_VIDEO_CAP) 
cea_db_payload_len(db) <token> 2; <answer> == 
<token> bool cea_db_is_hdmi_forum_scdb(const struct cea_db *db) <answer> static 
return <token> CTA_EXT_DB_HF_SCDB) && <answer> cea_db_is_extended_tag(db, 
cea_db_payload_len(db) <token> 7; <answer> >= 
static bool cea_db_is_y420cmdb(const <token> cea_db *db) <answer> struct 
return <token> CTA_EXT_DB_420_VIDEO_CAP_MAP); <answer> cea_db_is_extended_tag(db, 
<token> bool cea_db_is_y420vdb(const struct cea_db *db) <answer> static 
return <token> CTA_EXT_DB_420_VIDEO_DATA); <answer> cea_db_is_extended_tag(db, 
static bool cea_db_is_hdmi_hdr_metadata_block(const <token> cea_db *db) <answer> struct 
return cea_db_is_extended_tag(db, CTA_EXT_DB_HDR_STATIC_METADATA) <token> <answer> && 
cea_db_payload_len(db) <token> 3; <answer> >= 
static int <token> struct edid *edid) <answer> edid_hfeeodb_extension_block_count(const 
const <token> *cta; <answer> u8 
if <token> <answer> (!cea_db_is_hdmi_forum_eeodb(&cta[4])) 
<token> 0; <answer> return 
return cta[4 <token> 2]; <answer> + 
static void parse_cta_y420cmdb(struct drm_connector <token> <answer> *connector, 
const struct cea_db <token> u64 *y420cmdb_map) <answer> *db, 
<token> drm_display_info *info = &connector->display_info; <answer> struct 
int i, <token> = cea_db_payload_len(db) - 1; <answer> map_len 
const u8 *data = <token> + 1; <answer> cea_db_data(db) 
u64 <token> = 0; <answer> map 
if (map_len == 0) <token> <answer> { 
if (WARN_ON_ONCE(map_len > <token> <answer> 8)) 
<token> = 8; <answer> map_len 
for (i = 0; i < <token> i++) <answer> map_len; 
map <token> (u64)data[i] << (8 * i); <answer> |= 
<token> (map) <answer> if 
info->color_formats <token> DRM_COLOR_FORMAT_YCBCR420; <answer> |= 
*y420cmdb_map <token> map; <answer> = 
static int add_cea_modes(struct drm_connector <token> <answer> *connector, 
const <token> drm_edid *drm_edid) <answer> struct 
const struct <token> *db; <answer> cea_db 
struct cea_db_iter <token> <answer> iter; 
<token> modes; <answer> int 
vic = <token> 5); <answer> drm_match_cea_mode_clock_tolerance(mode, 
if (drm_valid_cea_vic(vic)) <token> <answer> { 
type = <token> <answer> "CEA"; 
cea_mode = <token> <answer> cea_mode_for_vic(vic); 
clock1 <token> cea_mode->clock; <answer> = 
clock2 = <token> <answer> cea_mode_alternate_clock(cea_mode); 
<token> else { <answer> } 
vic = drm_match_hdmi_mode_clock_tolerance(mode, <token> <answer> 5); 
if <token> { <answer> (drm_valid_hdmi_vic(vic)) 
<token> = "HDMI"; <answer> type 
cea_mode <token> &edid_4k_modes[vic]; <answer> = 
<token> = cea_mode->clock; <answer> clock1 
<token> = hdmi_mode_alternate_clock(cea_mode); <answer> clock2 
} else <token> <answer> { 
q <token> max_avg >> 5; <answer> = 
r = <token> % 32; <answer> max_avg 
max = <token> << q) * pre_computed_values[r]; <answer> (1 
void drm_edid_get_monitor_name(const struct edid *edid, char *name, int <token> <answer> bufsize) 
<token> name_length = 0; <answer> int 
<token> (bufsize <= 0) <answer> if 
<token> (edid) { <answer> if 
<token> buf[13]; <answer> char 
<token> drm_edid drm_edid = { <answer> struct 
.edid <token> edid, <answer> = 
.size = <token> <answer> edid_size(edid), 
name_length = min(get_monitor_name(&drm_edid, buf), <token> - 1); <answer> bufsize 
<token> buf, name_length); <answer> memcpy(name, 
name[name_length] = <token> <answer> '\0'; 
static void clear_eld(struct <token> *connector) <answer> drm_connector 
memset(connector->eld, <token> sizeof(connector->eld)); <answer> 0, 
<token> = false; <answer> connector->latency_present[0] 
<token> = false; <answer> connector->latency_present[1] 
connector->video_latency[0] <token> 0; <answer> = 
connector->audio_latency[0] <token> 0; <answer> = 
connector->video_latency[1] = <token> <answer> 0; 
connector->audio_latency[1] <token> 0; <answer> = 
void <token> struct cea_sad *cta_sad, u8 *sad) <answer> drm_edid_cta_sad_get(const 
sad[0] = cta_sad->format << 3 <token> cta_sad->channels; <answer> | 
<token> = cta_sad->freq; <answer> sad[1] 
sad[2] = <token> <answer> cta_sad->byte2; 
void drm_edid_cta_sad_set(struct cea_sad *cta_sad, const <token> *sad) <answer> u8 
cta_sad->format = (sad[0] & 0x78) >> <token> <answer> 3; 
cta_sad->channels = sad[0] & <token> <answer> 0x07; 
cta_sad->freq = sad[1] & <token> <answer> 0x7f; 
<token> = sad[2]; <answer> cta_sad->byte2 
static void drm_edid_to_eld(struct drm_connector <token> <answer> *connector, 
<token> struct drm_edid *drm_edid) <answer> const 
const struct drm_display_info *info <token> &connector->display_info; <answer> = 
const <token> cea_db *db; <answer> struct 
<token> cea_db_iter iter; <answer> struct 
uint8_t *eld = <token> <answer> connector->eld; 
<token> total_sad_count = 0; <answer> int 
<token> mnl; <answer> int 
if <token> <answer> (!drm_edid) 
mnl <token> get_monitor_name(drm_edid, &eld[DRM_ELD_MONITOR_NAME_STRING]); <answer> = 
drm_dbg_kms(connector->dev, "[CONNECTOR:%d:%s] ELD monitor <token> <answer> %s\n", 
connector->base.id, <token> <answer> connector->name, 
eld[DRM_ELD_CEA_EDID_VER_MNL] <token> info->cea_rev << DRM_ELD_CEA_EDID_VER_SHIFT; <answer> = 
eld[DRM_ELD_CEA_EDID_VER_MNL] |= <token> <answer> mnl; 
<token> = DRM_ELD_VER_CEA861D; <answer> eld[DRM_ELD_VER] 
eld[DRM_ELD_MANUFACTURER_NAME0] = <token> <answer> drm_edid->edid->mfg_id[0]; 
eld[DRM_ELD_MANUFACTURER_NAME1] <token> drm_edid->edid->mfg_id[1]; <answer> = 
<token> = drm_edid->edid->prod_code[0]; <answer> eld[DRM_ELD_PRODUCT_CODE0] 
<token> = drm_edid->edid->prod_code[1]; <answer> eld[DRM_ELD_PRODUCT_CODE1] 
<token> &iter); <answer> cea_db_iter_edid_begin(drm_edid, 
<token> &iter) { <answer> cea_db_iter_for_each(db, 
const <token> *data = cea_db_data(db); <answer> u8 
<token> len = cea_db_payload_len(db); <answer> int 
int <token> <answer> sad_count; 
<token> (cea_db_tag(db)) { <answer> switch 
case <token> <answer> CTA_DB_AUDIO: 
int drm_edid_to_sad(const struct edid *edid, <token> cea_sad **sads) <answer> struct 
<token> drm_edid drm_edid; <answer> struct 
return _drm_edid_to_sad(drm_edid_legacy_init(&drm_edid, edid), <token> <answer> sads); 
static <token> _drm_edid_to_speaker_allocation(const struct drm_edid *drm_edid, <answer> int 
u8 <token> <answer> **sadb) 
const struct cea_db <token> <answer> *db; 
<token> cea_db_iter iter; <answer> struct 
<token> count = 0; <answer> int 
cea_db_iter_edid_begin(drm_edid, <token> <answer> &iter); 
cea_db_iter_for_each(db, <token> { <answer> &iter) 
if (cea_db_tag(db) <token> CTA_DB_SPEAKER && <answer> == 
<token> == 3) { <answer> cea_db_payload_len(db) 
*sadb = <token> cea_db_payload_len(db), <answer> kmemdup(db->data, 
<token> (!*sadb) <answer> if 
<token> -ENOMEM; <answer> return 
count = <token> <answer> cea_db_payload_len(db); 
DRM_DEBUG_KMS("Found %d Speaker <token> Data Blocks\n", count); <answer> Allocation 
<token> count; <answer> return 
int drm_edid_to_speaker_allocation(const <token> edid *edid, u8 **sadb) <answer> struct 
<token> drm_edid drm_edid; <answer> struct 
return <token> edid), <answer> _drm_edid_to_speaker_allocation(drm_edid_legacy_init(&drm_edid, 
int drm_av_sync_delay(struct drm_connector <token> <answer> *connector, 
const struct drm_display_mode <token> <answer> *mode) 
int <token> = !!(mode->flags & DRM_MODE_FLAG_INTERLACE); <answer> i 
<token> a, v; <answer> int 
<token> (!connector->latency_present[0]) <answer> if 
return <token> <answer> 0; 
<token> (!connector->latency_present[1]) <answer> if 
<token> = 0; <answer> i 
a = <token> <answer> connector->audio_latency[i]; 
v = <token> <answer> connector->video_latency[i]; 
if (a == <token> || v == 255) <answer> 255 
<token> 0; <answer> return 
<token> (a) <answer> if 
a = min(2 * <token> - 1), 500); <answer> (a 
<token> (v) <answer> if 
v = min(2 * (v - 1), <token> <answer> 500); 
return max(v <token> a, 0); <answer> - 
static bool _drm_detect_hdmi_monitor(const struct drm_edid <token> <answer> *drm_edid) 
const <token> cea_db *db; <answer> struct 
<token> cea_db_iter iter; <answer> struct 
bool <token> = false; <answer> hdmi 
cea_db_iter_edid_begin(drm_edid, <token> <answer> &iter); 
cea_db_iter_for_each(db, &iter) <token> <answer> { 
if <token> { <answer> (cea_db_is_hdmi_vsdb(db)) 
<token> = true; <answer> hdmi 
<token> hdmi; <answer> return 
bool drm_detect_hdmi_monitor(const struct edid <token> <answer> *edid) 
struct drm_edid <token> <answer> drm_edid; 
<token> _drm_detect_hdmi_monitor(drm_edid_legacy_init(&drm_edid, edid)); <answer> return 
static bool _drm_detect_monitor_audio(const <token> drm_edid *drm_edid) <answer> struct 
struct drm_edid_iter <token> <answer> edid_iter; 
<token> struct cea_db *db; <answer> const 
struct cea_db_iter <token> <answer> iter; 
const u8 <token> <answer> *edid_ext; 
bool <token> = false; <answer> has_audio 
<token> &edid_iter); <answer> drm_edid_iter_begin(drm_edid, 
<token> &edid_iter) { <answer> drm_edid_iter_for_each(edid_ext, 
if (edid_ext[0] == CEA_EXT) <token> <answer> { 
has_audio = edid_ext[3] <token> EDID_BASIC_AUDIO; <answer> & 
<token> (has_audio) <answer> if 
if (has_audio) <token> <answer> { 
DRM_DEBUG_KMS("Monitor has basic <token> support\n"); <answer> audio 
goto <token> <answer> end; 
<token> &iter); <answer> cea_db_iter_edid_begin(drm_edid, 
cea_db_iter_for_each(db, &iter) <token> <answer> { 
if <token> == CTA_DB_AUDIO) { <answer> (cea_db_tag(db) 
<token> u8 *data = cea_db_data(db); <answer> const 
<token> <linux/string.h> <answer> #include 
#define OF(args) <token> <answer> args 
#define STATIC <token> <answer> static 
<token> memset <answer> #undef 
<token> memcpy <answer> #undef 
#define memzero(s, n) memset((s), <token> (n)) <answer> 0, 
typedef unsigned <token> uch; <answer> char 
typedef unsigned short <token> <answer> ush; 
<token> unsigned long ulg; <answer> typedef 
static int <token> <answer> fill_inbuf(void) 
if (insize != <token> <answer> 0) 
<token> out of input data"); <answer> error("ran 
inbuf = <token> <answer> input_data; 
<token> = input_len; <answer> insize 
inptr <token> 1; <answer> = 
<token> inbuf[0]; <answer> return 
<token> void flush_window(void) <answer> static 
<token> <linux/device.h> <answer> #include 
#include <token> <answer> <linux/module.h> 
#include <token> <answer> <linux/regulator/consumer.h> 
<token> <linux/spi/spi.h> <answer> #include 
<token> <linux/iio/iio.h> <answer> #include 
#include <token> <answer> <linux/iio/adc/ad_sigma_delta.h> 
#define MAX11205_BIT_SCALE <token> <answer> 15 
<token> MAX11205A_OUT_DATA_RATE 116 <answer> #define 
<token> MAX11205B_OUT_DATA_RATE 13 <answer> #define 
enum <token> { <answer> max11205_chip_type 
struct <token> { <answer> max11205_chip_info 
unsigned int <token> <answer> out_data_rate; 
const char <token> <answer> *name; 
<token> max11205_state { <answer> struct 
const struct max11205_chip_info <token> <answer> *chip_info; 
struct regulator <token> <answer> *vref; 
<token> ad_sigma_delta sd; <answer> struct 
static const struct <token> max11205_sigma_delta_info = { <answer> ad_sigma_delta_info 
.has_registers <token> false, <answer> = 
static int <token> iio_dev *indio_dev, <answer> max11205_read_raw(struct 
struct iio_chan_spec <token> *chan, <answer> const 
int *val, <token> *val2, long mask) <answer> int 
struct max11205_state *st <token> iio_priv(indio_dev); <answer> = 
int <token> <answer> reg_mv; 
switch (mask) <token> <answer> { 
<token> IIO_CHAN_INFO_RAW: <answer> case 
<token> ad_sigma_delta_single_conversion(indio_dev, chan, val); <answer> return 
<token> IIO_CHAN_INFO_SCALE: <answer> case 
<token> = regulator_get_voltage(st->vref); <answer> reg_mv 
if <token> < 0) <answer> (reg_mv 
<token> reg_mv; <answer> return 
<token> /= 1000; <answer> reg_mv 
*val <token> reg_mv; <answer> = 
*val2 = <token> <answer> MAX11205_BIT_SCALE; 
return <token> <answer> IIO_VAL_FRACTIONAL_LOG2; 
<token> IIO_CHAN_INFO_SAMP_FREQ: <answer> case 
*val <token> st->chip_info->out_data_rate; <answer> = 
<token> IIO_VAL_INT; <answer> return 
return <token> <answer> -EINVAL; 
static const struct <token> max11205_iio_info = { <answer> iio_info 
.read_raw = <token> <answer> max11205_read_raw, 
<token> = ad_sd_validate_trigger, <answer> .validate_trigger 
static <token> struct iio_chan_spec max11205_channels[] = { <answer> const 
<token> = IIO_VOLTAGE, <answer> .type 
.indexed = <token> <answer> 1, 
.scan_type = <token> <answer> { 
.sign <token> 's', <answer> = 
.realbits <token> 16, <answer> = 
.storagebits = <token> <answer> 16, 
.endianness <token> IIO_BE, <answer> = 
.info_mask_separate <token> BIT(IIO_CHAN_INFO_RAW) | <answer> = 
BIT(IIO_CHAN_INFO_SAMP_FREQ) <token> <answer> | 
static const struct max11205_chip_info max11205_chip_info[] <token> { <answer> = 
<token> = { <answer> [TYPE_MAX11205A] 
.out_data_rate <token> MAX11205A_OUT_DATA_RATE, <answer> = 
.name = <token> <answer> "max11205a", 
<token> = { <answer> [TYPE_MAX11205B] 
.out_data_rate = <token> <answer> MAX11205B_OUT_DATA_RATE, 
<token> = "max11205b", <answer> .name 
<token> void max11205_reg_disable(void *reg) <answer> static 
<token> int max11205_probe(struct spi_device *spi) <answer> static 
struct <token> *st; <answer> max11205_state 
<token> iio_dev *indio_dev; <answer> struct 
<token> ret; <answer> int 
indio_dev = <token> sizeof(*st)); <answer> devm_iio_device_alloc(&spi->dev, 
if <token> <answer> (!indio_dev) 
return <token> <answer> -ENOMEM; 
<token> = iio_priv(indio_dev); <answer> st 
<token> indio_dev, spi, &max11205_sigma_delta_info); <answer> ad_sd_init(&st->sd, 
st->chip_info <token> device_get_match_data(&spi->dev); <answer> = 
if <token> <answer> (!st->chip_info) 
st->chip_info <token> <answer> = 
(const struct <token> *)spi_get_device_id(spi)->driver_data; <answer> max11205_chip_info 
indio_dev->name = <token> <answer> st->chip_info->name; 
<token> = INDIO_DIRECT_MODE; <answer> indio_dev->modes 
indio_dev->channels = <token> <answer> max11205_channels; 
indio_dev->num_channels <token> 1; <answer> = 
<token> = &max11205_iio_info; <answer> indio_dev->info 
<token> = devm_regulator_get(&spi->dev, "vref"); <answer> st->vref 
<token> (IS_ERR(st->vref)) <answer> if 
return <token> PTR_ERR(st->vref), <answer> dev_err_probe(&spi->dev, 
"Failed to get <token> regulator\n"); <answer> vref 
ret <token> regulator_enable(st->vref); <answer> = 
if <token> <answer> (ret) 
<token> ret; <answer> return 
ret <token> devm_add_action_or_reset(&spi->dev, max11205_reg_disable, st->vref); <answer> = 
if <token> <answer> (ret) 
<token> ret; <answer> return 
ret <token> devm_ad_sd_setup_buffer_and_trigger(&spi->dev, indio_dev); <answer> = 
<token> (ret) <answer> if 
<token> ret; <answer> return 
return devm_iio_device_register(&spi->dev, <token> <answer> indio_dev); 
static const struct spi_device_id max11205_spi_ids[] = <token> <answer> { 
{ <token> (kernel_ulong_t)&max11205_chip_info[TYPE_MAX11205A] }, <answer> "max11205a", 
{ "max11205b", <token> }, <answer> (kernel_ulong_t)&max11205_chip_info[TYPE_MAX11205B] 
{ <token> <answer> } 
MODULE_DEVICE_TABLE(spi, <token> <answer> max11205_spi_ids); 
static const struct <token> max11205_dt_ids[] = { <answer> of_device_id 
.compatible <token> "maxim,max11205a", <answer> = 
.data <token> &max11205_chip_info[TYPE_MAX11205A], <answer> = 
.compatible <token> "maxim,max11205b", <answer> = 
.data = <token> <answer> &max11205_chip_info[TYPE_MAX11205B], 
<token> } <answer> { 
<token> max11205_dt_ids); <answer> MODULE_DEVICE_TABLE(of, 
static <token> spi_driver max11205_spi_driver = { <answer> struct 
.driver = <token> <answer> { 
.name = <token> <answer> "max11205", 
.of_match_table <token> max11205_dt_ids, <answer> = 
<token> = max11205_probe, <answer> .probe 
<token> = max11205_spi_ids, <answer> .id_table 
MODULE_AUTHOR("Ramona <token> <ramona.bolboaca@analog.com>"); <answer> Bolboaca 
MODULE_DESCRIPTION("MAX11205 ADC <token> <answer> driver"); 
<token> v2"); <answer> MODULE_LICENSE("GPL 
<token> <linux/cleanup.h> <answer> #include 
#include <token> <answer> <linux/kernel.h> 
#include <token> <answer> <linux/slab.h> 
<token> <linux/module.h> <answer> #include 
<token> <linux/gpio/driver.h> <answer> #include 
#include <token> <answer> <linux/mfd/core.h> 
#include <token> <answer> <linux/platform_device.h> 
#include <token> <answer> <linux/seq_file.h> 
#include <token> <answer> <linux/mfd/wm831x/core.h> 
<token> <linux/mfd/wm831x/pdata.h> <answer> #include 
#include <token> <answer> <linux/mfd/wm831x/gpio.h> 
#include <token> <answer> <linux/mfd/wm831x/irq.h> 
struct wm831x_gpio <token> <answer> { 
<token> wm831x *wm831x; <answer> struct 
struct <token> gpio_chip; <answer> gpio_chip 
static int wm831x_gpio_direction_in(struct gpio_chip *chip, <token> offset) <answer> unsigned 
struct wm831x_gpio <token> = gpiochip_get_data(chip); <answer> *wm831x_gpio 
<token> wm831x *wm831x = wm831x_gpio->wm831x; <answer> struct 
<token> val = WM831X_GPN_DIR; <answer> int 
<token> (wm831x->has_gpio_ena) <answer> if 
val <token> WM831X_GPN_TRI; <answer> |= 
return wm831x_set_bits(wm831x, WM831X_GPIO1_CONTROL <token> offset, <answer> + 
WM831X_GPN_DIR <token> WM831X_GPN_TRI | <answer> | 
WM831X_GPN_FN_MASK, <token> <answer> val); 
static int wm831x_gpio_get(struct <token> *chip, unsigned offset) <answer> gpio_chip 
struct wm831x_gpio *wm831x_gpio = <token> <answer> gpiochip_get_data(chip); 
struct <token> *wm831x = wm831x_gpio->wm831x; <answer> wm831x 
int <token> <answer> ret; 
ret <token> wm831x_reg_read(wm831x, WM831X_GPIO_LEVEL); <answer> = 
if (ret < <token> <answer> 0) 
return <token> <answer> ret; 
if (ret & 1 << <token> <answer> offset) 
return <token> <answer> 1; 
return <token> <answer> 0; 
static void wm831x_gpio_set(struct gpio_chip *chip, <token> offset, int value) <answer> unsigned 
struct <token> *wm831x_gpio = gpiochip_get_data(chip); <answer> wm831x_gpio 
struct <token> *wm831x = wm831x_gpio->wm831x; <answer> wm831x 
wm831x_set_bits(wm831x, <token> 1 << offset, <answer> WM831X_GPIO_LEVEL, 
<token> << offset); <answer> value 
static int wm831x_gpio_direction_out(struct gpio_chip <token> <answer> *chip, 
unsigned offset, int <token> <answer> value) 
struct wm831x_gpio *wm831x_gpio <token> gpiochip_get_data(chip); <answer> = 
struct <token> *wm831x = wm831x_gpio->wm831x; <answer> wm831x 
int <token> = 0; <answer> val 
<token> ret; <answer> int 
<token> (wm831x->has_gpio_ena) <answer> if 
<token> |= WM831X_GPN_TRI; <answer> val 
ret = wm831x_set_bits(wm831x, <token> + offset, <answer> WM831X_GPIO1_CONTROL 
WM831X_GPN_DIR | WM831X_GPN_TRI <token> <answer> | 
WM831X_GPN_FN_MASK, <token> <answer> val); 
<token> (ret < 0) <answer> if 
return <token> <answer> ret; 
<token> *label __free(kfree) = gpiochip_dup_line_label(chip, i); <answer> char 
<token> (IS_ERR(label)) { <answer> if 
<token> "Failed to duplicate label\n"); <answer> dev_err(wm831x->dev, 
seq_printf(s, <token> gpio-%-3d (%-20.20s) ", <answer> " 
gpio, <token> ?: "Unrequested"); <answer> label 
reg = wm831x_reg_read(wm831x, WM831X_GPIO1_CONTROL + <token> <answer> i); 
if (reg < 0) <token> <answer> { 
<token> control %d read failed: %d\n", <answer> "GPIO 
gpio, <token> <answer> reg); 
<token> '\n'); <answer> seq_putc(s, 
switch (reg & WM831X_GPN_PULL_MASK) <token> <answer> { 
<token> WM831X_GPIO_PULL_NONE: <answer> case 
pull = <token> <answer> "nopull"; 
<token> WM831X_GPIO_PULL_DOWN: <answer> case 
<token> = "pulldown"; <answer> pull 
<token> WM831X_GPIO_PULL_UP: <answer> case 
pull = <token> <answer> "pullup"; 
pull = "INVALID <token> <answer> PULL"; 
switch (i + <token> { <answer> 1) 
<token> 1 ... 3: <answer> case 
case 7 <token> 9: <answer> ... 
if (reg <token> WM831X_GPN_PWR_DOM) <answer> & 
powerdomain = <token> <answer> "VPMIC"; 
powerdomain = <token> <answer> "DBVDD"; 
case 4 <token> 6: <answer> ... 
<token> 10 ... 12: <answer> case 
if <token> & WM831X_GPN_PWR_DOM) <answer> (reg 
powerdomain = <token> <answer> "SYSVDD"; 
powerdomain <token> "DBVDD"; <answer> = 
case 13 <token> 16: <answer> ... 
powerdomain = <token> <answer> "TPVDD"; 
<token> = reg & WM831X_GPN_TRI; <answer> tristated 
<token> (wm831x->has_gpio_ena) <answer> if 
tristated <token> !tristated; <answer> = 
seq_printf(s, " <token> %s %s %s%s\n" <answer> %s 
" %s%s <token> <answer> (0x%4x)\n", 
reg & WM831X_GPN_DIR ? "in" <token> "out", <answer> : 
wm831x_gpio_get(chip, i) <token> "high" : "low", <answer> ? 
reg & WM831X_GPN_POL ? "" : <token> inverted", <answer> " 
reg & <token> ? "open-drain" : "push-pull", <answer> WM831X_GPN_OD 
tristated <token> " tristated" : "", <answer> ? 
#define <token> NULL <answer> wm831x_gpio_dbg_show 
static const struct gpio_chip <token> = { <answer> template_chip 
.label <token> "wm831x", <answer> = 
.owner <token> THIS_MODULE, <answer> = 
<token> = wm831x_gpio_direction_in, <answer> .direction_input 
.get = <token> <answer> wm831x_gpio_get, 
.direction_output = <token> <answer> wm831x_gpio_direction_out, 
.set = <token> <answer> wm831x_gpio_set, 
<token> = wm831x_gpio_to_irq, <answer> .to_irq 
.set_config = <token> <answer> wm831x_set_config, 
.dbg_show = <token> <answer> wm831x_gpio_dbg_show, 
.can_sleep = <token> <answer> true, 
static int <token> platform_device *pdev) <answer> wm831x_gpio_probe(struct 
struct wm831x *wm831x <token> dev_get_drvdata(pdev->dev.parent); <answer> = 
struct wm831x_pdata *pdata <token> &wm831x->pdata; <answer> = 
<token> wm831x_gpio *wm831x_gpio; <answer> struct 
<token> dev_fwnode(pdev->dev.parent)); <answer> device_set_node(&pdev->dev, 
<token> = devm_kzalloc(&pdev->dev, sizeof(*wm831x_gpio), <answer> wm831x_gpio 
if <token> == NULL) <answer> (wm831x_gpio 
return <token> <answer> -ENOMEM; 
wm831x_gpio->wm831x <token> wm831x; <answer> = 
wm831x_gpio->gpio_chip = <token> <answer> template_chip; 
wm831x_gpio->gpio_chip.ngpio <token> wm831x->num_gpio; <answer> = 
wm831x_gpio->gpio_chip.parent <token> &pdev->dev; <answer> = 
if (pdata <token> pdata->gpio_base) <answer> && 
wm831x_gpio->gpio_chip.base <token> pdata->gpio_base; <answer> = 
<token> = -1; <answer> wm831x_gpio->gpio_chip.base 
return <token> &wm831x_gpio->gpio_chip, wm831x_gpio); <answer> devm_gpiochip_add_data(&pdev->dev, 
static struct <token> wm831x_gpio_driver = { <answer> platform_driver 
<token> = "wm831x-gpio", <answer> .driver.name 
.probe <token> wm831x_gpio_probe, <answer> = 
static <token> __init wm831x_gpio_init(void) <answer> int 
<token> platform_driver_register(&wm831x_gpio_driver); <answer> return 
static void __exit <token> <answer> wm831x_gpio_exit(void) 
MODULE_AUTHOR("Mark Brown <token> <answer> <broonie@opensource.wolfsonmicro.com>"); 
MODULE_DESCRIPTION("GPIO interface for <token> PMICs"); <answer> WM831x 
<token> <linux/fs.h> <answer> #include 
<token> <linux/slab.h> <answer> #include 
<token> <linux/posix_acl.h> <answer> #include 
<token> "nfsfh.h" <answer> #include 
#include <token> <answer> "nfsd.h" 
#include <token> <answer> "acl.h" 
#include <token> <answer> "vfs.h" 
#define <token> 0x01 <answer> NFS4_ACL_TYPE_DEFAULT 
<token> NFS4_ACL_DIR 0x02 <answer> #define 
#define <token> 0x04 <answer> NFS4_ACL_OWNER 
static <token> <answer> void 
low_mode_from_nfs4(u32 perm, unsigned short *mode, unsigned int <token> <answer> flags) 
u32 <token> = NFS4_WRITE_MODE; <answer> write_mode 
if (flags <token> NFS4_ACL_DIR) <answer> & 
<token> |= NFS4_ACE_DELETE_CHILD; <answer> write_mode 
<token> = 0; <answer> *mode 
if ((perm <token> NFS4_READ_MODE) == NFS4_READ_MODE) <answer> & 
*mode |= <token> <answer> ACL_READ; 
if ((perm & write_mode) <token> write_mode) <answer> == 
<token> |= ACL_WRITE; <answer> *mode 
if ((perm <token> NFS4_EXECUTE_MODE) == NFS4_EXECUTE_MODE) <answer> & 
*mode <token> ACL_EXECUTE; <answer> |= 
static short ace2type(struct <token> *); <answer> nfs4_ace 
static void <token> posix_acl *, struct nfs4_acl *, <answer> _posix_to_nfsv4_one(struct 
<token> int); <answer> unsigned 
nfsd4_get_nfs4_acl(struct svc_rqst *rqstp, <token> dentry *dentry, <answer> struct 
struct nfs4_acl <token> <answer> **acl) 
struct inode *inode = <token> <answer> d_inode(dentry); 
<token> error = 0; <answer> int 
struct posix_acl *pacl = NULL, *dpacl <token> NULL; <answer> = 
unsigned int flags = <token> <answer> 0; 
int <token> = 0; <answer> size 
pacl = <token> ACL_TYPE_ACCESS); <answer> get_inode_acl(inode, 
<token> (!pacl) <answer> if 
<token> = posix_acl_from_mode(inode->i_mode, GFP_KERNEL); <answer> pacl 
<token> (IS_ERR(pacl)) <answer> if 
return <token> <answer> PTR_ERR(pacl); 
memset(pas, 0, <token> <answer> sizeof(*pas)); 
pas->mask <token> 07; <answer> = 
pe = acl->a_entries + <token> <answer> acl->a_count; 
<token> acl, pe) { <answer> FOREACH_ACL_ENTRY(pa, 
switch (pa->e_tag) <token> <answer> { 
<token> ACL_USER_OBJ: <answer> case 
pas->owner <token> pa->e_perm; <answer> = 
<token> ACL_GROUP_OBJ: <answer> case 
<token> = pa->e_perm; <answer> pas->group 
case <token> <answer> ACL_USER: 
pas->users <token> pa->e_perm; <answer> |= 
case <token> <answer> ACL_GROUP: 
pas->groups |= <token> <answer> pa->e_perm; 
<token> ACL_OTHER: <answer> case 
pas->other <token> pa->e_perm; <answer> = 
case <token> <answer> ACL_MASK: 
<token> = pa->e_perm; <answer> pas->mask 
deny &= pas.users | pas.group <token> pas.groups | pas.other; <answer> | 
<token> (deny) { <answer> if 
<token> = NFS4_ACE_ACCESS_DENIED_ACE_TYPE; <answer> ace->type 
ace->flag = <token> <answer> eflag; 
ace->access_mask = deny_mask_from_posix(deny, <token> <answer> flags); 
ace->whotype = <token> <answer> NFS4_ACL_WHO_OWNER; 
ace->type = <token> <answer> NFS4_ACE_ACCESS_ALLOWED_ACE_TYPE; 
ace->flag <token> eflag; <answer> = 
<token> = mask_from_posix(pa->e_perm, flags | NFS4_ACL_OWNER); <answer> ace->access_mask 
<token> = NFS4_ACL_WHO_OWNER; <answer> ace->whotype 
<token> (pa->e_tag == ACL_USER) { <answer> while 
<token> = ~(pa->e_perm & pas.mask); <answer> deny 
deny &= pas.groups | pas.group | <token> <answer> pas.other; 
if (deny) <token> <answer> { 
ace->type <token> NFS4_ACE_ACCESS_DENIED_ACE_TYPE; <answer> = 
ace->flag = <token> <answer> eflag; 
<token> = deny_mask_from_posix(deny, flags); <answer> ace->access_mask 
ace->whotype <token> NFS4_ACL_WHO_NAMED; <answer> = 
ace->who_uid = <token> <answer> pa->e_uid; 
ace->type <token> NFS4_ACE_ACCESS_ALLOWED_ACE_TYPE; <answer> = 
ace->flag <token> eflag; <answer> = 
ace->access_mask = mask_from_posix(pa->e_perm <token> pas.mask, <answer> & 
ace->whotype <token> NFS4_ACL_WHO_NAMED; <answer> = 
<token> = pa->e_uid; <answer> ace->who_uid 
while (!sorted) <token> <answer> { 
<token> = 1; <answer> sorted 
for (i = start; i < <token> i++) { <answer> end; 
<token> (pace_gt(&pacl->a_entries[i], <answer> if 
<token> { <answer> &pacl->a_entries[i+1])) 
sorted <token> 0; <answer> = 
pacl->a_entries[i + <token> <answer> 1]); 
static <token> <answer> void 
sort_pacl(struct posix_acl <token> <answer> *pacl) 
int <token> j; <answer> i, 
struct <token> { <answer> posix_ace_state 
u32 <token> <answer> allow; 
u32 <token> <answer> deny; 
struct posix_user_ace_state <token> <answer> { 
union <token> <answer> { 
<token> uid; <answer> kuid_t 
<token> gid; <answer> kgid_t 
<token> posix_ace_state perms; <answer> struct 
struct posix_ace_state_array <token> <answer> { 
int <token> <answer> n; 
struct <token> aces[]; <answer> posix_user_ace_state 
struct posix_acl_state <token> <answer> { 
<token> char valid; <answer> unsigned 
struct <token> owner; <answer> posix_ace_state 
struct posix_ace_state <token> <answer> group; 
struct <token> other; <answer> posix_ace_state 
struct posix_ace_state <token> <answer> everyone; 
alloc = sizeof(struct <token> <answer> posix_ace_state_array) 
+ cnt*sizeof(struct <token> <answer> posix_user_ace_state); 
state->users <token> kzalloc(alloc, GFP_KERNEL); <answer> = 
<token> (!state->users) <answer> if 
<token> -ENOMEM; <answer> return 
state->groups <token> kzalloc(alloc, GFP_KERNEL); <answer> = 
<token> (!state->groups) { <answer> if 
return <token> <answer> -ENOMEM; 
return <token> <answer> 0; 
<token> void <answer> static 
free_state(struct posix_acl_state <token> { <answer> *state) 
static inline void <token> posix_acl_state *state, struct posix_ace_state *astate) <answer> add_to_mask(struct 
state->mask.allow <token> astate->allow; <answer> |= 
static struct <token> * <answer> posix_acl 
<token> posix_acl_state *state, unsigned int flags) <answer> posix_state_to_acl(struct 
struct posix_acl_entry <token> <answer> *pace; 
struct <token> *pacl; <answer> posix_acl 
int <token> <answer> nace; 
<token> i; <answer> int 
<token> (!state->valid && (flags & NFS4_ACL_TYPE_DEFAULT)) <answer> if 
return <token> <answer> NULL; 
if (!state->users->n && <token> <answer> !state->groups->n) 
nace <token> 3; <answer> = 
process_one_v4_ace(&default_acl_state, <token> <answer> ace); 
if (!(ace->flag & <token> <answer> NFS4_ACE_INHERIT_ONLY_ACE)) 
process_one_v4_ace(&effective_acl_state, <token> <answer> ace); 
if (default_acl_state.valid) <token> <answer> { 
if (!(default_acl_state.valid <token> ACL_USER_OBJ)) <answer> & 
default_acl_state.owner = <token> <answer> effective_acl_state.owner; 
if (!(default_acl_state.valid & <token> <answer> ACL_GROUP_OBJ)) 
default_acl_state.group = <token> <answer> effective_acl_state.group; 
if (!(default_acl_state.valid & <token> <answer> ACL_OTHER)) 
default_acl_state.other = <token> <answer> effective_acl_state.other; 
*pacl <token> posix_state_to_acl(&effective_acl_state, flags); <answer> = 
if (IS_ERR(*pacl)) <token> <answer> { 
ret <token> PTR_ERR(*pacl); <answer> = 
*pacl = <token> <answer> NULL; 
<token> out_dstate; <answer> goto 
*dpacl <token> posix_state_to_acl(&default_acl_state, <answer> = 
<token> | NFS4_ACL_TYPE_DEFAULT); <answer> flags 
<token> (IS_ERR(*dpacl)) { <answer> if 
<token> = PTR_ERR(*dpacl); <answer> ret 
*dpacl = <token> <answer> NULL; 
*pacl = <token> <answer> NULL; 
<token> out_dstate; <answer> goto 
ret = <token> <answer> 0; 
return <token> <answer> ret; 
<token> nfsd4_acl_to_attr(enum nfs_ftype4 type, struct nfs4_acl *acl, <answer> __be32 
<token> nfsd_attrs *attr) <answer> struct 
<token> host_error; <answer> int 
<token> int flags = 0; <answer> unsigned 
if <token> <answer> (!acl) 
return <token> <answer> nfs_ok; 
if (type <token> NF4DIR) <answer> == 
flags <token> NFS4_ACL_DIR; <answer> = 
host_error = nfs4_acl_nfsv4_to_posix(acl, <token> <answer> &attr->na_pacl, 
<token> flags); <answer> &attr->na_dpacl, 
if <token> == -EINVAL) <answer> (host_error 
return <token> <answer> nfserr_attrnotsupp; 
<token> nfserrno(host_error); <answer> return 
static <token> <answer> short 
ace2type(struct nfs4_ace <token> <answer> *ace) 
switch (ace->whotype) <token> <answer> { 
<token> NFS4_ACL_WHO_NAMED: <answer> case 
return <token> & NFS4_ACE_IDENTIFIER_GROUP ? <answer> (ace->flag 
ACL_GROUP : <token> <answer> ACL_USER); 
case <token> <answer> NFS4_ACL_WHO_OWNER: 
<token> ACL_USER_OBJ; <answer> return 
<token> NFS4_ACL_WHO_GROUP: <answer> case 
<token> ACL_GROUP_OBJ; <answer> return 
<token> NFS4_ACL_WHO_EVERYONE: <answer> case 
return <token> <answer> ACL_OTHER; 
return <token> <answer> -1; 
<token> nfs4_acl_bytes(int entries) <answer> int 
return sizeof(struct nfs4_acl) <token> entries * sizeof(struct nfs4_ace); <answer> + 
static <token> { <answer> struct 
char <token> <answer> *string; 
<token> stringlen; <answer> int 
int <token> <answer> type; 
} <token> = { <answer> s2t_map[] 
.string = <token> <answer> "OWNER@", 
.stringlen = sizeof("OWNER@") - <token> <answer> 1, 
<token> = NFS4_ACL_WHO_OWNER, <answer> .type 
.string = <token> <answer> "GROUP@", 
.stringlen = <token> - 1, <answer> sizeof("GROUP@") 
.type = <token> <answer> NFS4_ACL_WHO_GROUP, 
.string <token> "EVERYONE@", <answer> = 
<token> = sizeof("EVERYONE@") - 1, <answer> .stringlen 
<token> = NFS4_ACL_WHO_EVERYONE, <answer> .type 
nfs4_acl_get_whotype(char *p, <token> len) <answer> u32 
<token> i; <answer> int 
for <token> = 0; i < ARRAY_SIZE(s2t_map); i++) { <answer> (i 
if (s2t_map[i].stringlen <token> len && <answer> == 
0 == memcmp(s2t_map[i].string, p, <token> <answer> len)) 
<token> s2t_map[i].type; <answer> return 
<token> NFS4_ACL_WHO_NAMED; <answer> return 
__be32 <token> xdr_stream *xdr, int who) <answer> nfs4_acl_write_who(struct 
<token> *p; <answer> __be32 
<token> i; <answer> int 
for (i = 0; i <token> ARRAY_SIZE(s2t_map); i++) { <answer> < 
if (s2t_map[i].type <token> who) <answer> != 
p = xdr_reserve_space(xdr, s2t_map[i].stringlen <token> 4); <answer> + 
<token> (!p) <answer> if 
return <token> <answer> nfserr_resource; 
p = xdr_encode_opaque(p, <token> <answer> s2t_map[i].string, 
<token> 0; <answer> return 
return <token> <answer> nfserr_serverfault; 
<token> <sys/types.h> <answer> #include 
<token> <perfmon/pfmlib.h> <answer> #include 
<token> main(void) <answer> int 
return <token> <answer> 0; 
<token> "priv.h" <answer> #include 
<token> <core/memory.h> <answer> #include 
static <token> <answer> void 
nvkm_fault_ntfy_fini(struct nvkm_event *event, int type, int <token> <answer> index) 
<token> nvkm_fault *fault = container_of(event, typeof(*fault), event); <answer> struct 
<token> false); <answer> fault->func->buffer.intr(fault->buffer[index], 
static <token> <answer> void 
nvkm_fault_ntfy_init(struct nvkm_event *event, <token> type, int index) <answer> int 
struct <token> *fault = container_of(event, typeof(*fault), event); <answer> nvkm_fault 
fault->func->buffer.intr(fault->buffer[index], <token> <answer> true); 
<token> const struct nvkm_event_func <answer> static 
nvkm_fault_ntfy = <token> <answer> { 
<token> = nvkm_fault_ntfy_init, <answer> .init 
.fini = <token> <answer> nvkm_fault_ntfy_fini, 
static <token> <answer> void 
<token> nvkm_subdev *subdev) <answer> nvkm_fault_intr(struct 
struct nvkm_fault *fault = <token> <answer> nvkm_fault(subdev); 
return <token> <answer> fault->func->intr(fault); 
static <token> <answer> int 
nvkm_fault_fini(struct nvkm_subdev <token> bool suspend) <answer> *subdev, 
struct nvkm_fault *fault = <token> <answer> nvkm_fault(subdev); 
if <token> <answer> (fault->func->fini) 
return <token> <answer> 0; 
<token> int <answer> static 
nvkm_fault_init(struct nvkm_subdev <token> <answer> *subdev) 
struct nvkm_fault *fault = <token> <answer> nvkm_fault(subdev); 
if <token> <answer> (fault->func->init) 
return <token> <answer> 0; 
static <token> <answer> int 
nvkm_fault_oneinit_buffer(struct nvkm_fault *fault, <token> id) <answer> int 
struct nvkm_subdev *subdev = <token> <answer> &fault->subdev; 
struct nvkm_device *device = <token> <answer> subdev->device; 
struct <token> *buffer; <answer> nvkm_fault_buffer 
<token> ret; <answer> int 
if (!(buffer = <token> GFP_KERNEL))) <answer> kzalloc(sizeof(*buffer), 
return <token> <answer> -ENOMEM; 
buffer->fault = <token> <answer> fault; 
buffer->id = <token> <answer> id; 
<token> = buffer; <answer> fault->buffer[id] 
<token> "buffer %d: %d entries\n", id, buffer->entries); <answer> nvkm_debug(subdev, 
ret = nvkm_memory_new(device, NVKM_MEM_TARGET_INST, <token> * <answer> buffer->entries 
<token> 0x1000, true, <answer> fault->func->buffer.entry_size, 
<token> (ret) <answer> if 
<token> ret; <answer> return 
<token> <linux/greybus.h> <answer> #include 
#include <token> <answer> <linux/cdev.h> 
<token> <linux/fs.h> <answer> #include 
#include <token> <answer> <linux/ioctl.h> 
<token> <linux/uaccess.h> <answer> #include 
#include <token> <answer> "greybus_authentication.h" 
#include <token> <answer> "firmware.h" 
#define <token> 1000 <answer> CAP_TIMEOUT_MS 
<token> NUM_MINORS U8_MAX <answer> #define 
struct gb_cap <token> <answer> { 
<token> device *parent; <answer> struct 
struct gb_connection <token> <answer> *connection; 
<token> kref kref; <answer> struct 
<token> list_head node; <answer> struct 
static void put_cap(struct <token> *cap) <answer> gb_cap 
<token> cap_kref_release); <answer> kref_put(&cap->kref, 
if (!cap->disabled) <token> <answer> { 
<token> = gb_pm_runtime_get_sync(bundle); <answer> ret 
<token> (!ret) { <answer> if 
ret = cap_ioctl(cap, cmd, (void __user <token> <answer> *)arg); 
return <token> <answer> ret; 
static const struct file_operations cap_fops = <token> <answer> { 
.owner <token> THIS_MODULE, <answer> = 
.open = <token> <answer> cap_open, 
.release <token> cap_release, <answer> = 
<token> = cap_ioctl_unlocked, <answer> .unlocked_ioctl 
int gb_cap_connection_init(struct <token> *connection) <answer> gb_connection 
struct gb_cap <token> <answer> *cap; 
<token> ret, minor; <answer> int 
<token> (!connection) <answer> if 
return <token> <answer> 0; 
cap = <token> GFP_KERNEL); <answer> kzalloc(sizeof(*cap), 
<token> (!cap) <answer> if 
<token> -ENOMEM; <answer> return 
<token> = &connection->bundle->dev; <answer> cap->parent 
cap->connection = <token> <answer> connection; 
<token> cap); <answer> gb_connection_set_data(connection, 
list_add(&cap->node, <token> <answer> &cap_list); 
<token> = gb_connection_enable(connection); <answer> ret 
if <token> <answer> (ret) 
<token> err_list_del; <answer> goto 
minor = ida_alloc_max(&cap_minors_map, NUM_MINORS <token> 1, GFP_KERNEL); <answer> - 
if (minor <token> 0) { <answer> < 
<token> = minor; <answer> ret 
goto <token> <answer> err_connection_disable; 
cap->disabled <token> true; <answer> = 
<token> cap_init(void) <answer> int 
int <token> <answer> ret; 
ret <token> class_register(&cap_class); <answer> = 
<token> (ret) <answer> if 
<token> ret; <answer> return 
<token> = alloc_chrdev_region(&cap_dev_num, 0, NUM_MINORS, <answer> ret 
if <token> <answer> (ret) 
goto <token> <answer> err_remove_class; 
return <token> <answer> 0; 
return <token> <answer> ret; 
<token> cap_exit(void) <answer> void 
<token> NUM_MINORS); <answer> unregister_chrdev_region(cap_dev_num, 
#include <token> <answer> <linux/export.h> 
#include <token> <answer> "jedec_ddr.h" 
<token> <drm/drm_fourcc.h> <answer> #include 
#include <token> <answer> <drm/drm_of.h> 
<token> <linux/clk.h> <answer> #include 
<token> <linux/component.h> <answer> #include 
#include <token> <answer> <linux/of.h> 
#include <token> <answer> <linux/of_address.h> 
<token> <linux/of_platform.h> <answer> #include 
<token> <linux/platform_device.h> <answer> #include 
<token> <linux/pm_runtime.h> <answer> #include 
<token> <linux/reset.h> <answer> #include 
<token> <linux/soc/mediatek/mtk-cmdq.h> <answer> #include 
<token> <linux/soc/mediatek/mtk-mmsys.h> <answer> #include 
<token> <linux/soc/mediatek/mtk-mutex.h> <answer> #include 
<token> "mtk_disp_drv.h" <answer> #include 
#include <token> <answer> "mtk_drm_crtc.h" 
<token> "mtk_drm_ddp_comp.h" <answer> #include 
<token> "mtk_drm_drv.h" <answer> #include 
<token> "mtk_ethdr.h" <answer> #include 
<token> MTK_OVL_ADAPTOR_RDMA_MAX_WIDTH 1920 <answer> #define 
#define <token> 4 <answer> MTK_OVL_ADAPTOR_LAYER_NUM 
<token> mtk_ovl_adaptor_comp_type { <answer> enum 
<token> mtk_ovl_adaptor_comp_id { <answer> enum 
<token> ovl_adaptor_comp_match { <answer> struct 
enum mtk_ovl_adaptor_comp_type <token> <answer> type; 
<token> mtk_ddp_comp_id comp_id; <answer> enum 
int <token> <answer> alias_id; 
<token> struct mtk_ddp_comp_funcs *funcs; <answer> const 
<token> mtk_disp_ovl_adaptor { <answer> struct 
struct device <token> <answer> *ovl_adaptor_comp[OVL_ADAPTOR_ID_MAX]; 
struct <token> *mmsys_dev; <answer> device 
bool <token> <answer> children_bound; 
static const char <token> const private_comp_stem[OVL_ADAPTOR_TYPE_NUM] = { <answer> * 
<token> = "ethdr", <answer> [OVL_ADAPTOR_TYPE_ETHDR] 
<token> = "vdo1-rdma", <answer> [OVL_ADAPTOR_TYPE_MDP_RDMA] 
[OVL_ADAPTOR_TYPE_MERGE] = <token> <answer> "merge", 
[OVL_ADAPTOR_TYPE_PADDING] = <token> <answer> "padding", 
static <token> struct mtk_ddp_comp_funcs ethdr = { <answer> const 
<token> = mtk_ethdr_clk_enable, <answer> .clk_enable 
.clk_disable = <token> <answer> mtk_ethdr_clk_disable, 
<token> = mtk_ethdr_start, <answer> .start 
.stop = <token> <answer> mtk_ethdr_stop, 
static const struct mtk_ddp_comp_funcs <token> = { <answer> merge 
.clk_enable = <token> <answer> mtk_merge_clk_enable, 
.clk_disable <token> mtk_merge_clk_disable, <answer> = 
.mode_valid <token> mtk_merge_mode_valid, <answer> = 
static const struct mtk_ddp_comp_funcs padding = <token> <answer> { 
<token> = mtk_padding_clk_enable, <answer> .clk_enable 
<token> = mtk_padding_clk_disable, <answer> .clk_disable 
.start = <token> <answer> mtk_padding_start, 
.stop <token> mtk_padding_stop, <answer> = 
<token> const struct mtk_ddp_comp_funcs rdma = { <answer> static 
.power_on = <token> <answer> mtk_mdp_rdma_power_on, 
<token> = mtk_mdp_rdma_power_off, <answer> .power_off 
.clk_enable = <token> <answer> mtk_mdp_rdma_clk_enable, 
.clk_disable = <token> <answer> mtk_mdp_rdma_clk_disable, 
static const struct ovl_adaptor_comp_match comp_matches[OVL_ADAPTOR_ID_MAX] <token> { <answer> = 
[OVL_ADAPTOR_ETHDR0] = { <token> DDP_COMPONENT_ETHDR_MIXER, 0, &ethdr }, <answer> OVL_ADAPTOR_TYPE_ETHDR, 
[OVL_ADAPTOR_MDP_RDMA0] = { OVL_ADAPTOR_TYPE_MDP_RDMA, DDP_COMPONENT_MDP_RDMA0, <token> &rdma }, <answer> 0, 
[OVL_ADAPTOR_MDP_RDMA1] <token> { OVL_ADAPTOR_TYPE_MDP_RDMA, DDP_COMPONENT_MDP_RDMA1, 1, &rdma }, <answer> = 
[OVL_ADAPTOR_MDP_RDMA2] = { <token> DDP_COMPONENT_MDP_RDMA2, 2, &rdma }, <answer> OVL_ADAPTOR_TYPE_MDP_RDMA, 
[OVL_ADAPTOR_MDP_RDMA3] = <token> OVL_ADAPTOR_TYPE_MDP_RDMA, DDP_COMPONENT_MDP_RDMA3, 3, &rdma }, <answer> { 
[OVL_ADAPTOR_MDP_RDMA4] = { OVL_ADAPTOR_TYPE_MDP_RDMA, DDP_COMPONENT_MDP_RDMA4, <token> &rdma }, <answer> 4, 
[OVL_ADAPTOR_MDP_RDMA5] <token> { OVL_ADAPTOR_TYPE_MDP_RDMA, DDP_COMPONENT_MDP_RDMA5, 5, &rdma }, <answer> = 
<token> = { OVL_ADAPTOR_TYPE_MDP_RDMA, DDP_COMPONENT_MDP_RDMA6, 6, &rdma }, <answer> [OVL_ADAPTOR_MDP_RDMA6] 
<token> = { OVL_ADAPTOR_TYPE_MDP_RDMA, DDP_COMPONENT_MDP_RDMA7, 7, &rdma }, <answer> [OVL_ADAPTOR_MDP_RDMA7] 
[OVL_ADAPTOR_MERGE0] = <token> OVL_ADAPTOR_TYPE_MERGE, DDP_COMPONENT_MERGE1, 1, &merge }, <answer> { 
[OVL_ADAPTOR_MERGE1] = { <token> DDP_COMPONENT_MERGE2, 2, &merge }, <answer> OVL_ADAPTOR_TYPE_MERGE, 
[OVL_ADAPTOR_MERGE2] = { OVL_ADAPTOR_TYPE_MERGE, DDP_COMPONENT_MERGE3, 3, <token> }, <answer> &merge 
[OVL_ADAPTOR_MERGE3] = { OVL_ADAPTOR_TYPE_MERGE, DDP_COMPONENT_MERGE4, 4, &merge <token> <answer> }, 
[OVL_ADAPTOR_PADDING0] = <token> OVL_ADAPTOR_TYPE_PADDING, DDP_COMPONENT_PADDING0, 0, &padding }, <answer> { 
[OVL_ADAPTOR_PADDING1] = { OVL_ADAPTOR_TYPE_PADDING, <token> 1, &padding }, <answer> DDP_COMPONENT_PADDING1, 
[OVL_ADAPTOR_PADDING2] = { OVL_ADAPTOR_TYPE_PADDING, DDP_COMPONENT_PADDING2, <token> &padding }, <answer> 2, 
[OVL_ADAPTOR_PADDING3] = { OVL_ADAPTOR_TYPE_PADDING, DDP_COMPONENT_PADDING3, <token> &padding }, <answer> 3, 
[OVL_ADAPTOR_PADDING4] <token> { OVL_ADAPTOR_TYPE_PADDING, DDP_COMPONENT_PADDING4, 4, &padding }, <answer> = 
[OVL_ADAPTOR_PADDING5] = { OVL_ADAPTOR_TYPE_PADDING, DDP_COMPONENT_PADDING5, 5, &padding <token> <answer> }, 
[OVL_ADAPTOR_PADDING6] = { OVL_ADAPTOR_TYPE_PADDING, <token> 6, &padding }, <answer> DDP_COMPONENT_PADDING6, 
[OVL_ADAPTOR_PADDING7] = <token> OVL_ADAPTOR_TYPE_PADDING, DDP_COMPONENT_PADDING7, 7, &padding }, <answer> { 
void mtk_ovl_adaptor_layer_config(struct device *dev, unsigned int <token> <answer> idx, 
<token> mtk_plane_state *state, <answer> struct 
struct <token> *cmdq_pkt) <answer> cmdq_pkt 
<token> mtk_disp_ovl_adaptor *ovl_adaptor = dev_get_drvdata(dev); <answer> struct 
struct mtk_plane_pending_state *pending <token> &state->pending; <answer> = 
struct <token> rdma_config = {0}; <answer> mtk_mdp_rdma_cfg 
<token> device *rdma_l; <answer> struct 
<token> device *rdma_r; <answer> struct 
<token> device *merge; <answer> struct 
<token> device *ethdr; <answer> struct 
<token> struct drm_format_info *fmt_info = drm_format_info(pending->format); <answer> const 
<token> use_dual_pipe = false; <answer> bool 
unsigned int <token> <answer> align_width; 
unsigned int <token> = 0; <answer> l_w 
unsigned int r_w = <token> <answer> 0; 
dev_dbg(dev, "%s+ idx:%d, enable:%d, fmt:0x%x\n", __func__, <token> <answer> idx, 
pending->enable, <token> <answer> pending->format); 
dev_dbg(dev, "addr 0x%pad, fb w:%d, <token> <answer> {%d,%d,%d,%d}\n", 
<token> (pending->pitch / fmt_info->cpp[0]), <answer> &pending->addr, 
pending->x, pending->y, <token> pending->height); <answer> pending->width, 
<token> = ovl_adaptor->ovl_adaptor_comp[OVL_ADAPTOR_MDP_RDMA0 + 2 * idx]; <answer> rdma_l 
rdma_r = ovl_adaptor->ovl_adaptor_comp[OVL_ADAPTOR_MDP_RDMA0 + 2 <token> idx + 1]; <answer> * 
<token> = ovl_adaptor->ovl_adaptor_comp[OVL_ADAPTOR_MERGE0 + idx]; <answer> merge 
ethdr <token> ovl_adaptor->ovl_adaptor_comp[OVL_ADAPTOR_ETHDR0]; <answer> = 
if (!pending->enable) <token> <answer> { 
<token> cmdq_pkt); <answer> mtk_merge_stop_cmdq(merge, 
<token> cmdq_pkt); <answer> mtk_mdp_rdma_stop(rdma_l, 
<token> cmdq_pkt); <answer> mtk_mdp_rdma_stop(rdma_r, 
mtk_ethdr_layer_config(ethdr, <token> state, cmdq_pkt); <answer> idx, 
static inline void <token> device *dev, int num) <answer> power_off(struct 
struct mtk_disp_ovl_adaptor <token> = dev_get_drvdata(dev); <answer> *ovl_adaptor 
int <token> <answer> i; 
if (num > <token> <answer> OVL_ADAPTOR_ID_MAX) 
num <token> OVL_ADAPTOR_ID_MAX; <answer> = 
for (i = num <token> 1; i >= 0; i--) { <answer> - 
if <token> || <answer> (!ovl_adaptor->ovl_adaptor_comp[i] 
int mtk_ovl_adaptor_power_on(struct device <token> <answer> *dev) 
<token> mtk_disp_ovl_adaptor *ovl_adaptor = dev_get_drvdata(dev); <answer> struct 
<token> i, ret; <answer> int 
for (i = <token> i < OVL_ADAPTOR_ID_MAX; i++) { <answer> 0; 
<token> (!ovl_adaptor->ovl_adaptor_comp[i] || <answer> if 
<token> = comp_matches[i].funcs->power_on(ovl_adaptor->ovl_adaptor_comp[i]); <answer> ret 
if (ret <token> 0) { <answer> < 
dev_err(dev, "Failed to enable power domain %d, err %d\n", <token> ret); <answer> i, 
<token> i); <answer> power_off(dev, 
return <token> <answer> ret; 
<token> 0; <answer> return 
void mtk_ovl_adaptor_power_off(struct <token> *dev) <answer> device 
power_off(dev, <token> <answer> OVL_ADAPTOR_ID_MAX); 
<token> mtk_ovl_adaptor_clk_enable(struct device *dev) <answer> int 
struct mtk_disp_ovl_adaptor *ovl_adaptor <token> dev_get_drvdata(dev); <answer> = 
<token> device *comp; <answer> struct 
<token> ret; <answer> int 
int <token> <answer> i; 
for (i = <token> i < OVL_ADAPTOR_ID_MAX; i++) { <answer> 0; 
<token> = ovl_adaptor->ovl_adaptor_comp[i]; <answer> comp 
<token> (!comp || !comp_matches[i].funcs->clk_enable) <answer> if 
<token> = comp_matches[i].funcs->clk_enable(comp); <answer> ret 
<token> (ret) { <answer> if 
<token> "Failed to enable clock %d, err %d\n", i, ret); <answer> dev_err(dev, 
<token> (--i >= 0) <answer> while 
return <token> <answer> ret; 
return <token> <answer> 0; 
void mtk_ovl_adaptor_clk_disable(struct <token> *dev) <answer> device 
<token> mtk_disp_ovl_adaptor *ovl_adaptor = dev_get_drvdata(dev); <answer> struct 
<token> device *comp; <answer> struct 
int <token> <answer> i; 
for (i = <token> i < OVL_ADAPTOR_ID_MAX; i++) { <answer> 0; 
comp <token> ovl_adaptor->ovl_adaptor_comp[i]; <answer> = 
if (!comp <token> !comp_matches[i].funcs->clk_disable) <answer> || 
if (i < <token> <answer> OVL_ADAPTOR_MERGE0) 
enum <token> mtk_ovl_adaptor_mode_valid(struct device *dev, <answer> drm_mode_status 
const struct <token> *mode) <answer> drm_display_mode 
<token> i; <answer> int 
struct mtk_disp_ovl_adaptor *ovl_adaptor = <token> <answer> dev_get_drvdata(dev); 
for (i = 0; i < OVL_ADAPTOR_ID_MAX; <token> { <answer> i++) 
<token> = ovl_adaptor->ovl_adaptor_comp[i]; <answer> dev 
if <token> || !comp_matches[i].funcs->mode_valid) <answer> (!dev 
return <token> mode); <answer> comp_matches[i].funcs->mode_valid(dev, 
return <token> <answer> MODE_OK; 
unsigned int mtk_ovl_adaptor_layer_nr(struct device <token> <answer> *dev) 
return <token> <answer> MTK_OVL_ADAPTOR_LAYER_NUM; 
<token> device *mtk_ovl_adaptor_dma_dev_get(struct device *dev) <answer> struct 
struct mtk_disp_ovl_adaptor *ovl_adaptor <token> dev_get_drvdata(dev); <answer> = 
<token> ovl_adaptor->ovl_adaptor_comp[OVL_ADAPTOR_MDP_RDMA0]; <answer> return 
void mtk_ovl_adaptor_register_vblank_cb(struct <token> *dev, void (*vblank_cb)(void *), <answer> device 
void <token> <answer> *vblank_cb_data) 
struct mtk_disp_ovl_adaptor <token> = dev_get_drvdata(dev); <answer> *ovl_adaptor 
<token> vblank_cb_data); <answer> vblank_cb, 
void mtk_ovl_adaptor_unregister_vblank_cb(struct device <token> <answer> *dev) 
<token> mtk_disp_ovl_adaptor *ovl_adaptor = dev_get_drvdata(dev); <answer> struct 
void <token> device *dev) <answer> mtk_ovl_adaptor_enable_vblank(struct 
struct mtk_disp_ovl_adaptor <token> = dev_get_drvdata(dev); <answer> *ovl_adaptor 
void mtk_ovl_adaptor_disable_vblank(struct <token> *dev) <answer> device 
struct mtk_disp_ovl_adaptor *ovl_adaptor = <token> <answer> dev_get_drvdata(dev); 
<token> u32 *mtk_ovl_adaptor_get_formats(struct device *dev) <answer> const 
struct mtk_disp_ovl_adaptor *ovl_adaptor = <token> <answer> dev_get_drvdata(dev); 
return <token> <answer> mtk_mdp_rdma_get_formats(ovl_adaptor->ovl_adaptor_comp[OVL_ADAPTOR_MDP_RDMA0]); 
size_t <token> device *dev) <answer> mtk_ovl_adaptor_get_num_formats(struct 
struct <token> *ovl_adaptor = dev_get_drvdata(dev); <answer> mtk_disp_ovl_adaptor 
return <token> <answer> mtk_mdp_rdma_get_num_formats(ovl_adaptor->ovl_adaptor_comp[OVL_ADAPTOR_MDP_RDMA0]); 
<token> mtk_ovl_adaptor_add_comp(struct device *dev, struct mtk_mutex *mutex) <answer> void 
int <token> <answer> i; 
<token> mtk_disp_ovl_adaptor *ovl_adaptor = dev_get_drvdata(dev); <answer> struct 
for (i = 0; i < OVL_ADAPTOR_ID_MAX; i++) <token> <answer> { 
if <token> <answer> (!ovl_adaptor->ovl_adaptor_comp[i]) 
mtk_mutex_add_comp(mutex, <token> <answer> comp_matches[i].comp_id); 
void <token> device *dev, struct mtk_mutex *mutex) <answer> mtk_ovl_adaptor_remove_comp(struct 
int <token> <answer> i; 
<token> mtk_disp_ovl_adaptor *ovl_adaptor = dev_get_drvdata(dev); <answer> struct 
for (i = 0; <token> < OVL_ADAPTOR_ID_MAX; i++) { <answer> i 
<token> (!ovl_adaptor->ovl_adaptor_comp[i]) <answer> if 
mtk_mutex_remove_comp(mutex, <token> <answer> comp_matches[i].comp_id); 
void mtk_ovl_adaptor_connect(struct <token> *dev, struct device *mmsys_dev, unsigned int next) <answer> device 
<token> DDP_COMPONENT_ETHDR_MIXER, next); <answer> mtk_mmsys_ddp_connect(mmsys_dev, 
mtk_mmsys_ddp_connect(mmsys_dev, <token> DDP_COMPONENT_MERGE1); <answer> DDP_COMPONENT_MDP_RDMA0, 
mtk_mmsys_ddp_connect(mmsys_dev, <token> DDP_COMPONENT_MERGE1); <answer> DDP_COMPONENT_MDP_RDMA1, 
mtk_mmsys_ddp_connect(mmsys_dev, DDP_COMPONENT_MDP_RDMA2, <token> <answer> DDP_COMPONENT_MERGE2); 
mtk_mmsys_ddp_connect(mmsys_dev, <token> DDP_COMPONENT_ETHDR_MIXER); <answer> DDP_COMPONENT_MERGE1, 
<token> DDP_COMPONENT_MERGE2, DDP_COMPONENT_ETHDR_MIXER); <answer> mtk_mmsys_ddp_connect(mmsys_dev, 
mtk_mmsys_ddp_connect(mmsys_dev, <token> DDP_COMPONENT_ETHDR_MIXER); <answer> DDP_COMPONENT_MERGE3, 
<token> DDP_COMPONENT_MERGE4, DDP_COMPONENT_ETHDR_MIXER); <answer> mtk_mmsys_ddp_connect(mmsys_dev, 
void mtk_ovl_adaptor_disconnect(struct <token> *dev, struct device *mmsys_dev, unsigned int next) <answer> device 
<token> DDP_COMPONENT_ETHDR_MIXER, next); <answer> mtk_mmsys_ddp_disconnect(mmsys_dev, 
<token> DDP_COMPONENT_MDP_RDMA0, DDP_COMPONENT_MERGE1); <answer> mtk_mmsys_ddp_disconnect(mmsys_dev, 
<token> DDP_COMPONENT_MDP_RDMA1, DDP_COMPONENT_MERGE1); <answer> mtk_mmsys_ddp_disconnect(mmsys_dev, 
mtk_mmsys_ddp_disconnect(mmsys_dev, <token> DDP_COMPONENT_MERGE2); <answer> DDP_COMPONENT_MDP_RDMA2, 
mtk_mmsys_ddp_disconnect(mmsys_dev, <token> DDP_COMPONENT_ETHDR_MIXER); <answer> DDP_COMPONENT_MERGE1, 
mtk_mmsys_ddp_disconnect(mmsys_dev, DDP_COMPONENT_MERGE2, <token> <answer> DDP_COMPONENT_ETHDR_MIXER); 
mtk_mmsys_ddp_disconnect(mmsys_dev, <token> DDP_COMPONENT_ETHDR_MIXER); <answer> DDP_COMPONENT_MERGE3, 
mtk_mmsys_ddp_disconnect(mmsys_dev, DDP_COMPONENT_MERGE4, <token> <answer> DDP_COMPONENT_ETHDR_MIXER); 
<token> int ovl_adaptor_comp_get_id(struct device *dev, struct device_node *node, <answer> static 
enum mtk_ovl_adaptor_comp_type <token> <answer> type) 
int alias_id = of_alias_get_id(node, <token> <answer> private_comp_stem[type]); 
<token> i; <answer> int 
for (i = 0; i < <token> i++) <answer> ARRAY_SIZE(comp_matches); 
if (comp_matches[i].type == <token> && <answer> type 
<token> == alias_id) <answer> comp_matches[i].alias_id 
<token> i; <answer> return 
dev_warn(dev, "Failed to get id. type: %d, alias: <token> type, alias_id); <answer> %d\n", 
return <token> <answer> -EINVAL; 
static const struct <token> mtk_ovl_adaptor_comp_dt_ids[] = { <answer> of_device_id 
{ .compatible = "mediatek,mt8188-disp-padding", <token> = (void *)OVL_ADAPTOR_TYPE_PADDING }, <answer> .data 
{ .compatible = "mediatek,mt8195-disp-ethdr", .data = (void *)OVL_ADAPTOR_TYPE_ETHDR <token> <answer> }, 
{ <token> = "mediatek,mt8195-disp-merge", .data = (void *)OVL_ADAPTOR_TYPE_MERGE }, <answer> .compatible 
<token> .compatible = "mediatek,mt8195-vdo1-rdma", .data = (void *)OVL_ADAPTOR_TYPE_MDP_RDMA }, <answer> { 
#include <token> <answer> "ubifs.h" 
<token> int dbg_check_orphans(struct ubifs_info *c); <answer> static 
static struct ubifs_orphan *orphan_add(struct <token> *c, ino_t inum, <answer> ubifs_info 
struct ubifs_orphan <token> <answer> *parent_orphan) 
<token> ubifs_orphan *orphan, *o; <answer> struct 
struct rb_node <token> *parent = NULL; <answer> **p, 
<token> = kzalloc(sizeof(struct ubifs_orphan), GFP_NOFS); <answer> orphan 
<token> (!orphan) <answer> if 
return <token> <answer> ERR_PTR(-ENOMEM); 
orphan->inum <token> inum; <answer> = 
orphan->new <token> 1; <answer> = 
<token> (c->tot_orphans >= c->max_orphans) { <answer> if 
return <token> <answer> ERR_PTR(-ENFILE); 
p <token> &c->orph_tree.rb_node; <answer> = 
<token> (*p) { <answer> while 
parent <token> *p; <answer> = 
o = <token> struct ubifs_orphan, rb); <answer> rb_entry(parent, 
if (inum < <token> <answer> o->inum) 
p = <token> <answer> &(*p)->rb_left; 
else <token> (inum > o->inum) <answer> if 
p <token> &(*p)->rb_right; <answer> = 
<token> { <answer> else 
ubifs_err(c, <token> twice"); <answer> "orphaned 
<token> ERR_PTR(-EINVAL); <answer> return 
c->tot_orphans += <token> <answer> 1; 
c->new_orphans += <token> <answer> 1; 
<token> parent, p); <answer> rb_link_node(&orphan->rb, 
<token> &c->orph_tree); <answer> rb_insert_color(&orphan->rb, 
list_add_tail(&orphan->list, <token> <answer> &c->orph_list); 
<token> &c->orph_new); <answer> list_add_tail(&orphan->new_list, 
<token> (parent_orphan) { <answer> if 
dbg_gen("ino %lu", <token> long)inum); <answer> (unsigned 
return <token> <answer> orphan; 
static <token> ubifs_orphan *lookup_orphan(struct ubifs_info *c, ino_t inum) <answer> struct 
struct <token> *o; <answer> ubifs_orphan 
<token> rb_node *p; <answer> struct 
p <token> c->orph_tree.rb_node; <answer> = 
while (p) <token> <answer> { 
o = rb_entry(p, struct <token> rb); <answer> ubifs_orphan, 
if (inum <token> o->inum) <answer> < 
p <token> p->rb_left; <answer> = 
else if (inum <token> o->inum) <answer> > 
p = <token> <answer> p->rb_right; 
<token> { <answer> else 
<token> o; <answer> return 
<token> NULL; <answer> return 
static void __orphan_drop(struct ubifs_info *c, <token> ubifs_orphan *o) <answer> struct 
rb_erase(&o->rb, <token> <answer> &c->orph_tree); 
<token> -= 1; <answer> c->tot_orphans 
if <token> { <answer> (o->new) 
c->new_orphans <token> 1; <answer> -= 
<token> void orphan_delete(struct ubifs_info *c, struct ubifs_orphan *orph) <answer> static 
if (orph->del) <token> <answer> { 
dbg_gen("deleted twice ino %lu", (unsigned <token> <answer> long)orph->inum); 
if <token> { <answer> (orph->cmt) 
orph->del = <token> <answer> 1; 
orph->dnext <token> c->orph_dnext; <answer> = 
c->orph_dnext = <token> <answer> orph; 
dbg_gen("delete <token> ino %lu", (unsigned long)orph->inum); <answer> later 
<token> orph); <answer> __orphan_drop(c, 
int ubifs_add_orphan(struct <token> *c, ino_t inum) <answer> ubifs_info 
int err = <token> <answer> 0; 
<token> xattr_inum; <answer> ino_t 
union <token> key; <answer> ubifs_key 
struct ubifs_dent_node <token> *pxent = NULL; <answer> *xent, 
<token> fscrypt_name nm = {0}; <answer> struct 
struct ubifs_orphan <token> <answer> *xattr_orphan; 
struct <token> *orphan; <answer> ubifs_orphan 
orphan = orphan_add(c, inum, <token> <answer> NULL); 
if <token> <answer> (IS_ERR(orphan)) 
<token> PTR_ERR(orphan); <answer> return 
<token> &key, inum); <answer> lowest_xent_key(c, 
while (1) <token> <answer> { 
xent = <token> &key, &nm); <answer> ubifs_tnc_next_ent(c, 
if <token> { <answer> (IS_ERR(xent)) 
err = <token> <answer> PTR_ERR(xent); 
if (err <token> -ENOENT) <answer> == 
<token> err; <answer> return 
<token> = xent->name; <answer> fname_name(&nm) 
<token> = le16_to_cpu(xent->nlen); <answer> fname_len(&nm) 
xattr_inum = <token> <answer> le64_to_cpu(xent->inum); 
xattr_orphan = orphan_add(c, xattr_inum, <token> <answer> orphan); 
if <token> { <answer> (IS_ERR(xattr_orphan)) 
<token> PTR_ERR(xattr_orphan); <answer> return 
pxent <token> xent; <answer> = 
<token> &xent->key, &key); <answer> key_read(c, 
<token> 0; <answer> return 
void <token> ubifs_info *c, ino_t inum) <answer> ubifs_delete_orphan(struct 
struct ubifs_orphan *orph, <token> *tmp_o; <answer> *child_orph, 
orph = lookup_orphan(c, <token> <answer> inum); 
if <token> { <answer> (!orph) 
ubifs_err(c, "missing orphan <token> %lu", (unsigned long)inum); <answer> ino 
<token> tmp_o, &orph->child_list, child_list) { <answer> list_for_each_entry_safe(child_orph, 
orphan_delete(c, <token> <answer> child_orph); 
orphan_delete(c, <token> <answer> orph); 
int ubifs_orphan_start_commit(struct ubifs_info <token> <answer> *c) 
<token> ubifs_orphan *orphan, **last; <answer> struct 
last = <token> <answer> &c->orph_cnext; 
list_for_each_entry(orphan, &c->orph_new, <token> { <answer> new_list) 
ubifs_assert(c, <token> <answer> orphan->new); 
<token> !orphan->cmt); <answer> ubifs_assert(c, 
<token> = 0; <answer> orphan->new 
orphan->cmt = <token> <answer> 1; 
*last <token> orphan; <answer> = 
last = <token> <answer> &orphan->cnext; 
*last = <token> <answer> NULL; 
<token> = c->new_orphans; <answer> c->cmt_orphans 
<token> = 0; <answer> c->new_orphans 
dbg_cmt("%d orphans <token> commit", c->cmt_orphans); <answer> to 
if (c->tot_orphans == <token> <answer> 0) 
c->no_orphs = <token> <answer> 1; 
c->no_orphs <token> 0; <answer> = 
<token> 0; <answer> return 
static int avail_orphs(struct <token> *c) <answer> ubifs_info 
int avail_lebs, <token> gap; <answer> avail, 
avail_lebs = c->orph_lebs - (c->ohead_lnum - c->orph_first) - <token> <answer> 1; 
avail <token> avail_lebs * <answer> = 
<token> - UBIFS_ORPH_NODE_SZ) / sizeof(__le64)); <answer> ((c->leb_size 
gap = <token> - c->ohead_offs; <answer> c->leb_size 
<token> (gap >= UBIFS_ORPH_NODE_SZ + sizeof(__le64)) <answer> if 
<token> += (gap - UBIFS_ORPH_NODE_SZ) / sizeof(__le64); <answer> avail 
<token> avail; <answer> return 
static int tot_avail_orphs(struct <token> *c) <answer> ubifs_info 
int <token> avail; <answer> avail_lebs, 
avail_lebs <token> c->orph_lebs; <answer> = 
avail <token> avail_lebs * <answer> = 
((c->leb_size - UBIFS_ORPH_NODE_SZ) <token> sizeof(__le64)); <answer> / 
<token> avail / 2; <answer> return 
static int do_write_orph_node(struct ubifs_info *c, int <token> int atomic) <answer> len, 
int err = <token> <answer> 0; 
<token> (atomic) { <answer> if 
ubifs_assert(c, <token> == 0); <answer> c->ohead_offs 
ubifs_prepare_node(c, <token> len, 1); <answer> c->orph_buf, 
<token> = ALIGN(len, c->min_io_size); <answer> len 
<token> = ubifs_leb_change(c, c->ohead_lnum, c->orph_buf, len); <answer> err 
} <token> { <answer> else 
if <token> == 0) { <answer> (c->ohead_offs 
static <token> write_orph_node(struct ubifs_info *c, int atomic) <answer> int 
struct ubifs_orphan <token> *cnext; <answer> *orphan, 
struct <token> *orph; <answer> ubifs_orph_node 
int gap, err, <token> cnt, i; <answer> len, 
ubifs_assert(c, c->cmt_orphans > <token> <answer> 0); 
gap = c->leb_size <token> c->ohead_offs; <answer> - 
if (gap < UBIFS_ORPH_NODE_SZ + sizeof(__le64)) <token> <answer> { 
<token> += 1; <answer> c->ohead_lnum 
<token> = 0; <answer> c->ohead_offs 
<token> = c->leb_size; <answer> gap 
if (c->ohead_lnum > c->orph_last) <token> <answer> { 
<token> "out of space in orphan area"); <answer> ubifs_err(c, 
<token> -EINVAL; <answer> return 
cnt = (gap - UBIFS_ORPH_NODE_SZ) <token> sizeof(__le64); <answer> / 
if <token> > c->cmt_orphans) <answer> (cnt 
cnt = <token> <answer> c->cmt_orphans; 
len = UBIFS_ORPH_NODE_SZ + <token> * sizeof(__le64); <answer> cnt 
<token> c->orph_buf); <answer> ubifs_assert(c, 
orph = <token> <answer> c->orph_buf; 
orph->ch.node_type = <token> <answer> UBIFS_ORPH_NODE; 
cnext <token> c->orph_cnext; <answer> = 
for (i = 0; <token> < cnt; i++) { <answer> i 
orphan = <token> <answer> cnext; 
ubifs_assert(c, <token> <answer> orphan->cmt); 
<token> = cpu_to_le64(orphan->inum); <answer> orph->inos[i] 
<token> = 0; <answer> orphan->cmt 
cnext <token> orphan->cnext; <answer> = 
orphan->cnext = <token> <answer> NULL; 
c->orph_cnext = <token> <answer> cnext; 
c->cmt_orphans <token> cnt; <answer> -= 
if <token> <answer> (c->cmt_orphans) 
orph->cmt_no <token> cpu_to_le64(c->cmt_no); <answer> = 
static int write_orph_nodes(struct ubifs_info *c, <token> atomic) <answer> int 
int <token> <answer> err; 
while (c->cmt_orphans <token> 0) { <answer> > 
err = write_orph_node(c, <token> <answer> atomic); 
if <token> <answer> (err) 
return <token> <answer> err; 
<token> (atomic) { <answer> if 
<token> lnum; <answer> int 
static int consolidate(struct ubifs_info <token> <answer> *c) 
int tot_avail = tot_avail_orphs(c), err = <token> <answer> 0; 
dbg_cmt("there is space for <token> orphans and there are %d", <answer> %d 
<token> c->tot_orphans); <answer> tot_avail, 
if (c->tot_orphans - c->new_orphans <= tot_avail) <token> <answer> { 
struct <token> *orphan, **last; <answer> ubifs_orphan 
int <token> = 0; <answer> cnt 
<token> "out of space in orphan area"); <answer> ubifs_err(c, 
err = <token> <answer> -EINVAL; 
return <token> <answer> err; 
static int <token> ubifs_info *c) <answer> commit_orphans(struct 
int avail, atomic = <token> err; <answer> 0, 
ubifs_assert(c, <token> > 0); <answer> c->cmt_orphans 
<token> = avail_orphs(c); <answer> avail 
if (avail < c->cmt_orphans) <token> <answer> { 
static void <token> ubifs_info *c) <answer> erase_deleted(struct 
struct ubifs_orphan <token> *dnext; <answer> *orphan, 
dnext <token> c->orph_dnext; <answer> = 
<token> (dnext) { <answer> while 
orphan = <token> <answer> dnext; 
<token> = orphan->dnext; <answer> dnext 
<token> !orphan->new); <answer> ubifs_assert(c, 
ubifs_assert(c, <token> <answer> orphan->del); 
rb_erase(&orphan->rb, <token> <answer> &c->orph_tree); 
c->tot_orphans -= <token> <answer> 1; 
dbg_gen("deleting orphan ino %lu", <token> long)orphan->inum); <answer> (unsigned 
<token> = NULL; <answer> c->orph_dnext 
int <token> ubifs_info *c) <answer> ubifs_orphan_end_commit(struct 
<token> err; <answer> int 
<token> (c->cmt_orphans != 0) { <answer> if 
err <token> commit_orphans(c); <answer> = 
if <token> <answer> (err) 
return <token> <answer> err; 
<token> = dbg_check_orphans(c); <answer> err 
<token> err; <answer> return 
int <token> ubifs_info *c) <answer> ubifs_clear_orphans(struct 
int lnum, <token> <answer> err; 
for <token> = c->orph_first; lnum <= c->orph_last; lnum++) { <answer> (lnum 
err <token> ubifs_leb_unmap(c, lnum); <answer> = 
if <token> <answer> (err) 
<token> err; <answer> return 
c->ohead_lnum = <token> <answer> c->orph_first; 
c->ohead_offs = <token> <answer> 0; 
return <token> <answer> 0; 
static int insert_dead_orphan(struct ubifs_info *c, <token> inum) <answer> ino_t 
struct ubifs_orphan *orphan, <token> <answer> *o; 
struct <token> **p, *parent = NULL; <answer> rb_node 
orphan = <token> ubifs_orphan), GFP_KERNEL); <answer> kzalloc(sizeof(struct 
<token> (!orphan) <answer> if 
return <token> <answer> -ENOMEM; 
orphan->inum <token> inum; <answer> = 
<token> = &c->orph_tree.rb_node; <answer> p 
while (*p) <token> <answer> { 
parent <token> *p; <answer> = 
o = rb_entry(parent, struct <token> rb); <answer> ubifs_orphan, 
<token> (inum < o->inum) <answer> if 
p <token> &(*p)->rb_left; <answer> = 
else <token> (inum > o->inum) <answer> if 
<token> = &(*p)->rb_right; <answer> p 
<token> { <answer> else 
static int <token> ubifs_info *c, struct ubifs_scan_leb *sleb, <answer> do_kill_orphans(struct 
unsigned long long <token> int *outofdate, <answer> *last_cmt_no, 
int <token> <answer> *last_flagged) 
struct ubifs_scan_node <token> <answer> *snod; 
<token> ubifs_orph_node *orph; <answer> struct 
struct <token> *ino = NULL; <answer> ubifs_ino_node 
unsigned long <token> cmt_no; <answer> long 
ino_t <token> <answer> inum; 
int <token> n, err, first = 1; <answer> i, 
<token> = kmalloc(UBIFS_MAX_INO_NODE_SZ, GFP_NOFS); <answer> ino 
if <token> <answer> (!ino) 
<token> -ENOMEM; <answer> return 
list_for_each_entry(snod, &sleb->nodes, list) <token> <answer> { 
if <token> != UBIFS_ORPH_NODE) { <answer> (snod->type 
ubifs_err(c, "invalid node type %d in <token> area at %d:%d", <answer> orphan 
<token> sleb->lnum, snod->offs); <answer> snod->type, 
<token> snod->node, <answer> ubifs_dump_node(c, 
c->leb_size <token> snod->offs); <answer> - 
err <token> -EINVAL; <answer> = 
<token> out_free; <answer> goto 
<token> = snod->node; <answer> orph 
<token> (cmt_no > c->cmt_no) <answer> if 
c->cmt_no <token> cmt_no; <answer> = 
<token> (cmt_no < *last_cmt_no && *last_flagged) { <answer> if 
if (!first) <token> <answer> { 
ubifs_err(c, "out of order commit number %llu in <token> node at %d:%d", <answer> orphan 
cmt_no, <token> snod->offs); <answer> sleb->lnum, 
<token> snod->node, <answer> ubifs_dump_node(c, 
c->leb_size <token> snod->offs); <answer> - 
err = <token> <answer> -EINVAL; 
goto <token> <answer> out_free; 
dbg_rcvry("out <token> date LEB %d", sleb->lnum); <answer> of 
*outofdate <token> 1; <answer> = 
err = <token> <answer> 0; 
goto <token> <answer> out_free; 
<token> (first) <answer> if 
<token> = 0; <answer> first 
n = (le32_to_cpu(orph->ch.len) - UBIFS_ORPH_NODE_SZ) <token> 3; <answer> >> 
for (i = 0; i < n; <token> { <answer> i++) 
union <token> key1, key2; <answer> ubifs_key 
inum <token> le64_to_cpu(orph->inos[i]); <answer> = 
<token> &key1, inum); <answer> ino_key_init(c, 
err = <token> &key1, ino); <answer> ubifs_tnc_lookup(c, 
if (err && <token> != -ENOENT) <answer> err 
goto <token> <answer> out_free; 
if (err == 0 && ino->nlink <token> 0) { <answer> == 
dbg_rcvry("deleting orphaned <token> %lu", <answer> inode 
(unsigned <token> <answer> long)inum); 
<token> &key1, inum); <answer> lowest_ino_key(c, 
<token> &key2, inum); <answer> highest_ino_key(c, 
<token> = ubifs_tnc_remove_range(c, &key1, &key2); <answer> err 
if <token> <answer> (err) 
<token> out_ro; <answer> goto 
<token> = insert_dead_orphan(c, inum); <answer> err 
if <token> <answer> (err) 
goto <token> <answer> out_free; 
<token> = cmt_no; <answer> *last_cmt_no 
if <token> & (1ULL << 63)) { <answer> (le64_to_cpu(orph->cmt_no) 
dbg_rcvry("last <token> node for commit %llu at %d:%d", <answer> orph 
<token> sleb->lnum, snod->offs); <answer> cmt_no, 
*last_flagged = <token> <answer> 1; 
} <token> <answer> else 
*last_flagged <token> 0; <answer> = 
err <token> 0; <answer> = 
return <token> <answer> err; 
ubifs_ro_mode(c, <token> <answer> err); 
return <token> <answer> err; 
<token> int kill_orphans(struct ubifs_info *c) <answer> static 
unsigned long long <token> = 0; <answer> last_cmt_no 
int lnum, err = 0, outofdate = 0, last_flagged <token> 0; <answer> = 
c->ohead_lnum = <token> <answer> c->orph_first; 
c->ohead_offs <token> 0; <answer> = 
for (lnum = <token> lnum <= c->orph_last; lnum++) { <answer> c->orph_first; 
<token> ubifs_scan_leb *sleb; <answer> struct 
dbg_rcvry("LEB <token> lnum); <answer> %d", 
sleb = ubifs_scan(c, <token> 0, c->sbuf, 1); <answer> lnum, 
if <token> { <answer> (IS_ERR(sleb)) 
<token> (PTR_ERR(sleb) == -EUCLEAN) <answer> if 
sleb = ubifs_recover_leb(c, <token> 0, <answer> lnum, 
<token> -1); <answer> c->sbuf, 
if <token> { <answer> (IS_ERR(sleb)) 
<token> = PTR_ERR(sleb); <answer> err 
err <token> do_kill_orphans(c, sleb, &last_cmt_no, &outofdate, <answer> = 
<token> (err || outofdate) { <answer> if 
if <token> { <answer> (sleb->endpt) 
<token> = lnum; <answer> c->ohead_lnum 
c->ohead_offs = <token> <answer> sleb->endpt; 
return <token> <answer> err; 
<token> ubifs_mount_orphans(struct ubifs_info *c, int unclean, int read_only) <answer> int 
int <token> = 0; <answer> err 
c->max_orphans = <token> <answer> tot_avail_orphs(c); 
if <token> { <answer> (!read_only) 
c->orph_buf <token> vmalloc(c->leb_size); <answer> = 
<token> (!c->orph_buf) <answer> if 
return <token> <answer> -ENOMEM; 
<token> (unclean) <answer> if 
err <token> kill_orphans(c); <answer> = 
<token> if (!read_only) <answer> else 
err <token> ubifs_clear_orphans(c); <answer> = 
return <token> <answer> err; 
<token> check_orphan { <answer> struct 
struct rb_node <token> <answer> rb; 
ino_t <token> <answer> inum; 
struct check_info <token> <answer> { 
unsigned <token> last_ino; <answer> long 
<token> long tot_inos; <answer> unsigned 
unsigned long <token> <answer> missing; 
unsigned long <token> leaf_cnt; <answer> long 
<token> ubifs_ino_node *node; <answer> struct 
<token> rb_root root; <answer> struct 
static bool dbg_find_orphan(struct <token> *c, ino_t inum) <answer> ubifs_info 
bool found <token> false; <answer> = 
found = !!lookup_orphan(c, <token> <answer> inum); 
<token> found; <answer> return 
static int dbg_ins_check_orphan(struct rb_root *root, ino_t <token> <answer> inum) 
<token> check_orphan *orphan, *o; <answer> struct 
struct <token> **p, *parent = NULL; <answer> rb_node 
orphan = kzalloc(sizeof(struct <token> GFP_NOFS); <answer> check_orphan), 
if <token> <answer> (!orphan) 
return <token> <answer> -ENOMEM; 
<token> = inum; <answer> orphan->inum 
<token> = &root->rb_node; <answer> p 
while (*p) <token> <answer> { 
parent <token> *p; <answer> = 
o = rb_entry(parent, <token> check_orphan, rb); <answer> struct 
<token> (inum < o->inum) <answer> if 
<token> = &(*p)->rb_left; <answer> p 
<token> if (inum > o->inum) <answer> else 
p <token> &(*p)->rb_right; <answer> = 
else <token> <answer> { 
return <token> <answer> 0; 
<token> parent, p); <answer> rb_link_node(&orphan->rb, 
<token> root); <answer> rb_insert_color(&orphan->rb, 
<token> 0; <answer> return 
static int dbg_find_check_orphan(struct rb_root <token> ino_t inum) <answer> *root, 
struct check_orphan <token> <answer> *o; 
<token> rb_node *p; <answer> struct 
p = <token> <answer> root->rb_node; 
while (p) <token> <answer> { 
<token> = rb_entry(p, struct check_orphan, rb); <answer> o 
<token> (inum < o->inum) <answer> if 
p = <token> <answer> p->rb_left; 
<token> if (inum > o->inum) <answer> else 
p = <token> <answer> p->rb_right; 
return <token> <answer> 1; 
return <token> <answer> 0; 
static <token> dbg_free_check_tree(struct rb_root *root) <answer> void 
struct check_orphan *o, <token> <answer> *n; 
rbtree_postorder_for_each_entry_safe(o, <token> root, rb) <answer> n, 
static int <token> ubifs_info *c, struct ubifs_zbranch *zbr, <answer> dbg_orphan_check(struct 
<token> *priv) <answer> void 
struct check_info <token> = priv; <answer> *ci 
<token> inum; <answer> ino_t 
<token> err; <answer> int 
inum <token> key_inum(c, &zbr->key); <answer> = 
<token> (inum != ci->last_ino) { <answer> if 
#include <token> <answer> <linux/types.h> 
#include <token> <answer> <linux/crc8.h> 
#include <token> <answer> <linux/delay.h> 
<token> <linux/kernel.h> <answer> #include 
#include <token> <answer> <linux/slab.h> 
#include <token> <answer> <linux/string.h> 
#include <token> <answer> "qed_hsi.h" 
<token> "qed_hw.h" <answer> #include 
#include <token> <answer> "qed_init_ops.h" 
#include <token> <answer> "qed_iro_hsi.h" 
<token> "qed_reg_addr.h" <answer> #include 
#define <token> CDU_CONTEXT_VALIDATION_DEFAULT_CFG <answer> CDU_VALIDATION_DEFAULT_CFG 
static u16 con_region_offsets[3][NUM_OF_CONNECTION_TYPES] = <token> <answer> { 
<token> QM_VOQ_LINE_CRD(pbf_cmd_lines) \ <answer> #define 
((((pbf_cmd_lines) <token> 4) * 2) | QM_LINE_CRD_REG_SIGN_BIT) <answer> - 
static void qed_cmdq_lines_voq_rt_init(struct <token> *p_hwfn, <answer> qed_hwfn 
u8 ext_voq, u16 <token> <answer> cmdq_lines) 
<token> qm_line_crd = QM_VOQ_LINE_CRD(cmdq_lines); <answer> u32 
OVERWRITE_RT_REG(p_hwfn, <token> <answer> PBF_CMDQ_LINES_RT_OFFSET(ext_voq), 
<token> QM_REG_VOQCRDLINE_RT_OFFSET + ext_voq, <answer> STORE_RT_REG(p_hwfn, 
STORE_RT_REG(p_hwfn, <token> + ext_voq, <answer> QM_REG_VOQINITCRDLINE_RT_OFFSET 
phys_lines <token> port_params[port_id].num_pbf_cmd_lines; <answer> = 
phys_lines -= <token> <answer> PBF_CMDQ_PURE_LB_LINES; 
static <token> <answer> void 
qed_btb_blocks_rt_init(struct <token> *p_hwfn, <answer> qed_hwfn 
<token> max_ports_per_engine, <answer> u8 
<token> max_phys_tcs_per_port, <answer> u8 
struct <token> port_params[MAX_NUM_PORTS]) <answer> init_qm_port_params 
u32 usable_blocks, <token> phys_blocks; <answer> pure_lb_blocks, 
u8 <token> ext_voq, port_id, num_tcs_in_port; <answer> tc, 
for (port_id = 0; port_id < max_ports_per_engine; port_id++) <token> <answer> { 
if <token> <answer> (!port_params[port_id].active) 
num_tcs_in_port <token> 0; <answer> = 
for <token> = 0; tc < NUM_OF_PHYS_TCS; tc++) <answer> (tc 
<token> (((port_params[port_id].active_phys_tcs >> <answer> if 
tc) & 0x1) == <token> <answer> 1) 
pure_lb_blocks = <token> * BTB_PURE_LB_FACTOR) / <answer> (usable_blocks 
(num_tcs_in_port * BTB_PURE_LB_FACTOR <token> <answer> + 
<token> = max_t(u32, BTB_JUMBO_PKT_BLOCKS, <answer> pure_lb_blocks 
pure_lb_blocks / <token> <answer> BTB_PURE_LB_FACTOR); 
<token> = (usable_blocks - pure_lb_blocks) / <answer> phys_blocks 
static <token> qed_global_rl_rt_init(struct qed_hwfn *p_hwfn) <answer> int 
u32 <token> = QM_GLOBAL_RL_UPPER_BOUND(QM_MAX_LINK_SPEED) | <answer> upper_bound 
u32 <token> <answer> inc_val; 
u16 <token> <answer> rl_id; 
static u32 qed_get_vport_rl_upper_bound(enum <token> vport_rl_type, <answer> init_qm_rl_type 
u32 <token> <answer> link_speed) 
switch <token> { <answer> (vport_rl_type) 
<token> QM_RL_TYPE_NORMAL: <answer> case 
return <token> <answer> QM_INITIAL_VOQ_BYTE_CRD; 
<token> QM_RL_TYPE_QCN: <answer> case 
<token> QM_GLOBAL_RL_UPPER_BOUND(link_speed); <answer> return 
<token> 0; <answer> return 
static <token> qed_vport_rl_rt_init(struct qed_hwfn *p_hwfn, <answer> int 
<token> start_rl, <answer> u16 
u16 <token> <answer> num_rls, 
<token> link_speed, <answer> u32 
<token> init_qm_rl_params *rl_params) <answer> struct 
<token> i, rl_id; <answer> u16 
<token> (num_rls && start_rl + num_rls >= MAX_QM_GLOBAL_RLS) { <answer> if 
DP_NOTICE(p_hwfn, "Invalid rate limiter <token> <answer> configuration\n"); 
<token> -1; <answer> return 
pq_group = <token> <answer> pf_id; 
<token> = num_pf_cids + num_tids; <answer> pq_size 
pq_mem_4kb <token> QM_PQ_MEM_4KB(pq_size); <answer> = 
<token> = base_mem_addr_4kb; <answer> mem_addr_4kb 
static <token> qed_pf_wfq_rt_init(struct qed_hwfn *p_hwfn, <answer> int 
struct <token> *p_params) <answer> qed_qm_pf_rt_init_params 
u16 num_tx_pqs = p_params->num_pf_pqs + <token> <answer> p_params->num_vf_pqs; 
struct <token> *pq_params = p_params->pq_params; <answer> init_qm_pq_params 
u32 inc_val, <token> <answer> crd_reg_offset; 
u8 <token> <answer> ext_voq; 
u16 <token> <answer> i; 
<token> = QM_PF_WFQ_INC_VAL(p_params->pf_wfq); <answer> inc_val 
<token> (!inc_val || inc_val > QM_PF_WFQ_MAX_INC_VAL) { <answer> if 
DP_NOTICE(p_hwfn, "Invalid <token> WFQ weight configuration\n"); <answer> PF 
return <token> <answer> -1; 
for (i = 0; i < <token> i++) { <answer> num_tx_pqs; 
<token> = qed_get_ext_voq(p_hwfn, <answer> ext_voq 
<token> = <answer> crd_reg_offset 
<token> < MAX_NUM_PFS_BB ? <answer> (p_params->pf_id 
QM_REG_WFQPFCRD_RT_OFFSET <token> <answer> : 
QM_REG_WFQPFCRD_MSB_RT_OFFSET) <token> <answer> + 
ext_voq * MAX_NUM_PFS_BB <token> <answer> + 
(p_params->pf_id <token> MAX_NUM_PFS_BB); <answer> % 
crd_reg_offset, <token> <answer> (u32)QM_WFQ_CRD_REG_SIGN_BIT); 
QM_REG_WFQPFUPPERBOUND_RT_OFFSET + <token> <answer> p_params->pf_id, 
<token> | (u32)QM_WFQ_CRD_REG_SIGN_BIT); <answer> QM_PF_WFQ_UPPER_BOUND 
STORE_RT_REG(p_hwfn, QM_REG_WFQPFWEIGHT_RT_OFFSET + <token> <answer> p_params->pf_id, 
return <token> <answer> 0; 
static int qed_pf_rl_rt_init(struct qed_hwfn *p_hwfn, u8 pf_id, <token> pf_rl) <answer> u32 
u32 inc_val = <token> <answer> QM_RL_INC_VAL(pf_rl); 
if (inc_val > QM_PF_RL_MAX_INC_VAL) <token> <answer> { 
DP_NOTICE(p_hwfn, "Invalid PF rate limit <token> <answer> configuration\n"); 
<token> -1; <answer> return 
<token> + pf_id, <answer> QM_REG_RLPFCRD_RT_OFFSET 
QM_REG_RLPFUPPERBOUND_RT_OFFSET <token> pf_id, <answer> + 
<token> | (u32)QM_RL_CRD_REG_SIGN_BIT); <answer> QM_PF_RL_UPPER_BOUND 
<token> QM_REG_RLPFINCVAL_RT_OFFSET + pf_id, inc_val); <answer> STORE_RT_REG(p_hwfn, 
return <token> <answer> 0; 
static int qed_vp_wfq_rt_init(struct qed_hwfn <token> <answer> *p_hwfn, 
<token> num_vports, <answer> u16 
<token> init_qm_vport_params *vport_params) <answer> struct 
u16 vport_pq_id, wfq, <token> <answer> i; 
u32 <token> <answer> inc_val; 
u8 <token> <answer> tc; 
static int qed_dmae_to_grc(struct qed_hwfn *p_hwfn, <token> qed_ptt *p_ptt, <answer> struct 
__le32 *p_data, u32 addr, u32 <token> <answer> len_in_dwords) 
struct qed_dmae_params params = { 0 <token> <answer> }; 
u32 <token> <answer> *data_cpu; 
<token> rc; <answer> int 
if <token> <answer> (!p_data) 
<token> -1; <answer> return 
if ((validation_cfg <token> CDU_CONTEXT_VALIDATION_CFG_USE_CID) & 1) <answer> >> 
validation_string |= (cid & 0xFFF00000) <token> ((cid & 0xFFF) << 8); <answer> | 
if ((validation_cfg >> CDU_CONTEXT_VALIDATION_CFG_USE_REGION) & <token> <answer> 1) 
<token> |= ((region & 0xF) << 4); <answer> validation_string 
if ((validation_cfg >> CDU_CONTEXT_VALIDATION_CFG_USE_TYPE) & <token> <answer> 1) 
<token> |= (conn_type & 0xF); <answer> validation_string 
<token> |= <answer> validation_byte 
<token> >> <answer> ((validation_cfg 
<token> & 1) << 7; <answer> CDU_CONTEXT_VALIDATION_CFG_USE_ACTIVE) 
<token> ((validation_cfg >> <answer> if 
CDU_CONTEXT_VALIDATION_CFG_VALIDATION_TYPE_SHIFT) & <token> <answer> 1) 
<token> |= ((conn_type & 0xF) << 3) | (crc & 0x7); <answer> validation_byte 
<token> |= crc & 0x7F; <answer> validation_byte 
return <token> <answer> validation_byte; 
<token> | HV_HYPERCALL_FAST_BIT, 0x0, <answer> __hyperv_hypercall(HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE 
HV_FLUSH_ALL_VIRTUAL_ADDRESS_SPACES <token> HV_FLUSH_ALL_PROCESSORS, <answer> | 
<token> == EXIT_REASON_VMCALL); <answer> GUEST_ASSERT(vmreadz(VM_EXIT_REASON) 
<token> == EXIT_REASON_MSR_READ); <answer> GUEST_ASSERT(vmreadz(VM_EXIT_REASON) 
if <token> == 9) { <answer> (stage 
pr_info("Trying extra KVM_GET_NESTED_STATE/KVM_SET_NESTED_STATE <token> <answer> cycle\n"); 
vcpu <token> save_restore_vm(vm, vcpu); <answer> = 
<token> <linux/kernel.h> <answer> #include 
<token> <linux/module.h> <answer> #include 
<token> <linux/sched.h> <answer> #include 
<token> <linux/interrupt.h> <answer> #include 
#include <token> <answer> <linux/device.h> 
#include <token> <answer> <linux/of.h> 
#include <token> <answer> <linux/platform_device.h> 
#include <token> <answer> <linux/input.h> 
#include <token> <answer> <linux/input/touchscreen.h> 
#include <token> <answer> <linux/slab.h> 
<token> <linux/delay.h> <answer> #include 
#include <token> <answer> <linux/i2c.h> 
<token> <linux/workqueue.h> <answer> #include 
<token> <linux/mfd/stmpe.h> <answer> #include 
<token> STMPE_REG_INT_STA 0x0B <answer> #define 
#define STMPE_REG_TSC_CTRL <token> <answer> 0x40 
#define STMPE_REG_TSC_CFG <token> <answer> 0x41 
<token> STMPE_REG_FIFO_TH 0x4A <answer> #define 
#define <token> 0x4B <answer> STMPE_REG_FIFO_STA 
<token> STMPE_REG_FIFO_SIZE 0x4C <answer> #define 
<token> STMPE_REG_TSC_DATA_XYZ 0x52 <answer> #define 
<token> STMPE_REG_TSC_FRACTION_Z 0x56 <answer> #define 
#define <token> 0x58 <answer> STMPE_REG_TSC_I_DRIVE 
#define <token> 0 <answer> OP_MOD_XYZ 
#define <token> (1<<0) <answer> STMPE_TSC_CTRL_TSC_EN 
#define <token> (1<<0) <answer> STMPE_FIFO_STA_RESET 
#define <token> 0 <answer> STMPE_IRQ_TOUCH_DET 
#define <token> "stmpe-ts" <answer> STMPE_TS_NAME 
#define XY_MASK <token> <answer> 0xfff 
struct stmpe_touch <token> <answer> { 
struct <token> *stmpe; <answer> stmpe 
<token> input_dev *idev; <answer> struct 
<token> delayed_work work; <answer> struct 
struct <token> *dev; <answer> device 
struct <token> prop; <answer> touchscreen_properties 
u8 <token> <answer> ave_ctrl; 
u8 <token> <answer> touch_det_delay; 
u8 <token> <answer> settling; 
<token> fraction_z; <answer> u8 
<token> i_drive; <answer> u8 
static int __stmpe_reset_fifo(struct stmpe <token> <answer> *stmpe) 
int <token> <answer> ret; 
ret = stmpe_set_bits(stmpe, <token> <answer> STMPE_REG_FIFO_STA, 
STMPE_FIFO_STA_RESET, <token> <answer> STMPE_FIFO_STA_RESET); 
if <token> <answer> (ret) 
<token> ret; <answer> return 
return <token> STMPE_REG_FIFO_STA, <answer> stmpe_set_bits(stmpe, 
<token> 0); <answer> STMPE_FIFO_STA_RESET, 
static void stmpe_work(struct work_struct <token> <answer> *work) 
int <token> <answer> int_sta; 
u32 <token> = 40; <answer> timeout 
struct stmpe_touch *ts <token> <answer> = 
container_of(work, struct <token> work.work); <answer> stmpe_touch, 
int_sta <token> stmpe_reg_read(ts->stmpe, STMPE_REG_INT_STA); <answer> = 
while ((int_sta & (1 <token> STMPE_IRQ_TOUCH_DET)) && (timeout > 0)) { <answer> << 
int_sta = <token> STMPE_REG_INT_STA); <answer> stmpe_reg_read(ts->stmpe, 
stmpe_set_bits(ts->stmpe, <token> <answer> STMPE_REG_TSC_CTRL, 
STMPE_TSC_CTRL_TSC_EN, <token> <answer> 0); 
stmpe_block_read(ts->stmpe, STMPE_REG_TSC_DATA_XYZ, <token> data_set); <answer> 4, 
x = <token> << 4) | (data_set[1] >> 4); <answer> (data_set[0] 
y = <token> & 0xf) << 8) | data_set[2]; <answer> ((data_set[1] 
z = <token> <answer> data_set[3]; 
touchscreen_report_pos(ts->idev, <token> x, y, false); <answer> &ts->prop, 
input_report_abs(ts->idev, ABS_PRESSURE, <token> <answer> z); 
<token> BTN_TOUCH, 1); <answer> input_report_key(ts->idev, 
#define pr_fmt(fmt) <token> " fmt <answer> "damon: 
#include <token> <answer> <linux/damon.h> 
#include <token> <answer> <linux/delay.h> 
#include <token> <answer> <linux/kthread.h> 
#include <token> <answer> <linux/mm.h> 
#include <token> <answer> <linux/psi.h> 
<token> <linux/slab.h> <answer> #include 
<token> <linux/string.h> <answer> #include 
<token> CREATE_TRACE_POINTS <answer> #define 
<token> <trace/events/damon.h> <answer> #include 
<token> CONFIG_DAMON_KUNIT_TEST <answer> #ifdef 
<token> DAMON_MIN_REGION <answer> #undef 
<token> DAMON_MIN_REGION 1 <answer> #define 
<token> DEFINE_MUTEX(damon_lock); <answer> static 
static int <token> <answer> nr_running_ctxs; 
<token> bool running_exclusive_ctxs; <answer> static 
<token> DEFINE_MUTEX(damon_ops_lock); <answer> static 
<token> struct damon_operations damon_registered_ops[NR_DAMON_OPS]; <answer> static 
<token> struct kmem_cache *damon_region_cache __ro_after_init; <answer> static 
bool damon_is_registered_ops(enum <token> id) <answer> damon_ops_id 
bool <token> <answer> registered; 
if <token> >= NR_DAMON_OPS) <answer> (id 
return <token> <answer> false; 
registered = <token> <answer> __damon_is_registered_ops(id); 
return <token> <answer> registered; 
int damon_register_ops(struct damon_operations <token> <answer> *ops) 
int err <token> 0; <answer> = 
if (ops->id <token> NR_DAMON_OPS) <answer> >= 
<token> -EINVAL; <answer> return 
int damon_select_ops(struct damon_ctx *ctx, enum damon_ops_id <token> <answer> id) 
<token> err = 0; <answer> int 
<token> (id >= NR_DAMON_OPS) <answer> if 
return <token> <answer> -EINVAL; 
<token> (!__damon_is_registered_ops(id)) <answer> if 
err = <token> <answer> -EINVAL; 
<token> = damon_registered_ops[id]; <answer> ctx->ops 
<token> err; <answer> return 
struct damon_region <token> long start, unsigned long end) <answer> *damon_new_region(unsigned 
<token> damon_region *region; <answer> struct 
region = kmem_cache_alloc(damon_region_cache, <token> <answer> GFP_KERNEL); 
<token> (!region) <answer> if 
<token> NULL; <answer> return 
<token> = start; <answer> region->ar.start 
<token> = end; <answer> region->ar.end 
region->nr_accesses <token> 0; <answer> = 
region->nr_accesses_bp <token> 0; <answer> = 
<token> = 0; <answer> region->age 
region->last_nr_accesses = <token> <answer> 0; 
<token> region; <answer> return 
void damon_add_region(struct <token> *r, struct damon_target *t) <answer> damon_region 
list_add_tail(&r->list, <token> <answer> &t->regions_list); 
static void damon_del_region(struct damon_region *r, struct <token> *t) <answer> damon_target 
static <token> damon_free_region(struct damon_region *r) <answer> void 
kmem_cache_free(damon_region_cache, <token> <answer> r); 
<token> damon_destroy_region(struct damon_region *r, struct damon_target *t) <answer> void 
damon_del_region(r, <token> <answer> t); 
static bool <token> damon_region *r, <answer> damon_intersect(struct 
struct <token> *re) <answer> damon_addr_range 
return !(r->ar.end <token> re->start || re->end <= r->ar.start); <answer> <= 
static int <token> damon_region *first, <answer> damon_fill_regions_holes(struct 
<token> damon_region *last, struct damon_target *t) <answer> struct 
struct damon_region *r = <token> <answer> first; 
damon_for_each_region_from(r, <token> { <answer> t) 
struct damon_region *next, <token> <answer> *newr; 
if <token> == last) <answer> (r 
next = <token> <answer> damon_next_region(r); 
if (r->ar.end != next->ar.start) <token> <answer> { 
newr <token> damon_new_region(r->ar.end, next->ar.start); <answer> = 
<token> (!newr) <answer> if 
return <token> <answer> -ENOMEM; 
damon_insert_region(newr, r, next, <token> <answer> t); 
return <token> <answer> 0; 
<token> damon_set_regions(struct damon_target *t, struct damon_addr_range *ranges, <answer> int 
<token> int nr_ranges) <answer> unsigned 
struct <token> *r, *next; <answer> damon_region 
unsigned <token> i; <answer> int 
int <token> <answer> err; 
<token> = 0; <answer> scheme->next_apply_sis 
scheme->stat = (struct <token> <answer> damos_stat){}; 
<token> = *(damos_quota_init(quota)); <answer> scheme->quota 
struct <token> *damon_new_target(void) <answer> damon_target 
struct damon_target <token> <answer> *t; 
t <token> kmalloc(sizeof(*t), GFP_KERNEL); <answer> = 
if <token> <answer> (!t) 
<token> NULL; <answer> return 
<token> = NULL; <answer> t->pid 
<token> = 0; <answer> t->nr_regions 
<token> t; <answer> return 
void damon_add_target(struct <token> *ctx, struct damon_target *t) <answer> damon_ctx 
<token> &ctx->adaptive_targets); <answer> list_add_tail(&t->list, 
bool damon_targets_empty(struct <token> *ctx) <answer> damon_ctx 
<token> list_empty(&ctx->adaptive_targets); <answer> return 
static void <token> damon_target *t) <answer> damon_del_target(struct 
void <token> damon_target *t) <answer> damon_free_target(struct 
<token> damon_region *r, *next; <answer> struct 
damon_for_each_region_safe(r, <token> t) <answer> next, 
void damon_destroy_target(struct <token> *t) <answer> damon_target 
unsigned int damon_nr_regions(struct <token> *t) <answer> damon_target 
return <token> <answer> t->nr_regions; 
struct <token> *damon_new_ctx(void) <answer> damon_ctx 
<token> damon_ctx *ctx; <answer> struct 
ctx = kzalloc(sizeof(*ctx), <token> <answer> GFP_KERNEL); 
if <token> <answer> (!ctx) 
<token> NULL; <answer> return 
ctx->attrs.sample_interval = 5 <token> 1000; <answer> * 
ctx->attrs.aggr_interval = 100 <token> 1000; <answer> * 
ctx->attrs.ops_update_interval = 60 * 1000 * <token> <answer> 1000; 
ctx->passed_sample_intervals = <token> <answer> 0; 
static void damon_update_monitoring_results(struct <token> *ctx, <answer> damon_ctx 
struct damon_attrs <token> <answer> *new_attrs) 
struct <token> *old_attrs = &ctx->attrs; <answer> damon_attrs 
<token> damon_target *t; <answer> struct 
<token> damon_region *r; <answer> struct 
int damon_set_attrs(struct damon_ctx *ctx, <token> damon_attrs *attrs) <answer> struct 
<token> long sample_interval = attrs->sample_interval ? <answer> unsigned 
attrs->sample_interval <token> 1; <answer> : 
struct <token> *s; <answer> damos 
<token> (attrs->min_nr_regions < 3) <answer> if 
<token> -EINVAL; <answer> return 
<token> (attrs->min_nr_regions > attrs->max_nr_regions) <answer> if 
return <token> <answer> -EINVAL; 
<token> (attrs->sample_interval > attrs->aggr_interval) <answer> if 
return <token> <answer> -EINVAL; 
ctx->next_aggregation_sis = <token> + <answer> ctx->passed_sample_intervals 
attrs->aggr_interval <token> sample_interval; <answer> / 
ctx->next_ops_update_sis = ctx->passed_sample_intervals <token> <answer> + 
attrs->ops_update_interval / <token> <answer> sample_interval; 
damon_update_monitoring_results(ctx, <token> <answer> attrs); 
ctx->attrs = <token> <answer> *attrs; 
damon_for_each_scheme(s, <token> <answer> ctx) 
<token> ctx); <answer> damos_set_next_apply_sis(s, 
<token> 0; <answer> return 
void damon_set_schemes(struct damon_ctx *ctx, struct <token> **schemes, <answer> damos 
ssize_t <token> <answer> nr_schemes) 
struct damos <token> *next; <answer> *s, 
ssize_t <token> <answer> i; 
damon_for_each_scheme_safe(s, <token> ctx) <answer> next, 
for (i = 0; <token> < nr_schemes; i++) <answer> i 
<token> schemes[i]); <answer> damon_add_scheme(ctx, 
<token> damon_nr_running_ctxs(void) <answer> int 
int <token> <answer> nr_ctxs; 
nr_ctxs <token> nr_running_ctxs; <answer> = 
return <token> <answer> nr_ctxs; 
static <token> __damon_start(struct damon_ctx *ctx) <answer> int 
<token> err = -EBUSY; <answer> int 
<token> (!ctx->kdamond) { <answer> if 
err <token> 0; <answer> = 
ctx->kdamond = <token> ctx, "kdamond.%d", <answer> kthread_run(kdamond_fn, 
<token> (IS_ERR(ctx->kdamond)) { <answer> if 
err <token> PTR_ERR(ctx->kdamond); <answer> = 
ctx->kdamond <token> NULL; <answer> = 
} else <token> <answer> { 
<token> err; <answer> return 
int damon_start(struct <token> **ctxs, int nr_ctxs, bool exclusive) <answer> damon_ctx 
int <token> <answer> i; 
int err <token> 0; <answer> = 
if ((exclusive <token> nr_running_ctxs) || <answer> && 
(!exclusive && running_exclusive_ctxs)) <token> <answer> { 
return <token> <answer> -EBUSY; 
for (i = 0; <token> < nr_ctxs; i++) { <answer> i 
err = <token> <answer> __damon_start(ctxs[i]); 
<token> (err) <answer> if 
if (exclusive && <token> <answer> nr_running_ctxs) 
running_exclusive_ctxs = <token> <answer> true; 
<token> err; <answer> return 
static <token> __damon_stop(struct damon_ctx *ctx) <answer> int 
struct <token> *tsk; <answer> task_struct 
tsk <token> ctx->kdamond; <answer> = 
if <token> { <answer> (tsk) 
return <token> <answer> 0; 
<token> -EPERM; <answer> return 
int damon_stop(struct damon_ctx **ctxs, <token> nr_ctxs) <answer> int 
int i, err <token> 0; <answer> = 
for (i = 0; i < nr_ctxs; <token> { <answer> i++) 
static void kdamond_reset_aggregated(struct <token> *c) <answer> damon_ctx 
struct damon_target <token> <answer> *t; 
static bool damos_skip_charged_region(struct damon_target <token> <answer> *t, 
struct damon_region **rp, struct damos <token> <answer> *s) 
struct <token> *r = *rp; <answer> damon_region 
struct <token> *quota = &s->quota; <answer> damos_quota 
unsigned long <token> <answer> sz_to_skip; 
<token> int cidx = 0; <answer> unsigned 
static unsigned long damon_feed_loop_next_input(unsigned <token> last_input, <answer> long 
<token> long score) <answer> unsigned 
const unsigned long goal = <token> <answer> 10000; 
unsigned long score_goal_diff = max(goal, <token> - min(goal, score); <answer> score) 
unsigned long score_goal_diff_bp = score_goal_diff * <token> / goal; <answer> 10000 
unsigned <token> compensation = last_input * score_goal_diff_bp / 10000; <answer> long 
static void damos_set_effective_quota(struct damos_quota <token> <answer> *quota) 
unsigned long <token> <answer> throughput; 
unsigned <token> esz; <answer> long 
if (!quota->ms && <token> { <answer> list_empty(&quota->goals)) 
<token> = quota->sz; <answer> quota->esz 
if (!list_empty(&quota->goals)) <token> <answer> { 
unsigned long score = <token> <answer> damos_quota_score(quota); 
quota->esz_bp = <token> <answer> damon_feed_loop_next_input( 
max(quota->esz_bp, <token> <answer> 10000UL), 
esz = <token> / 10000; <answer> quota->esz_bp 
if (quota->ms) <token> <answer> { 
if <token> <answer> (quota->total_charged_ns) 
throughput = quota->total_charged_sz * 1000000 <token> <answer> / 
throughput = <token> * 1024; <answer> PAGE_SIZE 
<token> (!list_empty(&quota->goals)) <answer> if 
esz = min(throughput * quota->ms, <token> <answer> esz); 
esz = throughput * <token> <answer> quota->ms; 
if (quota->sz <token> quota->sz < esz) <answer> && 
<token> = quota->sz; <answer> esz 
quota->esz = <token> <answer> esz; 
static void <token> damon_ctx *c, struct damos *s) <answer> damos_adjust_quota(struct 
<token> damos_quota *quota = &s->quota; <answer> struct 
struct <token> *t; <answer> damon_target 
<token> damon_region *r; <answer> struct 
<token> long cumulated_sz; <answer> unsigned 
unsigned int <token> max_score = 0; <answer> score, 
if (!quota->ms <token> !quota->sz && list_empty(&quota->goals)) <answer> && 
<token> void damon_merge_two_regions(struct damon_target *t, <answer> static 
struct damon_region *l, struct damon_region <token> <answer> *r) 
unsigned long <token> = damon_sz_region(l), sz_r = damon_sz_region(r); <answer> sz_l 
<token> = (l->nr_accesses * sz_l + r->nr_accesses * sz_r) / <answer> l->nr_accesses 
(sz_l <token> sz_r); <answer> + 
l->nr_accesses_bp = l->nr_accesses * <token> <answer> 10000; 
l->age = (l->age * <token> + r->age * sz_r) / (sz_l + sz_r); <answer> sz_l 
l->ar.end <token> r->ar.end; <answer> = 
damon_destroy_region(r, <token> <answer> t); 
static void damon_merge_regions_of(struct damon_target *t, unsigned int <token> <answer> thres, 
<token> long sz_limit) <answer> unsigned 
struct damon_region *r, *prev = <token> *next; <answer> NULL, 
damon_for_each_region_safe(r, next, <token> { <answer> t) 
if (abs(r->nr_accesses <token> r->last_nr_accesses) > thres) <answer> - 
r->age <token> 0; <answer> = 
<token> (prev && prev->ar.end == r->ar.start && <answer> if 
abs(prev->nr_accesses - r->nr_accesses) <token> thres && <answer> <= 
damon_sz_region(prev) + damon_sz_region(r) <= <token> <answer> sz_limit) 
damon_merge_two_regions(t, prev, <token> <answer> r); 
prev <token> r; <answer> = 
static void kdamond_merge_regions(struct damon_ctx *c, unsigned <token> threshold, <answer> int 
unsigned <token> sz_limit) <answer> long 
<token> damon_target *t; <answer> struct 
<token> c) <answer> damon_for_each_target(t, 
damon_merge_regions_of(t, threshold, <token> <answer> sz_limit); 
static <token> damon_split_region_at(struct damon_target *t, <answer> void 
struct damon_region *r, unsigned long <token> <answer> sz_r) 
struct <token> *new; <answer> damon_region 
new = <token> + sz_r, r->ar.end); <answer> damon_new_region(r->ar.start 
<token> (!new) <answer> if 
<token> = new->ar.start; <answer> r->ar.end 
<token> = r->age; <answer> new->age 
<token> = r->last_nr_accesses; <answer> new->last_nr_accesses 
new->nr_accesses_bp = <token> <answer> r->nr_accesses_bp; 
new->nr_accesses = <token> <answer> r->nr_accesses; 
<token> r, damon_next_region(r), t); <answer> damon_insert_region(new, 
sz_sub = ALIGN_DOWN(damon_rand(1, 10) <token> <answer> * 
<token> / 10, DAMON_MIN_REGION); <answer> sz_region 
static <token> kdamond_split_regions(struct damon_ctx *ctx) <answer> void 
struct <token> *t; <answer> damon_target 
unsigned int <token> = 0; <answer> nr_regions 
static unsigned int <token> <answer> last_nr_regions; 
int <token> = 2; <answer> nr_subregions 
damon_for_each_target(t, <token> <answer> ctx) 
<token> += damon_nr_regions(t); <answer> nr_regions 
<token> (nr_regions > ctx->attrs.max_nr_regions / 2) <answer> if 
static bool kdamond_need_stop(struct damon_ctx <token> <answer> *ctx) 
struct damon_target <token> <answer> *t; 
<token> (kthread_should_stop()) <answer> if 
return <token> <answer> true; 
if <token> <answer> (!ctx->ops.target_valid) 
<token> false; <answer> return 
damon_for_each_target(t, ctx) <token> <answer> { 
<token> (ctx->ops.target_valid(t)) <answer> if 
<token> false; <answer> return 
<token> true; <answer> return 
<token> unsigned long damos_wmark_metric_value(enum damos_wmark_metric metric) <answer> static 
switch <token> { <answer> (metric) 
case <token> <answer> DAMOS_WMARK_FREE_MEM_RATE: 
return <token> * 1000 / <answer> global_zone_page_state(NR_FREE_PAGES) 
<token> -EINVAL; <answer> return 
static <token> long damos_wmark_wait_us(struct damos *scheme) <answer> unsigned 
unsigned <token> metric; <answer> long 
<token> (scheme->wmarks.metric == DAMOS_WMARK_NONE) <answer> if 
<token> 0; <answer> return 
metric <token> damos_wmark_metric_value(scheme->wmarks.metric); <answer> = 
static int <token> *data) <answer> kdamond_fn(void 
struct <token> *ctx = data; <answer> damon_ctx 
struct damon_target <token> <answer> *t; 
<token> damon_region *r, *next; <answer> struct 
unsigned <token> max_nr_accesses = 0; <answer> int 
unsigned long sz_limit <token> 0; <answer> = 
pr_debug("kdamond <token> starts\n", current->pid); <answer> (%d) 
<token> (ctx->ops.init) <answer> if 
if (ctx->callback.before_start <token> ctx->callback.before_start(ctx)) <answer> && 
<token> done; <answer> goto 
<token> = damon_region_sz_limit(ctx); <answer> sz_limit 
while <token> { <answer> (!kdamond_need_stop(ctx)) 
unsigned long next_aggregation_sis <token> ctx->next_aggregation_sis; <answer> = 
unsigned long next_ops_update_sis <token> ctx->next_ops_update_sis; <answer> = 
unsigned long sample_interval <token> ctx->attrs.sample_interval; <answer> = 
<token> (kdamond_wait_activation(ctx)) <answer> if 
if <token> <answer> (ctx->ops.prepare_access_checks) 
if (ctx->callback.after_sampling <token> <answer> && 
<token> (ctx->ops.check_accesses) <answer> if 
max_nr_accesses <token> ctx->ops.check_accesses(ctx); <answer> = 
if <token> == next_aggregation_sis) { <answer> (ctx->passed_sample_intervals 
<token> / 10, <answer> max_nr_accesses 
<token> (ctx->callback.after_aggregation && <answer> if 
if <token> <answer> (!list_empty(&ctx->schemes)) 
sample_interval <token> ctx->attrs.sample_interval ? <answer> = 
ctx->attrs.sample_interval : <token> <answer> 1; 
if (ctx->passed_sample_intervals <token> next_aggregation_sis) { <answer> == 
<token> = next_aggregation_sis + <answer> ctx->next_aggregation_sis 
<token> / sample_interval; <answer> ctx->attrs.aggr_interval 
<token> (ctx->ops.reset_aggregated) <answer> if 
if (ctx->passed_sample_intervals <token> next_ops_update_sis) { <answer> == 
<token> = next_ops_update_sis + <answer> ctx->next_ops_update_sis 
<token> / <answer> ctx->attrs.ops_update_interval 
<token> (ctx->ops.update) <answer> if 
<token> = damon_region_sz_limit(ctx); <answer> sz_limit 
damon_for_each_target(t, ctx) <token> <answer> { 
damon_for_each_region_safe(r, <token> t) <answer> next, 
<token> t); <answer> damon_destroy_region(r, 
if <token> <answer> (ctx->callback.before_terminate) 
if <token> <answer> (ctx->ops.cleanup) 
pr_debug("kdamond <token> finishes\n", current->pid); <answer> (%d) 
<token> = NULL; <answer> ctx->kdamond 
if (!nr_running_ctxs && <token> <answer> running_exclusive_ctxs) 
running_exclusive_ctxs = <token> <answer> false; 
return <token> <answer> 0; 
struct <token> { <answer> damon_system_ram_region 
unsigned <token> start; <answer> long 
<token> long end; <answer> unsigned 
static int walk_system_ram(struct resource <token> void *arg) <answer> *res, 
struct damon_system_ram_region <token> = arg; <answer> *a 
if (a->end - <token> < resource_size(res)) { <answer> a->start 
a->start <token> res->start; <answer> = 
a->end = <token> <answer> res->end; 
return <token> <answer> 0; 
static bool damon_find_biggest_system_ram(unsigned long <token> <answer> *start, 
unsigned long <token> <answer> *end) 
struct <token> arg = {}; <answer> damon_system_ram_region 
<token> ULONG_MAX, &arg, walk_system_ram); <answer> walk_system_ram_res(0, 
if (arg.end <token> arg.start) <answer> <= 
<token> false; <answer> return 
<token> = arg.start; <answer> *start 
*end <token> arg.end; <answer> = 
<token> true; <answer> return 
int <token> damon_target *t, <answer> damon_set_region_biggest_system_ram_default(struct 
<token> long *start, unsigned long *end) <answer> unsigned 
struct damon_addr_range <token> <answer> addr_range; 
if (*start <token> *end) <answer> > 
return <token> <answer> -EINVAL; 
if (!*start <token> !*end && <answer> && 
<token> end)) <answer> !damon_find_biggest_system_ram(start, 
<token> -EINVAL; <answer> return 
addr_range.start = <token> <answer> *start; 
<token> = *end; <answer> addr_range.end 
return damon_set_regions(t, <token> 1); <answer> &addr_range, 
static unsigned int damon_moving_sum(unsigned <token> mvsum, unsigned int nomvsum, <answer> int 
unsigned int len_window, <token> int new_value) <answer> unsigned 
<token> mvsum - nomvsum / len_window + new_value; <answer> return 
<token> damon_update_region_access_rate(struct damon_region *r, bool accessed, <answer> void 
struct <token> *attrs) <answer> damon_attrs 
unsigned <token> len_window = 1; <answer> int 
<token> (attrs->sample_interval) <answer> if 
<token> = damon_max_nr_accesses(attrs); <answer> len_window 
r->nr_accesses_bp = <token> <answer> damon_moving_sum(r->nr_accesses_bp, 
r->last_nr_accesses * 10000, <token> <answer> len_window, 
accessed ? <token> : 0); <answer> 10000 
if <token> <answer> (accessed) 
static <token> __init damon_init(void) <answer> int 
damon_region_cache = KMEM_CACHE(damon_region, <token> <answer> 0); 
if <token> { <answer> (unlikely(!damon_region_cache)) 
<token> damon_region_cache fails\n"); <answer> pr_err("creating 
<token> -ENOMEM; <answer> return 
<token> 0; <answer> return 
<token> "core-test.h" <answer> #include 
#include <token> <answer> <linux/errno.h> 
<token> <linux/hrtimer.h> <answer> #include 
#include <token> <answer> <linux/iopoll.h> 
<token> <linux/delay.h> <answer> #include 
<token> <linux/slab.h> <answer> #include 
<token> <linux/timekeeping.h> <answer> #include 
<token> <linux/clk-provider.h> <answer> #include 
#include <token> <answer> <linux/io.h> 
<token> "clk.h" <answer> #include 
<token> "clk-pll.h" <answer> #include 
#define PLL_TIMEOUT_US <token> <answer> 20000U 
#define <token> 1000000U <answer> PLL_TIMEOUT_LOOPS 
struct samsung_clk_pll <token> <answer> { 
struct <token> hw; <answer> clk_hw 
<token> __iomem *lock_reg; <answer> void 
<token> __iomem *con_reg; <answer> void 
if (pll_early_timeout || <token> { <answer> timekeeping_suspended) 
<token> = PLL_TIMEOUT_LOOPS; <answer> i 
while <token> > 0) { <answer> (i-- 
if <token> & reg_mask) <answer> (readl_relaxed(pll->con_reg) 
<token> 0; <answer> return 
ret <token> -ETIMEDOUT; <answer> = 
} else <token> <answer> { 
<token> = readl_relaxed_poll_timeout_atomic(pll->con_reg, val, <answer> ret 
val <token> reg_mask, 0, PLL_TIMEOUT_US); <answer> & 
if <token> < 0) <answer> (ret 
pr_err("Could not lock PLL %s\n", <token> <answer> clk_hw_get_name(&pll->hw)); 
return <token> <answer> ret; 
<token> int samsung_pll3xxx_enable(struct clk_hw *hw) <answer> static 
struct samsung_clk_pll *pll <token> to_clk_pll(hw); <answer> = 
<token> tmp; <answer> u32 
<token> = readl_relaxed(pll->con_reg); <answer> tmp 
tmp <token> BIT(pll->enable_offs); <answer> |= 
writel_relaxed(tmp, <token> <answer> pll->con_reg); 
return samsung_pll_lock_wait(pll, <token> <answer> BIT(pll->lock_offs)); 
static void samsung_pll3xxx_disable(struct clk_hw <token> <answer> *hw) 
struct samsung_clk_pll <token> = to_clk_pll(hw); <answer> *pll 
u32 <token> <answer> tmp; 
tmp <token> readl_relaxed(pll->con_reg); <answer> = 
<token> &= ~BIT(pll->enable_offs); <answer> tmp 
<token> pll->con_reg); <answer> writel_relaxed(tmp, 
#define <token> (0xff) <answer> PLL2126_MDIV_MASK 
<token> PLL2126_PDIV_MASK (0x3f) <answer> #define 
<token> PLL2126_SDIV_MASK (0x3) <answer> #define 
#define PLL2126_MDIV_SHIFT <token> <answer> (16) 
#define <token> (8) <answer> PLL2126_PDIV_SHIFT 
<token> PLL2126_SDIV_SHIFT (0) <answer> #define 
static <token> long samsung_pll2126_recalc_rate(struct clk_hw *hw, <answer> unsigned 
unsigned long <token> <answer> parent_rate) 
struct samsung_clk_pll *pll <token> to_clk_pll(hw); <answer> = 
u32 pll_con, mdiv, <token> sdiv; <answer> pdiv, 
u64 <token> = parent_rate; <answer> fvco 
pll_con = <token> <answer> readl_relaxed(pll->con_reg); 
mdiv <token> (pll_con >> PLL2126_MDIV_SHIFT) & PLL2126_MDIV_MASK; <answer> = 
pdiv = (pll_con >> PLL2126_PDIV_SHIFT) <token> PLL2126_PDIV_MASK; <answer> & 
sdiv = (pll_con >> <token> & PLL2126_SDIV_MASK; <answer> PLL2126_SDIV_SHIFT) 
fvco *= <token> + 8); <answer> (mdiv 
do_div(fvco, (pdiv + 2) << <token> <answer> sdiv); 
<token> (unsigned long)fvco; <answer> return 
static const struct <token> samsung_pll2126_clk_ops = { <answer> clk_ops 
.recalc_rate = <token> <answer> samsung_pll2126_recalc_rate, 
<token> PLL3000_MDIV_MASK (0xff) <answer> #define 
<token> PLL3000_PDIV_MASK (0x3) <answer> #define 
#define PLL3000_SDIV_MASK <token> <answer> (0x3) 
#define PLL3000_MDIV_SHIFT <token> <answer> (16) 
#define <token> (8) <answer> PLL3000_PDIV_SHIFT 
#define PLL3000_SDIV_SHIFT <token> <answer> (0) 
static <token> long samsung_pll3000_recalc_rate(struct clk_hw *hw, <answer> unsigned 
unsigned <token> parent_rate) <answer> long 
<token> samsung_clk_pll *pll = to_clk_pll(hw); <answer> struct 
u32 <token> mdiv, pdiv, sdiv; <answer> pll_con, 
<token> fvco = parent_rate; <answer> u64 
pll_con <token> readl_relaxed(pll->con_reg); <answer> = 
mdiv <token> (pll_con >> PLL3000_MDIV_SHIFT) & PLL3000_MDIV_MASK; <answer> = 
pdiv <token> (pll_con >> PLL3000_PDIV_SHIFT) & PLL3000_PDIV_MASK; <answer> = 
sdiv = (pll_con >> PLL3000_SDIV_SHIFT) & <token> <answer> PLL3000_SDIV_MASK; 
fvco *= (2 * <token> + 8)); <answer> (mdiv 
do_div(fvco, pdiv << <token> <answer> sdiv); 
<token> (unsigned long)fvco; <answer> return 
static const struct <token> samsung_pll3000_clk_ops = { <answer> clk_ops 
.recalc_rate = <token> <answer> samsung_pll3000_recalc_rate, 
<token> |= ((u16)rate->kdiv << PLL0831X_KDIV_SHIFT); <answer> pll_con5 
#define PLL4502_LOCK_FACTOR <token> <answer> 400 
#define PLL4508_LOCK_FACTOR <token> <answer> 240 
#define PLL45XX_MDIV_MASK <token> <answer> (0x3FF) 
<token> PLL45XX_PDIV_MASK (0x3F) <answer> #define 
<token> PLL45XX_SDIV_MASK (0x7) <answer> #define 
#define <token> (0x1F) <answer> PLL45XX_AFC_MASK 
#define <token> (16) <answer> PLL45XX_MDIV_SHIFT 
#define <token> (8) <answer> PLL45XX_PDIV_SHIFT 
#define PLL45XX_SDIV_SHIFT <token> <answer> (0) 
#define <token> (0) <answer> PLL45XX_AFC_SHIFT 
#define PLL45XX_ENABLE <token> <answer> BIT(31) 
#define PLL45XX_LOCKED <token> <answer> BIT(29) 
static unsigned long samsung_pll45xx_recalc_rate(struct clk_hw <token> <answer> *hw, 
unsigned <token> parent_rate) <answer> long 
<token> samsung_clk_pll *pll = to_clk_pll(hw); <answer> struct 
u32 <token> pdiv, sdiv, pll_con; <answer> mdiv, 
u64 fvco <token> parent_rate; <answer> = 
pll_con = <token> <answer> readl_relaxed(pll->con_reg); 
mdiv = (pll_con >> PLL45XX_MDIV_SHIFT) & <token> <answer> PLL45XX_MDIV_MASK; 
<token> = (pll_con >> PLL45XX_PDIV_SHIFT) & PLL45XX_PDIV_MASK; <answer> pdiv 
sdiv = (pll_con >> <token> & PLL45XX_SDIV_MASK; <answer> PLL45XX_SDIV_SHIFT) 
<token> (pll->type == pll_4508) <answer> if 
sdiv = sdiv - <token> <answer> 1; 
fvco *= <token> <answer> mdiv; 
do_div(fvco, (pdiv <token> sdiv)); <answer> << 
<token> (unsigned long)fvco; <answer> return 
static bool samsung_pll45xx_mp_change(u32 <token> u32 pll_con1, <answer> pll_con0, 
const struct samsung_pll_rate_table <token> <answer> *rate) 
u32 old_mdiv, <token> old_afc; <answer> old_pdiv, 
old_mdiv <token> (pll_con0 >> PLL45XX_MDIV_SHIFT) & PLL45XX_MDIV_MASK; <answer> = 
old_pdiv = (pll_con0 >> <token> & PLL45XX_PDIV_MASK; <answer> PLL45XX_PDIV_SHIFT) 
old_afc = (pll_con1 >> PLL45XX_AFC_SHIFT) & <token> <answer> PLL45XX_AFC_MASK; 
return (old_mdiv != rate->mdiv || old_pdiv != <token> <answer> rate->pdiv 
|| old_afc <token> rate->afc); <answer> != 
static int samsung_pll45xx_set_rate(struct clk_hw *hw, <token> long drate, <answer> unsigned 
<token> long prate) <answer> unsigned 
struct samsung_clk_pll *pll <token> to_clk_pll(hw); <answer> = 
const struct samsung_pll_rate_table <token> <answer> *rate; 
u32 con0, <token> <answer> con1; 
#define PLL46XX_LOCK_FACTOR <token> <answer> 3000 
#define <token> (1) <answer> PLL46XX_VSEL_MASK 
#define PLL46XX_MDIV_MASK <token> <answer> (0x1FF) 
#define PLL1460X_MDIV_MASK <token> <answer> (0x3FF) 
<token> PLL46XX_PDIV_MASK (0x3F) <answer> #define 
#define <token> (0x7) <answer> PLL46XX_SDIV_MASK 
<token> PLL46XX_VSEL_SHIFT (27) <answer> #define 
#define <token> (16) <answer> PLL46XX_MDIV_SHIFT 
#define <token> (8) <answer> PLL46XX_PDIV_SHIFT 
#define <token> (0) <answer> PLL46XX_SDIV_SHIFT 
#define PLL46XX_KDIV_MASK <token> <answer> (0xFFFF) 
#define PLL4650C_KDIV_MASK <token> <answer> (0xFFF) 
#define <token> (0) <answer> PLL46XX_KDIV_SHIFT 
<token> PLL46XX_MFR_MASK (0x3F) <answer> #define 
#define <token> (0x1F) <answer> PLL46XX_MRR_MASK 
#define PLL46XX_KDIV_SHIFT <token> <answer> (0) 
#define PLL46XX_MFR_SHIFT <token> <answer> (16) 
<token> PLL46XX_MRR_SHIFT (24) <answer> #define 
<token> PLL46XX_ENABLE BIT(31) <answer> #define 
#define <token> BIT(29) <answer> PLL46XX_LOCKED 
#define PLL46XX_VSEL <token> <answer> BIT(27) 
static unsigned long samsung_pll46xx_recalc_rate(struct <token> *hw, <answer> clk_hw 
unsigned <token> parent_rate) <answer> long 
struct <token> *pll = to_clk_pll(hw); <answer> samsung_clk_pll 
u32 mdiv, pdiv, sdiv, kdiv, pll_con0, pll_con1, <token> <answer> shift; 
u64 fvco <token> parent_rate; <answer> = 
pll_con0 = <token> <answer> readl_relaxed(pll->con_reg); 
pll_con1 = readl_relaxed(pll->con_reg <token> 4); <answer> + 
mdiv = (pll_con0 >> PLL46XX_MDIV_SHIFT) & ((pll->type == pll_1460x) <token> <answer> ? 
PLL1460X_MDIV_MASK <token> PLL46XX_MDIV_MASK); <answer> : 
pdiv = (pll_con0 >> PLL46XX_PDIV_SHIFT) <token> PLL46XX_PDIV_MASK; <answer> & 
sdiv = <token> >> PLL46XX_SDIV_SHIFT) & PLL46XX_SDIV_MASK; <answer> (pll_con0 
kdiv = <token> == pll_4650c ? pll_con1 & PLL4650C_KDIV_MASK : <answer> pll->type 
pll_con1 <token> PLL46XX_KDIV_MASK; <answer> & 
shift = ((pll->type == pll_4600) || (pll->type == pll_1460x)) ? 16 : <token> <answer> 10; 
<token> *= (mdiv << shift) + kdiv; <answer> fvco 
do_div(fvco, (pdiv << <token> <answer> sdiv)); 
<token> >>= shift; <answer> fvco 
return <token> long)fvco; <answer> (unsigned 
static bool samsung_pll46xx_mpk_change(u32 pll_con0, u32 <token> <answer> pll_con1, 
const <token> samsung_pll_rate_table *rate) <answer> struct 
<token> old_mdiv, old_pdiv, old_kdiv; <answer> u32 
old_mdiv <token> (pll_con0 >> PLL46XX_MDIV_SHIFT) & PLL46XX_MDIV_MASK; <answer> = 
old_pdiv = (pll_con0 >> PLL46XX_PDIV_SHIFT) <token> PLL46XX_PDIV_MASK; <answer> & 
old_kdiv = (pll_con1 >> PLL46XX_KDIV_SHIFT) <token> PLL46XX_KDIV_MASK; <answer> & 
return (old_mdiv != rate->mdiv || old_pdiv != <token> <answer> rate->pdiv 
|| old_kdiv != <token> <answer> rate->kdiv); 
static int samsung_pll46xx_set_rate(struct <token> *hw, unsigned long drate, <answer> clk_hw 
<token> long prate) <answer> unsigned 
<token> samsung_clk_pll *pll = to_clk_pll(hw); <answer> struct 
const struct samsung_pll_rate_table <token> <answer> *rate; 
u32 con0, con1, <token> <answer> lock; 
#define PLL6552_MDIV_MASK <token> <answer> 0x3ff 
#define <token> 0x3f <answer> PLL6552_PDIV_MASK 
<token> PLL6552_SDIV_MASK 0x7 <answer> #define 
<token> PLL6552_MDIV_SHIFT 16 <answer> #define 
<token> PLL6552_MDIV_SHIFT_2416 14 <answer> #define 
<token> PLL6552_PDIV_SHIFT 8 <answer> #define 
#define <token> 5 <answer> PLL6552_PDIV_SHIFT_2416 
#define <token> 0 <answer> PLL6552_SDIV_SHIFT 
static unsigned <token> samsung_pll6552_recalc_rate(struct clk_hw *hw, <answer> long 
<token> long parent_rate) <answer> unsigned 
<token> samsung_clk_pll *pll = to_clk_pll(hw); <answer> struct 
<token> mdiv, pdiv, sdiv, pll_con; <answer> u32 
u64 fvco = <token> <answer> parent_rate; 
<token> = readl_relaxed(pll->con_reg); <answer> pll_con 
if (pll->type == pll_6552_s3c2416) <token> <answer> { 
mdiv = (pll_con >> <token> & PLL6552_MDIV_MASK; <answer> PLL6552_MDIV_SHIFT_2416) 
pdiv = <token> >> PLL6552_PDIV_SHIFT_2416) & PLL6552_PDIV_MASK; <answer> (pll_con 
<token> else { <answer> } 
<token> = (pll_con >> PLL6552_MDIV_SHIFT) & PLL6552_MDIV_MASK; <answer> mdiv 
pdiv = (pll_con >> PLL6552_PDIV_SHIFT) <token> PLL6552_PDIV_MASK; <answer> & 
sdiv = (pll_con <token> PLL6552_SDIV_SHIFT) & PLL6552_SDIV_MASK; <answer> >> 
fvco <token> mdiv; <answer> *= 
do_div(fvco, (pdiv << <token> <answer> sdiv)); 
return <token> long)fvco; <answer> (unsigned 
static const struct clk_ops <token> = { <answer> samsung_pll6552_clk_ops 
<token> = samsung_pll6552_recalc_rate, <answer> .recalc_rate 
<token> PLL6553_MDIV_MASK 0xff <answer> #define 
#define <token> 0x3f <answer> PLL6553_PDIV_MASK 
#define <token> 0x7 <answer> PLL6553_SDIV_MASK 
#define PLL6553_KDIV_MASK <token> <answer> 0xffff 
<token> PLL6553_MDIV_SHIFT 16 <answer> #define 
#define PLL6553_PDIV_SHIFT <token> <answer> 8 
#define <token> 0 <answer> PLL6553_SDIV_SHIFT 
#define PLL6553_KDIV_SHIFT <token> <answer> 0 
static unsigned <token> samsung_pll6553_recalc_rate(struct clk_hw *hw, <answer> long 
unsigned long <token> <answer> parent_rate) 
struct <token> *pll = to_clk_pll(hw); <answer> samsung_clk_pll 
u32 <token> pdiv, sdiv, kdiv, pll_con0, pll_con1; <answer> mdiv, 
u64 <token> = parent_rate; <answer> fvco 
<token> = readl_relaxed(pll->con_reg); <answer> pll_con0 
pll_con1 = readl_relaxed(pll->con_reg <token> 0x4); <answer> + 
mdiv = <token> >> PLL6553_MDIV_SHIFT) & PLL6553_MDIV_MASK; <answer> (pll_con0 
pdiv <token> (pll_con0 >> PLL6553_PDIV_SHIFT) & PLL6553_PDIV_MASK; <answer> = 
sdiv = (pll_con0 >> <token> & PLL6553_SDIV_MASK; <answer> PLL6553_SDIV_SHIFT) 
kdiv = (pll_con1 >> PLL6553_KDIV_SHIFT) & <token> <answer> PLL6553_KDIV_MASK; 
<token> *= (mdiv << 16) + kdiv; <answer> fvco 
<token> (pdiv << sdiv)); <answer> do_div(fvco, 
fvco <token> 16; <answer> >>= 
<token> (unsigned long)fvco; <answer> return 
static const struct <token> samsung_pll6553_clk_ops = { <answer> clk_ops 
<token> = samsung_pll6553_recalc_rate, <answer> .recalc_rate 
<token> PLL2550X_R_MASK (0x1) <answer> #define 
#define <token> (0x3F) <answer> PLL2550X_P_MASK 
#define PLL2550X_M_MASK <token> <answer> (0x3FF) 
#define <token> (0x7) <answer> PLL2550X_S_MASK 
#define PLL2550X_R_SHIFT <token> <answer> (20) 
#define <token> (14) <answer> PLL2550X_P_SHIFT 
#define <token> (4) <answer> PLL2550X_M_SHIFT 
<token> PLL2550X_S_SHIFT (0) <answer> #define 
static unsigned <token> samsung_pll2550x_recalc_rate(struct clk_hw *hw, <answer> long 
unsigned <token> parent_rate) <answer> long 
struct samsung_clk_pll *pll = <token> <answer> to_clk_pll(hw); 
u32 r, <token> m, s, pll_stat; <answer> p, 
u64 fvco = <token> <answer> parent_rate; 
pll_stat = <token> <answer> readl_relaxed(pll->con_reg); 
r = (pll_stat >> <token> & PLL2550X_R_MASK; <answer> PLL2550X_R_SHIFT) 
<token> (!r) <answer> if 
return <token> <answer> 0; 
<token> = (pll_stat >> PLL2550X_P_SHIFT) & PLL2550X_P_MASK; <answer> p 
m = (pll_stat >> <token> & PLL2550X_M_MASK; <answer> PLL2550X_M_SHIFT) 
s = (pll_stat >> PLL2550X_S_SHIFT) & <token> <answer> PLL2550X_S_MASK; 
<token> *= m; <answer> fvco 
do_div(fvco, (p <token> s)); <answer> << 
return <token> long)fvco; <answer> (unsigned 
static const struct clk_ops samsung_pll2550x_clk_ops = <token> <answer> { 
.recalc_rate = <token> <answer> samsung_pll2550x_recalc_rate, 
#include <token> <answer> <linux/syscalls.h> 
<token> <linux/export.h> <answer> #include 
#include <token> <answer> <linux/fs.h> 
<token> <linux/file.h> <answer> #include 
#include <token> <answer> <linux/mount.h> 
<token> <linux/namei.h> <answer> #include 
#include <token> <answer> <linux/statfs.h> 
#include <token> <answer> <linux/security.h> 
#include <token> <answer> <linux/uaccess.h> 
<token> <linux/compat.h> <answer> #include 
<token> "internal.h" <answer> #include 
static int <token> mnt_flags) <answer> flags_by_mnt(int 
<token> flags = 0; <answer> int 
<token> (mnt_flags & MNT_READONLY) <answer> if 
flags <token> ST_RDONLY; <answer> |= 
if (mnt_flags & <token> <answer> MNT_NOSUID) 
flags <token> ST_NOSUID; <answer> |= 
if <token> & MNT_NODEV) <answer> (mnt_flags 
flags <token> ST_NODEV; <answer> |= 
if (mnt_flags & <token> <answer> MNT_NOEXEC) 
<token> |= ST_NOEXEC; <answer> flags 
<token> (mnt_flags & MNT_NOATIME) <answer> if 
flags <token> ST_NOATIME; <answer> |= 
if (mnt_flags & <token> <answer> MNT_NODIRATIME) 
flags |= <token> <answer> ST_NODIRATIME; 
if <token> & MNT_RELATIME) <answer> (mnt_flags 
flags |= <token> <answer> ST_RELATIME; 
if <token> & MNT_NOSYMFOLLOW) <answer> (mnt_flags 
flags <token> ST_NOSYMFOLLOW; <answer> |= 
<token> flags; <answer> return 
static int flags_by_sb(int <token> <answer> s_flags) 
<token> flags = 0; <answer> int 
if (s_flags <token> SB_SYNCHRONOUS) <answer> & 
flags |= <token> <answer> ST_SYNCHRONOUS; 
if (s_flags <token> SB_MANDLOCK) <answer> & 
flags <token> ST_MANDLOCK; <answer> |= 
if (s_flags & <token> <answer> SB_RDONLY) 
flags <token> ST_RDONLY; <answer> |= 
<token> flags; <answer> return 
static int calculate_f_flags(struct <token> *mnt) <answer> vfsmount 
<token> ST_VALID | flags_by_mnt(mnt->mnt_flags) | <answer> return 
static int statfs_by_dentry(struct <token> *dentry, struct kstatfs *buf) <answer> dentry 
<token> retval; <answer> int 
if <token> <answer> (!dentry->d_sb->s_op->statfs) 
return <token> <answer> -ENOSYS; 
memset(buf, <token> sizeof(*buf)); <answer> 0, 
<token> = security_sb_statfs(dentry); <answer> retval 
if <token> <answer> (retval) 
<token> retval; <answer> return 
retval = dentry->d_sb->s_op->statfs(dentry, <token> <answer> buf); 
if (retval == <token> && buf->f_frsize == 0) <answer> 0 
buf->f_frsize = <token> <answer> buf->f_bsize; 
return <token> <answer> retval; 
int vfs_get_fsid(struct dentry *dentry, <token> *fsid) <answer> __kernel_fsid_t 
struct <token> st; <answer> kstatfs 
int <token> <answer> error; 
error = statfs_by_dentry(dentry, <token> <answer> &st); 
if <token> <answer> (error) 
return <token> <answer> error; 
*fsid <token> st.f_fsid; <answer> = 
<token> 0; <answer> return 
int <token> struct path *path, struct kstatfs *buf) <answer> vfs_statfs(const 
<token> error; <answer> int 
<token> = statfs_by_dentry(path->dentry, buf); <answer> error 
<token> (!error) <answer> if 
buf->f_flags = <token> <answer> calculate_f_flags(path->mnt); 
<token> error; <answer> return 
<token> user_statfs(const char __user *pathname, struct kstatfs *st) <answer> int 
struct <token> path; <answer> path 
<token> error; <answer> int 
unsigned int <token> = LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT; <answer> lookup_flags 
<token> = user_path_at(AT_FDCWD, pathname, lookup_flags, &path); <answer> error 
<token> (!error) { <answer> if 
error <token> vfs_statfs(&path, st); <answer> = 
<token> (retry_estale(error, lookup_flags)) { <answer> if 
lookup_flags |= <token> <answer> LOOKUP_REVAL; 
goto <token> <answer> retry; 
<token> error; <answer> return 
int fd_statfs(int fd, <token> kstatfs *st) <answer> struct 
struct fd <token> = fdget_raw(fd); <answer> f 
<token> error = -EBADF; <answer> int 
<token> (f.file) { <answer> if 
error = vfs_statfs(&f.file->f_path, <token> <answer> st); 
return <token> <answer> error; 
static int do_statfs_native(struct <token> *st, struct statfs __user *p) <answer> kstatfs 
struct <token> buf; <answer> statfs 
if <token> == sizeof(*st)) <answer> (sizeof(buf) 
<token> st, sizeof(*st)); <answer> memcpy(&buf, 
else <token> <answer> { 
memset(&buf, <token> sizeof(buf)); <answer> 0, 
if (sizeof buf.f_blocks <token> 4) { <answer> == 
<token> ((st->f_blocks | st->f_bfree | st->f_bavail | <answer> if 
st->f_bsize | <token> & <answer> st->f_frsize) 
<token> -EOVERFLOW; <answer> return 
if (st->f_files != -1 <token> <answer> && 
(st->f_files <token> 0xffffffff00000000ULL)) <answer> & 
<token> -EOVERFLOW; <answer> return 
<token> (st->f_ffree != -1 && <answer> if 
(st->f_ffree <token> 0xffffffff00000000ULL)) <answer> & 
return <token> <answer> -EOVERFLOW; 
buf.f_type <token> st->f_type; <answer> = 
<token> = st->f_bsize; <answer> buf.f_bsize 
buf.f_blocks <token> st->f_blocks; <answer> = 
buf.f_bfree = <token> <answer> st->f_bfree; 
buf.f_bavail = <token> <answer> st->f_bavail; 
<token> = st->f_files; <answer> buf.f_files 
buf.f_ffree = <token> <answer> st->f_ffree; 
buf.f_fsid <token> st->f_fsid; <answer> = 
<token> = st->f_namelen; <answer> buf.f_namelen 
<token> = st->f_frsize; <answer> buf.f_frsize 
buf.f_flags = <token> <answer> st->f_flags; 
if (copy_to_user(p, <token> sizeof(buf))) <answer> &buf, 
return <token> <answer> -EFAULT; 
<token> 0; <answer> return 
static int do_statfs64(struct kstatfs <token> struct statfs64 __user *p) <answer> *st, 
struct <token> buf; <answer> statfs64 
if (sizeof(buf) == <token> <answer> sizeof(*st)) 
memcpy(&buf, st, <token> <answer> sizeof(*st)); 
else <token> <answer> { 
memset(&buf, <token> sizeof(buf)); <answer> 0, 
buf.f_type = <token> <answer> st->f_type; 
<token> = st->f_bsize; <answer> buf.f_bsize 
buf.f_blocks <token> st->f_blocks; <answer> = 
buf.f_bfree <token> st->f_bfree; <answer> = 
buf.f_bavail <token> st->f_bavail; <answer> = 
buf.f_files = <token> <answer> st->f_files; 
buf.f_ffree <token> st->f_ffree; <answer> = 
<token> = st->f_fsid; <answer> buf.f_fsid 
buf.f_namelen <token> st->f_namelen; <answer> = 
<token> = st->f_frsize; <answer> buf.f_frsize 
buf.f_flags <token> st->f_flags; <answer> = 
if (copy_to_user(p, &buf, <token> <answer> sizeof(buf))) 
return <token> <answer> -EFAULT; 
<token> 0; <answer> return 
<token> const char __user *, pathname, struct statfs __user *, buf) <answer> SYSCALL_DEFINE2(statfs, 
struct kstatfs <token> <answer> st; 
int error <token> user_statfs(pathname, &st); <answer> = 
if <token> <answer> (!error) 
error <token> do_statfs_native(&st, buf); <answer> = 
return <token> <answer> error; 
SYSCALL_DEFINE3(statfs64, const char __user <token> pathname, size_t, sz, struct statfs64 __user *, buf) <answer> *, 
struct <token> st; <answer> kstatfs 
<token> error; <answer> int 
if (sz != <token> <answer> sizeof(*buf)) 
return <token> <answer> -EINVAL; 
<token> = user_statfs(pathname, &st); <answer> error 
<token> (!error) <answer> if 
error = do_statfs64(&st, <token> <answer> buf); 
<token> error; <answer> return 
SYSCALL_DEFINE2(fstatfs, unsigned int, fd, struct <token> __user *, buf) <answer> statfs 
struct <token> st; <answer> kstatfs 
int <token> = fd_statfs(fd, &st); <answer> error 
<token> (!error) <answer> if 
error = do_statfs_native(&st, <token> <answer> buf); 
return <token> <answer> error; 
SYSCALL_DEFINE3(fstatfs64, unsigned int, fd, size_t, sz, <token> statfs64 __user *, buf) <answer> struct 
struct kstatfs <token> <answer> st; 
<token> error; <answer> int 
if (sz != <token> <answer> sizeof(*buf)) 
<token> -EINVAL; <answer> return 
error = <token> &st); <answer> fd_statfs(fd, 
if <token> <answer> (!error) 
error <token> do_statfs64(&st, buf); <answer> = 
return <token> <answer> error; 
static int vfs_ustat(dev_t dev, struct <token> *sbuf) <answer> kstatfs 
<token> super_block *s = user_get_super(dev, false); <answer> struct 
<token> err; <answer> int 
<token> (!s) <answer> if 
return <token> <answer> -EINVAL; 
err <token> statfs_by_dentry(s->s_root, sbuf); <answer> = 
<token> err; <answer> return 
SYSCALL_DEFINE2(ustat, unsigned, dev, struct ustat __user <token> ubuf) <answer> *, 
struct <token> tmp; <answer> ustat 
struct <token> sbuf; <answer> kstatfs 
<token> err = vfs_ustat(new_decode_dev(dev), &sbuf); <answer> int 
<token> (err) <answer> if 
<token> err; <answer> return 
memset(&tmp,0,sizeof(struct <token> <answer> ustat)); 
tmp.f_tfree = <token> <answer> sbuf.f_bfree; 
<token> (IS_ENABLED(CONFIG_ARCH_32BIT_USTAT_F_TINODE)) <answer> if 
tmp.f_tinode = min_t(u64, sbuf.f_ffree, <token> <answer> UINT_MAX); 
tmp.f_tinode <token> sbuf.f_ffree; <answer> = 
return copy_to_user(ubuf, <token> sizeof(struct ustat)) ? -EFAULT : 0; <answer> &tmp, 
<token> CONFIG_COMPAT <answer> #ifdef 
static int <token> compat_statfs __user *ubuf, struct kstatfs *kbuf) <answer> put_compat_statfs(struct 
struct <token> buf; <answer> compat_statfs 
<token> (sizeof ubuf->f_blocks == 4) { <answer> if 
if ((kbuf->f_blocks | kbuf->f_bfree | <token> | <answer> kbuf->f_bavail 
<token> | kbuf->f_frsize) & 0xffffffff00000000ULL) <answer> kbuf->f_bsize 
<token> -EOVERFLOW; <answer> return 
if (kbuf->f_files <token> 0xffffffffffffffffULL <answer> != 
<token> (kbuf->f_files & 0xffffffff00000000ULL)) <answer> && 
<token> -EOVERFLOW; <answer> return 
if (kbuf->f_ffree != <token> <answer> 0xffffffffffffffffULL 
<token> (kbuf->f_ffree & 0xffffffff00000000ULL)) <answer> && 
<token> -EOVERFLOW; <answer> return 
memset(&buf, 0, sizeof(struct <token> <answer> compat_statfs)); 
<token> = kbuf->f_type; <answer> buf.f_type 
buf.f_bsize <token> kbuf->f_bsize; <answer> = 
buf.f_blocks <token> kbuf->f_blocks; <answer> = 
buf.f_bfree <token> kbuf->f_bfree; <answer> = 
buf.f_bavail <token> kbuf->f_bavail; <answer> = 
buf.f_files <token> kbuf->f_files; <answer> = 
buf.f_ffree <token> kbuf->f_ffree; <answer> = 
buf.f_namelen = <token> <answer> kbuf->f_namelen; 
buf.f_fsid.val[0] = <token> <answer> kbuf->f_fsid.val[0]; 
<token> = kbuf->f_fsid.val[1]; <answer> buf.f_fsid.val[1] 
buf.f_frsize <token> kbuf->f_frsize; <answer> = 
buf.f_flags <token> kbuf->f_flags; <answer> = 
if <token> &buf, sizeof(struct compat_statfs))) <answer> (copy_to_user(ubuf, 
<token> -EFAULT; <answer> return 
return <token> <answer> 0; 
COMPAT_SYSCALL_DEFINE2(statfs, const char __user *, pathname, struct compat_statfs <token> *, buf) <answer> __user 
<token> kstatfs tmp; <answer> struct 
int error = user_statfs(pathname, <token> <answer> &tmp); 
<token> (!error) <answer> if 
error <token> put_compat_statfs(buf, &tmp); <answer> = 
return <token> <answer> error; 
COMPAT_SYSCALL_DEFINE2(fstatfs, unsigned int, fd, struct compat_statfs __user *, <token> <answer> buf) 
struct kstatfs <token> <answer> tmp; 
int error = <token> &tmp); <answer> fd_statfs(fd, 
<token> (!error) <answer> if 
error <token> put_compat_statfs(buf, &tmp); <answer> = 
return <token> <answer> error; 
static int <token> compat_statfs64 __user *ubuf, struct kstatfs *kbuf) <answer> put_compat_statfs64(struct 
struct <token> buf; <answer> compat_statfs64 
if ((kbuf->f_bsize | kbuf->f_frsize) <token> 0xffffffff00000000ULL) <answer> & 
<token> -EOVERFLOW; <answer> return 
memset(&buf, 0, sizeof(struct <token> <answer> compat_statfs64)); 
buf.f_type <token> kbuf->f_type; <answer> = 
buf.f_bsize <token> kbuf->f_bsize; <answer> = 
buf.f_blocks = <token> <answer> kbuf->f_blocks; 
buf.f_bfree <token> kbuf->f_bfree; <answer> = 
<token> = kbuf->f_bavail; <answer> buf.f_bavail 
buf.f_files <token> kbuf->f_files; <answer> = 
<token> = kbuf->f_ffree; <answer> buf.f_ffree 
buf.f_namelen <token> kbuf->f_namelen; <answer> = 
buf.f_fsid.val[0] <token> kbuf->f_fsid.val[0]; <answer> = 
<token> = kbuf->f_fsid.val[1]; <answer> buf.f_fsid.val[1] 
<token> = kbuf->f_frsize; <answer> buf.f_frsize 
<token> = kbuf->f_flags; <answer> buf.f_flags 
if <token> &buf, sizeof(struct compat_statfs64))) <answer> (copy_to_user(ubuf, 
return <token> <answer> -EFAULT; 
<token> 0; <answer> return 
int kcompat_sys_statfs64(const char __user * pathname, compat_size_t sz, struct compat_statfs64 __user * <token> <answer> buf) 
struct kstatfs <token> <answer> tmp; 
int <token> <answer> error; 
if (sz != <token> <answer> sizeof(*buf)) 
return <token> <answer> -EINVAL; 
<token> = user_statfs(pathname, &tmp); <answer> error 
if <token> <answer> (!error) 
error <token> put_compat_statfs64(buf, &tmp); <answer> = 
<token> error; <answer> return 
COMPAT_SYSCALL_DEFINE3(statfs64, const char __user *, pathname, compat_size_t, sz, struct compat_statfs64 __user *, <token> <answer> buf) 
<token> kcompat_sys_statfs64(pathname, sz, buf); <answer> return 
<token> kcompat_sys_fstatfs64(unsigned int fd, compat_size_t sz, struct compat_statfs64 __user * buf) <answer> int 
struct <token> tmp; <answer> kstatfs 
<token> error; <answer> int 
if <token> != sizeof(*buf)) <answer> (sz 
return <token> <answer> -EINVAL; 
error <token> fd_statfs(fd, &tmp); <answer> = 
<token> (!error) <answer> if 
error = <token> &tmp); <answer> put_compat_statfs64(buf, 
<token> error; <answer> return 
COMPAT_SYSCALL_DEFINE3(fstatfs64, unsigned int, fd, compat_size_t, sz, struct compat_statfs64 __user *, <token> <answer> buf) 
return <token> sz, buf); <answer> kcompat_sys_fstatfs64(fd, 
<token> unsigned, dev, struct compat_ustat __user *, u) <answer> COMPAT_SYSCALL_DEFINE2(ustat, 
<token> compat_ustat tmp; <answer> struct 
<token> kstatfs sbuf; <answer> struct 
int <token> = vfs_ustat(new_decode_dev(dev), &sbuf); <answer> err 
if <token> <answer> (err) 
<token> err; <answer> return 
memset(&tmp, 0, sizeof(struct <token> <answer> compat_ustat)); 
tmp.f_tfree = <token> <answer> sbuf.f_bfree; 
tmp.f_tinode = <token> <answer> sbuf.f_ffree; 
if (copy_to_user(u, &tmp, sizeof(struct <token> <answer> compat_ustat))) 
<token> -EFAULT; <answer> return 
<token> 0; <answer> return 
<token> DRV_NAME "xen_wdt" <answer> #define 
<token> <linux/bug.h> <answer> #include 
<token> <linux/errno.h> <answer> #include 
<token> <linux/fs.h> <answer> #include 
<token> <linux/hrtimer.h> <answer> #include 
#include <token> <answer> <linux/kernel.h> 
#include <token> <answer> <linux/ktime.h> 
#include <token> <answer> <linux/init.h> 
#include <token> <answer> <linux/module.h> 
#include <token> <answer> <linux/moduleparam.h> 
<token> <linux/platform_device.h> <answer> #include 
<token> <linux/watchdog.h> <answer> #include 
<token> <xen/xen.h> <answer> #include 
#include <token> <answer> <asm/xen/hypercall.h> 
<token> <xen/interface/sched.h> <answer> #include 
static struct platform_device <token> <answer> *platform_device; 
static <token> sched_watchdog wdt; <answer> struct 
<token> time64_t wdt_expires; <answer> static 
#define pr_fmt(fmt) <token> Watchdog: " fmt <answer> "IPMI 
<token> <linux/module.h> <answer> #include 
#include <token> <answer> <linux/moduleparam.h> 
#include <token> <answer> <linux/ipmi.h> 
<token> <linux/ipmi_smi.h> <answer> #include 
#include <token> <answer> <linux/mutex.h> 
#include <token> <answer> <linux/watchdog.h> 
#include <token> <answer> <linux/miscdevice.h> 
#include <token> <answer> <linux/init.h> 
#include <token> <answer> <linux/completion.h> 
#include <token> <answer> <linux/kdebug.h> 
#include <token> <answer> <linux/kstrtox.h> 
#include <token> <answer> <linux/rwsem.h> 
<token> <linux/errno.h> <answer> #include 
<token> <linux/uaccess.h> <answer> #include 
<token> <linux/notifier.h> <answer> #include 
<token> <linux/nmi.h> <answer> #include 
<token> <linux/reboot.h> <answer> #include 
#include <token> <answer> <linux/wait.h> 
#include <token> <answer> <linux/poll.h> 
<token> <linux/string.h> <answer> #include 
#include <token> <answer> <linux/ctype.h> 
<token> <linux/delay.h> <answer> #include 
#include <token> <answer> <linux/atomic.h> 
#include <token> <answer> <linux/sched/signal.h> 
#ifdef <token> <answer> CONFIG_X86 
#include <token> <answer> <asm/kdebug.h> 
#include <token> <answer> <asm/nmi.h> 
<token> HAVE_DIE_NMI <answer> #define 
#define <token> byte2, val) \ <answer> WDOG_SET_TIMEOUT(byte1, 
(byte1) = (((val) * 10) & 0xff), (byte2) = (((val) * <token> >> 8) <answer> 10) 
#define <token> byte2) \ <answer> WDOG_GET_TIMEOUT(byte1, 
(((byte1) | <token> << 8)) / 10) <answer> ((byte2) 
#define <token> 0x22 <answer> IPMI_WDOG_RESET_TIMER 
#define <token> 0x24 <answer> IPMI_WDOG_SET_TIMER 
#define IPMI_WDOG_GET_TIMER <token> <answer> 0x25 
<token> IPMI_WDOG_TIMER_NOT_INIT_RESP 0x80 <answer> #define 
static <token> <answer> DEFINE_MUTEX(ipmi_watchdog_mutex); 
static bool nowayout = <token> <answer> WATCHDOG_NOWAYOUT; 
static struct <token> *watchdog_user; <answer> ipmi_user 
static int <token> <answer> watchdog_ifnum; 
static <token> start_now; <answer> int 
static int set_param_timeout(const char <token> const struct kernel_param *kp) <answer> *val, 
<token> *endp; <answer> char 
<token> l; <answer> int 
int rv <token> 0; <answer> = 
if <token> <answer> (!val) 
<token> -EINVAL; <answer> return 
l = <token> &endp, 0); <answer> simple_strtoul(val, 
if (endp <token> val) <answer> == 
<token> -EINVAL; <answer> return 
*((int <token> = l; <answer> *)kp->arg) 
<token> (watchdog_user) <answer> if 
<token> = ipmi_set_timeout(IPMI_SET_TIMEOUT_HB_IF_NECESSARY); <answer> rv 
<token> rv; <answer> return 
static const struct kernel_param_ops param_ops_timeout <token> { <answer> = 
.set <token> set_param_timeout, <answer> = 
.get = <token> <answer> param_get_int, 
#define <token> param_check_int <answer> param_check_timeout 
typedef <token> (*action_fn)(const char *intval, char *outval); <answer> int 
<token> int action_op(const char *inval, char *outval); <answer> static 
static int preaction_op(const char *inval, char <token> <answer> *outval); 
static int preop_op(const <token> *inval, char *outval); <answer> char 
static void <token> <answer> check_parms(void); 
static int set_param_str(const char *val, const struct kernel_param <token> <answer> *kp) 
action_fn <token> = (action_fn) kp->arg; <answer> fn 
int <token> = 0; <answer> rv 
char <token> <answer> valcp[16]; 
<token> *s; <answer> char 
strscpy(valcp, val, <token> <answer> 16); 
s <token> strstrip(valcp); <answer> = 
rv <token> fn(s, NULL); <answer> = 
if <token> <answer> (rv) 
goto <token> <answer> out; 
<token> (watchdog_user) <answer> if 
rv = <token> <answer> ipmi_set_timeout(IPMI_SET_TIMEOUT_HB_IF_NECESSARY); 
<token> rv; <answer> return 
static int get_param_str(char <token> const struct kernel_param *kp) <answer> *buffer, 
action_fn fn = (action_fn) <token> <answer> kp->arg; 
<token> rv, len; <answer> int 
rv <token> fn(NULL, buffer); <answer> = 
<token> (rv) <answer> if 
return <token> <answer> rv; 
len = <token> <answer> strlen(buffer); 
buffer[len++] <token> '\n'; <answer> = 
<token> = 0; <answer> buffer[len] 
<token> len; <answer> return 
static int set_param_wdog_ifnum(const char <token> const struct kernel_param *kp) <answer> *val, 
int rv = param_set_int(val, <token> <answer> kp); 
if <token> <answer> (rv) 
return <token> <answer> rv; 
if ((ifnum_to_use < <token> || (ifnum_to_use == watchdog_ifnum)) <answer> 0) 
<token> 0; <answer> return 
<token> 0; <answer> return 
static const <token> kernel_param_ops param_ops_wdog_ifnum = { <answer> struct 
.set <token> set_param_wdog_ifnum, <answer> = 
.get <token> param_get_int, <answer> = 
<token> param_check_wdog_ifnum param_check_int <answer> #define 
static const struct kernel_param_ops param_ops_str <token> { <answer> = 
<token> = set_param_str, <answer> .set 
<token> = get_param_str, <answer> .get 
<token> wdog_ifnum, 0644); <answer> module_param(ifnum_to_use, 
MODULE_PARM_DESC(ifnum_to_use, "The interface number to use <token> the watchdog " <answer> for 
"timer. Setting to <token> defaults to the first registered " <answer> -1 
module_param(timeout, timeout, <token> <answer> 0644); 
MODULE_PARM_DESC(timeout, "Timeout <token> in seconds."); <answer> value 
<token> timeout, 0644); <answer> module_param(pretimeout, 
<token> "Pretimeout value in seconds."); <answer> MODULE_PARM_DESC(pretimeout, 
<token> timeout, 0644); <answer> module_param(panic_wdt_timeout, 
MODULE_PARM_DESC(panic_wdt_timeout, "Timeout value on kernel panic in <token> <answer> seconds."); 
module_param_cb(action, &param_ops_str, action_op, <token> <answer> 0644); 
MODULE_PARM_DESC(action, "Timeout <token> One of: " <answer> action. 
<token> none, power_cycle, power_off."); <answer> "reset, 
<token> &param_ops_str, preaction_op, 0644); <answer> module_param_cb(preaction, 
MODULE_PARM_DESC(preaction, "Pretimeout action. One of: <token> <answer> " 
"pre_none, pre_smi, <token> pre_int."); <answer> pre_nmi, 
<token> &param_ops_str, preop_op, 0644); <answer> module_param_cb(preop, 
MODULE_PARM_DESC(preop, "Pretimeout driver operation. <token> of: " <answer> One 
"preop_none, preop_panic, <token> <answer> preop_give_data."); 
module_param(start_now, int, <token> <answer> 0444); 
MODULE_PARM_DESC(start_now, <token> to 1 to start the watchdog as" <answer> "Set 
<token> as the driver is loaded."); <answer> "soon 
<token> bool, 0644); <answer> module_param(nowayout, 
MODULE_PARM_DESC(nowayout, "Watchdog cannot be stopped once <token> " <answer> started 
static int <token> <answer> ipmi_start_timer_on_heartbeat; 
<token> atomic_t msg_tofree = ATOMIC_INIT(0); <answer> static 
<token> DECLARE_COMPLETION(msg_wait); <answer> static 
static void <token> ipmi_smi_msg *msg) <answer> msg_free_smi(struct 
if <token> { <answer> (atomic_dec_and_test(&msg_tofree)) 
<token> (!oops_in_progress) <answer> if 
static void msg_free_recv(struct <token> *msg) <answer> ipmi_recv_msg 
if <token> { <answer> (atomic_dec_and_test(&msg_tofree)) 
if <token> <answer> (!oops_in_progress) 
static struct ipmi_smi_msg smi_msg <token> INIT_IPMI_SMI_MSG(msg_free_smi); <answer> = 
static struct <token> recv_msg = INIT_IPMI_RECV_MSG(msg_free_recv); <answer> ipmi_recv_msg 
static int __ipmi_set_timeout(struct ipmi_smi_msg <token> <answer> *smi_msg, 
<token> ipmi_recv_msg *recv_msg, <answer> struct 
<token> *send_heartbeat_now) <answer> int 
struct kernel_ipmi_msg <token> <answer> msg; 
<token> char data[6]; <answer> unsigned 
<token> rv; <answer> int 
<token> ipmi_system_interface_addr addr; <answer> struct 
<token> hbnow = 0; <answer> int 
<token> = 0; <answer> data[0] 
<token> WDOG_TIMER_USE_SMS_OS); <answer> WDOG_SET_TIMER_USE(data[0], 
if (ipmi_watchdog_state <token> WDOG_TIMEOUT_NONE) { <answer> != 
if ((ipmi_version_major <token> 1) || <answer> > 
((ipmi_version_major == 1) && (ipmi_version_minor <token> 5))) { <answer> >= 
hbnow <token> 1; <answer> = 
data[1] <token> 0; <answer> = 
<token> ipmi_watchdog_state); <answer> WDOG_SET_TIMEOUT_ACT(data[1], 
if ((pretimeout > 0) && (ipmi_watchdog_state != WDOG_TIMEOUT_NONE)) <token> <answer> { 
WDOG_SET_PRETIMEOUT_ACT(data[1], <token> <answer> preaction_val); 
<token> = pretimeout; <answer> data[2] 
} else <token> <answer> { 
WDOG_SET_PRETIMEOUT_ACT(data[1], <token> <answer> WDOG_PRETIMEOUT_NONE); 
if (ipmi_watchdog_state <token> WDOG_TIMEOUT_NONE) <answer> == 
addr.addr_type = <token> <answer> IPMI_SYSTEM_INTERFACE_ADDR_TYPE; 
<token> = IPMI_BMC_CHANNEL; <answer> addr.channel 
addr.lun <token> 0; <answer> = 
msg.netfn = <token> <answer> 0x06; 
<token> = IPMI_WDOG_RESET_TIMER; <answer> msg.cmd 
<token> = NULL; <answer> msg.data 
msg.data_len = <token> <answer> 0; 
<token> &panic_done_count); <answer> atomic_add(2, 
<token> = ipmi_request_supply_msgs(watchdog_user, <answer> rv 
(struct <token> *) &addr, <answer> ipmi_addr 
<token> (rv) <answer> if 
<token> &panic_done_count); <answer> atomic_sub(2, 
static <token> ipmi_smi_msg panic_halt_smi_msg = <answer> struct 
static struct <token> panic_halt_recv_msg = <answer> ipmi_recv_msg 
static <token> panic_halt_ipmi_set_timeout(void) <answer> void 
int <token> <answer> send_heartbeat_now; 
<token> rv; <answer> int 
<token> (ipmi_watchdog_state == WDOG_TIMEOUT_NONE) <answer> if 
return <token> <answer> 0; 
<token> 2); <answer> atomic_set(&msg_tofree, 
addr.addr_type = <token> <answer> IPMI_SYSTEM_INTERFACE_ADDR_TYPE; 
addr.channel <token> IPMI_BMC_CHANNEL; <answer> = 
<token> = 0; <answer> addr.lun 
msg.netfn <token> 0x06; <answer> = 
msg.cmd = <token> <answer> IPMI_WDOG_RESET_TIMER; 
<token> = NULL; <answer> msg.data 
<token> = 0; <answer> msg.data_len 
<token> = ipmi_request_supply_msgs(watchdog_user, <answer> rv 
<token> ipmi_addr *) &addr, <answer> (struct 
if (rv) <token> <answer> { 
<token> 0); <answer> atomic_set(&msg_tofree, 
pr_warn("heartbeat send <token> %d\n", rv); <answer> failure: 
return <token> <answer> rv; 
rv = <token> <answer> _ipmi_set_timeout(IPMI_SET_TIMEOUT_NO_HB); 
if (rv) <token> <answer> { 
pr_err("Unable to send the command to set the watchdog's settings, giving <token> <answer> up\n"); 
<token> out; <answer> goto 
rv <token> -EINVAL; <answer> = 
<token> rv; <answer> return 
<token> int _ipmi_heartbeat(void) <answer> static 
int <token> <answer> rv; 
if <token> <answer> (!watchdog_user) 
return <token> <answer> -ENODEV; 
if <token> { <answer> (ipmi_start_timer_on_heartbeat) 
<token> = 0; <answer> ipmi_start_timer_on_heartbeat 
<token> = action_val; <answer> ipmi_watchdog_state 
rv <token> _ipmi_set_timeout(IPMI_SET_TIMEOUT_FORCE_HB); <answer> = 
} <token> if (atomic_cmpxchg(&pretimeout_since_last_heartbeat, 1, 0)) { <answer> else 
<token> = _ipmi_set_timeout(IPMI_SET_TIMEOUT_HB_IF_NECESSARY); <answer> rv 
} else <token> <answer> { 
rv = <token> <answer> __ipmi_heartbeat(); 
return <token> <answer> rv; 
<token> int ipmi_heartbeat(void) <answer> static 
<token> rv; <answer> int 
rv = <token> <answer> _ipmi_heartbeat(); 
<token> rv; <answer> return 
static <token> struct watchdog_info ident = { <answer> const 
if <token> { <answer> (!data_to_read) 
<token> (file->f_flags & O_NONBLOCK) { <answer> if 
rv <token> -EAGAIN; <answer> = 
<token> out; <answer> goto 
<token> current); <answer> init_waitqueue_entry(&wait, 
<token> &wait); <answer> add_wait_queue(&read_q, 
while <token> && !signal_pending(current)) { <answer> (!data_to_read 
remove_wait_queue(&read_q, <token> <answer> &wait); 
<token> (signal_pending(current)) { <answer> if 
rv = <token> <answer> -ERESTARTSYS; 
goto <token> <answer> out; 
data_to_read = <token> <answer> 0; 
if (rv == 0) <token> <answer> { 
if <token> &data_to_read, 1)) <answer> (copy_to_user(buf, 
rv <token> -EFAULT; <answer> = 
rv = <token> <answer> 1; 
<token> rv; <answer> return 
static int ipmi_open(struct inode *ino, struct <token> *filep) <answer> file 
switch (iminor(ino)) <token> <answer> { 
case <token> <answer> WATCHDOG_MINOR: 
<token> (test_and_set_bit(0, &ipmi_wdog_open)) <answer> if 
<token> -EBUSY; <answer> return 
<token> = 1; <answer> ipmi_start_timer_on_heartbeat 
return stream_open(ino, <token> <answer> filep); 
return <token> <answer> (-ENODEV); 
static __poll_t ipmi_poll(struct <token> *file, poll_table *wait) <answer> file 
__poll_t <token> = 0; <answer> mask 
<token> &read_q, wait); <answer> poll_wait(file, 
if <token> <answer> (data_to_read) 
mask |= (EPOLLIN <token> EPOLLRDNORM); <answer> | 
return <token> <answer> mask; 
static int ipmi_fasync(int fd, <token> file *file, int on) <answer> struct 
int <token> <answer> result; 
result = fasync_helper(fd, file, on, <token> <answer> &fasync_q); 
return <token> <answer> (result); 
static int ipmi_close(struct <token> *ino, struct file *filep) <answer> inode 
if (iminor(ino) == WATCHDOG_MINOR) <token> <answer> { 
if (expect_close <token> 42) { <answer> == 
<token> = WDOG_TIMEOUT_NONE; <answer> ipmi_watchdog_state 
<token> else { <answer> } 
<token> close, not stopping watchdog!\n"); <answer> pr_crit("Unexpected 
clear_bit(0, <token> <answer> &ipmi_wdog_open); 
<token> = 0; <answer> expect_close 
return <token> <answer> 0; 
static const <token> file_operations ipmi_wdog_fops = { <answer> struct 
.owner = <token> <answer> THIS_MODULE, 
.read = <token> <answer> ipmi_read, 
.poll = <token> <answer> ipmi_poll, 
.write <token> ipmi_write, <answer> = 
.unlocked_ioctl = <token> <answer> ipmi_unlocked_ioctl, 
.compat_ioctl = <token> <answer> compat_ptr_ioctl, 
<token> = ipmi_open, <answer> .open 
<token> = ipmi_close, <answer> .release 
<token> = ipmi_fasync, <answer> .fasync 
.llseek = <token> <answer> no_llseek, 
<token> struct miscdevice ipmi_wdog_miscdev = { <answer> static 
.minor <token> WATCHDOG_MINOR, <answer> = 
<token> = "watchdog", <answer> .name 
<token> = &ipmi_wdog_fops <answer> .fops 
<token> void ipmi_wdog_msg_handler(struct ipmi_recv_msg *msg, <answer> static 
<token> *handler_data) <answer> void 
if (msg->msg.cmd == IPMI_WDOG_RESET_TIMER <token> <answer> && 
msg->msg.data[0] <token> IPMI_WDOG_TIMER_NOT_INIT_RESP) <answer> == 
pr_info("response: The <token> controller appears to have been reset, will attempt to reinitialize the watchdog timer\n"); <answer> IPMI 
else if <token> != 0) <answer> (msg->msg.data[0] 
pr_err("response: Error %x <token> cmd %x\n", <answer> on 
static void <token> *handler_data) <answer> ipmi_wdog_pretimeout_handler(void 
if (preaction_val <token> WDOG_PRETIMEOUT_NONE) { <answer> != 
if <token> == WDOG_PREOP_PANIC) { <answer> (preop_val 
<token> (atomic_inc_and_test(&preop_panic_excl)) <answer> if 
<token> pre-timeout"); <answer> panic("Watchdog 
} <token> if (preop_val == WDOG_PREOP_GIVE_DATA) { <answer> else 
unsigned <token> flags; <answer> long 
<token> flags); <answer> spin_lock_irqsave(&ipmi_read_lock, 
<token> = 1; <answer> data_to_read 
kill_fasync(&fasync_q, <token> POLL_IN); <answer> SIGIO, 
<token> flags); <answer> spin_unlock_irqrestore(&ipmi_read_lock, 
<token> 1); <answer> atomic_set(&pretimeout_since_last_heartbeat, 
static <token> ipmi_wdog_panic_handler(void *user_data) <answer> void 
<token> int panic_event_handled; <answer> static 
if <token> && !panic_event_handled && <answer> (watchdog_user 
ipmi_watchdog_state != <token> { <answer> WDOG_TIMEOUT_NONE) 
ipmi_watchdog_state = <token> <answer> WDOG_TIMEOUT_RESET; 
while <token> <answer> (atomic_read(&msg_tofree)) 
<token> (testing_nmi) { <answer> if 
testing_nmi <token> 2; <answer> = 
<token> NMI_HANDLED; <answer> return 
if (preop_val == <token> { <answer> WDOG_PREOP_PANIC) 
atomic_set(&pretimeout_since_last_heartbeat, <token> <answer> 1); 
if <token> <answer> (atomic_inc_and_test(&preop_panic_excl)) 
<token> "pre-timeout"); <answer> nmi_panic(regs, 
return <token> <answer> NMI_HANDLED; 
<token> int wdog_reboot_handler(struct notifier_block *this, <answer> static 
unsigned <token> code, <answer> long 
<token> *unused) <answer> void 
static int <token> <answer> reboot_event_handled; 
if ((watchdog_user) <token> (!reboot_event_handled)) { <answer> && 
if <token> < 120) <answer> (timeout 
timeout <token> 120; <answer> = 
pretimeout = <token> <answer> 0; 
ipmi_watchdog_state <token> WDOG_TIMEOUT_RESET; <answer> = 
return <token> <answer> NOTIFY_OK; 
static <token> notifier_block wdog_reboot_notifier = { <answer> struct 
.notifier_call = <token> <answer> wdog_reboot_handler, 
.next <token> NULL, <answer> = 
.priority <token> 0 <answer> = 
static void ipmi_new_smi(int if_num, struct device <token> <answer> *device) 
static void ipmi_smi_gone(int <token> <answer> if_num) 
static struct ipmi_smi_watcher <token> = { <answer> smi_watcher 
<token> = THIS_MODULE, <answer> .owner 
.new_smi <token> ipmi_new_smi, <answer> = 
.smi_gone <token> ipmi_smi_gone <answer> = 
static int action_op(const char *inval, <token> *outval) <answer> char 
if <token> <answer> (outval) 
<token> action); <answer> strcpy(outval, 
if <token> <answer> (!inval) 
<token> 0; <answer> return 
if (strcmp(inval, "reset") == <token> <answer> 0) 
action_val = <token> <answer> WDOG_TIMEOUT_RESET; 
else if (strcmp(inval, <token> == 0) <answer> "none") 
action_val <token> WDOG_TIMEOUT_NONE; <answer> = 
else if (strcmp(inval, "power_cycle") == <token> <answer> 0) 
<token> = WDOG_TIMEOUT_POWER_CYCLE; <answer> action_val 
else if <token> "power_off") == 0) <answer> (strcmp(inval, 
<token> = WDOG_TIMEOUT_POWER_DOWN; <answer> action_val 
return <token> <answer> -EINVAL; 
strcpy(action, <token> <answer> inval); 
<token> 0; <answer> return 
static <token> preaction_op(const char *inval, char *outval) <answer> int 
if <token> <answer> (outval) 
<token> preaction); <answer> strcpy(outval, 
<token> (!inval) <answer> if 
<token> 0; <answer> return 
if (strcmp(inval, "pre_none") == <token> <answer> 0) 
preaction_val = <token> <answer> WDOG_PRETIMEOUT_NONE; 
else <token> (strcmp(inval, "pre_smi") == 0) <answer> if 
preaction_val = <token> <answer> WDOG_PRETIMEOUT_SMI; 
<token> HAVE_DIE_NMI <answer> #ifdef 
else <token> (strcmp(inval, "pre_nmi") == 0) <answer> if 
preaction_val = <token> <answer> WDOG_PRETIMEOUT_NMI; 
else if (strcmp(inval, "pre_int") == <token> <answer> 0) 
preaction_val <token> WDOG_PRETIMEOUT_MSG_INT; <answer> = 
<token> -EINVAL; <answer> return 
strcpy(preaction, <token> <answer> inval); 
<token> 0; <answer> return 
static int preop_op(const char *inval, char <token> <answer> *outval) 
if <token> <answer> (outval) 
<token> preop); <answer> strcpy(outval, 
<token> (!inval) <answer> if 
return <token> <answer> 0; 
if (strcmp(inval, "preop_none") == <token> <answer> 0) 
<token> = WDOG_PREOP_NONE; <answer> preop_val 
else if <token> "preop_panic") == 0) <answer> (strcmp(inval, 
preop_val = <token> <answer> WDOG_PREOP_PANIC; 
else if (strcmp(inval, "preop_give_data") == <token> <answer> 0) 
preop_val = <token> <answer> WDOG_PREOP_GIVE_DATA; 
<token> -EINVAL; <answer> return 
<token> inval); <answer> strcpy(preop, 
<token> 0; <answer> return 
<token> void check_parms(void) <answer> static 
<token> HAVE_DIE_NMI <answer> #ifdef 
int <token> = 0; <answer> do_nmi 
int <token> <answer> rv; 
if (preaction_val == WDOG_PRETIMEOUT_NMI) <token> <answer> { 
do_nmi = <token> <answer> 1; 
if (preop_val <token> WDOG_PREOP_GIVE_DATA) { <answer> == 
pr_warn("Pretimeout op is to give data but NMI pretimeout is enabled, <token> pretimeout op to none\n"); <answer> setting 
preop_op("preop_none", <token> <answer> NULL); 
do_nmi = <token> <answer> 0; 
if (do_nmi <token> !nmi_handler_registered) { <answer> && 
<token> = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0, <answer> rv 
if (rv) <token> <answer> { 
pr_warn("Can't <token> nmi handler\n"); <answer> register 
} <token> <answer> else 
nmi_handler_registered <token> 1; <answer> = 
} else if (!do_nmi <token> nmi_handler_registered) { <answer> && 
unregister_nmi_handler(NMI_UNKNOWN, <token> <answer> "ipmi"); 
<token> = 0; <answer> nmi_handler_registered 
<token> int __init ipmi_wdog_init(void) <answer> static 
<token> rv; <answer> int 
if (action_op(action, <token> { <answer> NULL)) 
<token> NULL); <answer> action_op("reset", 
pr_info("Unknown action <token> defaulting to reset\n", action); <answer> '%s', 
if (preaction_op(preaction, NULL)) <token> <answer> { 
<token> NULL); <answer> preaction_op("pre_none", 
<token> preaction '%s', defaulting to none\n", <answer> pr_info("Unknown 
if (preop_op(preop, NULL)) <token> <answer> { 
<token> NULL); <answer> preop_op("preop_none", 
pr_info("Unknown preop <token> defaulting to none\n", preop); <answer> '%s', 
<token> = ipmi_smi_watcher_register(&smi_watcher); <answer> rv 
if (rv) <token> <answer> { 
<token> HAVE_DIE_NMI <answer> #ifdef 
<token> (nmi_handler_registered) <answer> if 
unregister_nmi_handler(NMI_UNKNOWN, <token> <answer> "ipmi"); 
pr_warn("can't register <token> watcher\n"); <answer> smi 
<token> rv; <answer> return 
pr_info("driver <token> <answer> initialized\n"); 
return <token> <answer> 0; 
static <token> __exit ipmi_wdog_exit(void) <answer> void 
<token> HAVE_DIE_NMI <answer> #ifdef 
if <token> <answer> (nmi_handler_registered) 
unregister_nmi_handler(NMI_UNKNOWN, <token> <answer> "ipmi"); 
MODULE_AUTHOR("Corey <token> <minyard@mvista.com>"); <answer> Minyard 
MODULE_DESCRIPTION("watchdog timer <token> upon the IPMI interface."); <answer> based 
#include <token> <answer> <linux/kernel.h> 
#include <token> <answer> <linux/errno.h> 
<token> <linux/fs.h> <answer> #include 
#include <token> <answer> <linux/file.h> 
#include <token> <answer> <linux/mm.h> 
#include <token> <answer> <linux/slab.h> 
#include <token> <answer> <linux/namei.h> 
<token> <linux/io_uring.h> <answer> #include 
#include <token> <answer> <linux/fsnotify.h> 
#include <token> <answer> <uapi/linux/io_uring.h> 
#include <token> <answer> "io_uring.h" 
<token> "sync.h" <answer> #include 
struct io_sync <token> <answer> { 
struct file <token> <answer> *file; 
loff_t <token> <answer> len; 
loff_t <token> <answer> off; 
<token> flags; <answer> int 
int <token> <answer> mode; 
int io_sfr_prep(struct io_kiocb <token> const struct io_uring_sqe *sqe) <answer> *req, 
struct io_sync *sync = io_kiocb_to_cmd(req, <token> io_sync); <answer> struct 
if <token> || sqe->buf_index || sqe->splice_fd_in)) <answer> (unlikely(sqe->addr 
<token> -EINVAL; <answer> return 
<token> = READ_ONCE(sqe->off); <answer> sync->off 
<token> = READ_ONCE(sqe->len); <answer> sync->len 
sync->flags <token> READ_ONCE(sqe->sync_range_flags); <answer> = 
<token> |= REQ_F_FORCE_ASYNC; <answer> req->flags 
<token> 0; <answer> return 
int io_sync_file_range(struct <token> *req, unsigned int issue_flags) <answer> io_kiocb 
struct io_sync *sync <token> io_kiocb_to_cmd(req, struct io_sync); <answer> = 
<token> ret; <answer> int 
<token> <stdio.h> <answer> #include 
#include <token> <answer> <unistd.h> 
<token> <string.h> <answer> #include 
#include <token> <answer> <assert.h> 
#include <token> <answer> <bpf/libbpf.h> 
#include <token> <answer> <bpf/bpf.h> 
#include <token> <answer> "trace_helpers.h" 
int <token> ac, char **argv) <answer> main(int 
struct <token> *obj = NULL; <answer> bpf_object 
struct bpf_link <token> <answer> *links[20]; 
<token> key, next_key, value; <answer> long 
<token> bpf_program *prog; <answer> struct 
int map_fd, <token> j = 0; <answer> i, 
<token> filename[256]; <answer> char 
struct ksym <token> <answer> *sym; 
if (load_kallsyms()) <token> <answer> { 
<token> to process /proc/kallsyms\n"); <answer> printf("failed 
<token> 2; <answer> return 
<token> sizeof(filename), "%s.bpf.o", argv[0]); <answer> snprintf(filename, 
<token> = bpf_object__open_file(filename, NULL); <answer> obj 
<token> (libbpf_get_error(obj)) { <answer> if 
fprintf(stderr, "ERROR: <token> BPF object file failed\n"); <answer> opening 
<token> = NULL; <answer> obj 
goto <token> <answer> cleanup; 
<token> <linux/clk.h> <answer> #include 
<token> <linux/io.h> <answer> #include 
#include <token> <answer> <linux/module.h> 
#include <token> <answer> <linux/of.h> 
#include <token> <answer> <linux/phy/phy.h> 
#include <token> <answer> <linux/platform_device.h> 
<token> <linux/spinlock.h> <answer> #include 
phy->phy = devm_phy_create(dev, NULL, <token> <answer> &r8a77980_phy_pcie_ops); 
<token> (IS_ERR(phy->phy)) { <answer> if 
dev_err(dev, "Failed to create <token> PHY\n"); <answer> PCIe 
<token> = PTR_ERR(phy->phy); <answer> error 
<token> error; <answer> goto 
phy_set_drvdata(phy->phy, <token> <answer> phy); 
<token> = devm_of_phy_provider_register(dev, of_phy_simple_xlate); <answer> provider 
if <token> { <answer> (IS_ERR(provider)) 
<token> "Failed to register PHY provider\n"); <answer> dev_err(dev, 
error = <token> <answer> PTR_ERR(provider); 
<token> error; <answer> goto 
<token> 0; <answer> return 
return <token> <answer> error; 
static void rcar_gen3_phy_pcie_remove(struct platform_device <token> <answer> *pdev) 
<token> struct platform_driver rcar_gen3_phy_driver = { <answer> static 
<token> = { <answer> .driver 
.name <token> "phy_rcar_gen3_pcie", <answer> = 
.of_match_table = <token> <answer> rcar_gen3_phy_pcie_match_table, 
<token> = rcar_gen3_phy_pcie_probe, <answer> .probe 
.remove_new = <token> <answer> rcar_gen3_phy_pcie_remove, 
MODULE_LICENSE("GPL <token> <answer> v2"); 
MODULE_DESCRIPTION("Renesas R-Car Gen3 PCIe <token> <answer> PHY"); 
MODULE_AUTHOR("Sergei Shtylyov <token> <answer> <sergei.shtylyov@cogentembedded.com>"); 
<token> <linux/slab.h> <answer> #include 
#include <token> <answer> <linux/crc32.h> 
#include <token> <answer> <linux/freezer.h> 
<token> <linux/kthread.h> <answer> #include 
#include <token> <answer> "ubi.h" 
<token> "wl.h" <answer> #include 
#define UBI_WL_THRESHOLD <token> <answer> CONFIG_MTD_UBI_WL_THRESHOLD 
#define WL_FREE_MAX_DIFF <token> <answer> (2*UBI_WL_THRESHOLD) 
<token> WL_MAX_FAILURES 32 <answer> #define 
static int self_check_ec(struct ubi_device *ubi, int pnum, int <token> <answer> ec); 
static int self_check_in_wl_tree(const <token> ubi_device *ubi, <answer> struct 
struct ubi_wl_entry *e, struct rb_root <token> <answer> *root); 
<token> int self_check_in_pq(const struct ubi_device *ubi, <answer> static 
<token> ubi_wl_entry *e); <answer> struct 
<token> void wl_tree_add(struct ubi_wl_entry *e, struct rb_root *root) <answer> static 
struct rb_node **p, <token> = NULL; <answer> *parent 
<token> = &root->rb_node; <answer> p 
while <token> { <answer> (*p) 
struct <token> *e1; <answer> ubi_wl_entry 
parent = <token> <answer> *p; 
e1 = rb_entry(parent, <token> ubi_wl_entry, u.rb); <answer> struct 
if (e->ec <token> e1->ec) <answer> < 
p <token> &(*p)->rb_left; <answer> = 
else if (e->ec > <token> <answer> e1->ec) 
p = <token> <answer> &(*p)->rb_right; 
else <token> <answer> { 
ubi_assert(e->pnum != <token> <answer> e1->pnum); 
if (e->pnum <token> e1->pnum) <answer> < 
p <token> &(*p)->rb_left; <answer> = 
p = <token> <answer> &(*p)->rb_right; 
<token> parent, p); <answer> rb_link_node(&e->u.rb, 
rb_insert_color(&e->u.rb, <token> <answer> root); 
static void wl_entry_destroy(struct <token> *ubi, struct ubi_wl_entry *e) <answer> ubi_device 
ubi->lookuptbl[e->pnum] = <token> <answer> NULL; 
kmem_cache_free(ubi_wl_entry_slab, <token> <answer> e); 
static int <token> ubi_device *ubi, int *executed) <answer> do_work(struct 
int <token> <answer> err; 
struct <token> *wrk; <answer> ubi_work 
if <token> { <answer> (list_empty(&ubi->works)) 
if <token> <answer> (executed) 
*executed <token> 0; <answer> = 
return <token> <answer> 0; 
<token> (executed) <answer> if 
<token> = 1; <answer> *executed 
wrk = list_entry(ubi->works.next, struct ubi_work, <token> <answer> list); 
<token> -= 1; <answer> ubi->works_count 
<token> >= 0); <answer> ubi_assert(ubi->works_count 
<token> = wrk->func(ubi, wrk, 0); <answer> err 
if <token> <answer> (err) 
ubi_err(ubi, "work failed with error <token> %d", err); <answer> code 
<token> err; <answer> return 
static int in_wl_tree(struct ubi_wl_entry *e, struct rb_root <token> <answer> *root) 
struct <token> *p; <answer> rb_node 
p = <token> <answer> root->rb_node; 
<token> (p) { <answer> while 
struct <token> *e1; <answer> ubi_wl_entry 
e1 = rb_entry(p, <token> ubi_wl_entry, u.rb); <answer> struct 
if (e->pnum <token> e1->pnum) { <answer> == 
ubi_assert(e <token> e1); <answer> == 
return <token> <answer> 1; 
<token> (e->ec < e1->ec) <answer> if 
p <token> p->rb_left; <answer> = 
else if (e->ec <token> e1->ec) <answer> > 
p = <token> <answer> p->rb_right; 
else <token> <answer> { 
<token> != e1->pnum); <answer> ubi_assert(e->pnum 
<token> (e->pnum < e1->pnum) <answer> if 
p <token> p->rb_left; <answer> = 
p = <token> <answer> p->rb_right; 
<token> 0; <answer> return 
static inline int in_pq(const struct ubi_device *ubi, struct ubi_wl_entry <token> <answer> *e) 
<token> ubi_wl_entry *p; <answer> struct 
<token> i; <answer> int 
for (i = 0; i < <token> ++i) <answer> UBI_PROT_QUEUE_LEN; 
<token> &ubi->pq[i], u.list) <answer> list_for_each_entry(p, 
if <token> == e) <answer> (p 
<token> 1; <answer> return 
return <token> <answer> 0; 
static <token> prot_queue_add(struct ubi_device *ubi, struct ubi_wl_entry *e) <answer> void 
int pq_tail = ubi->pq_head - <token> <answer> 1; 
if (pq_tail <token> 0) <answer> < 
<token> = UBI_PROT_QUEUE_LEN - 1; <answer> pq_tail 
ubi_assert(pq_tail >= 0 && pq_tail < <token> <answer> UBI_PROT_QUEUE_LEN); 
<token> &ubi->pq[pq_tail]); <answer> list_add_tail(&e->u.list, 
dbg_wl("added PEB %d EC %d to the protection queue", e->pnum, <token> <answer> e->ec); 
static struct ubi_wl_entry *find_wl_entry(struct <token> *ubi, <answer> ubi_device 
struct rb_root *root, int <token> <answer> diff, 
int <token> <answer> pick_max) 
struct rb_node <token> <answer> *p; 
struct ubi_wl_entry <token> <answer> *e; 
int <token> <answer> max; 
e = rb_entry(rb_first(root), struct <token> u.rb); <answer> ubi_wl_entry, 
max = e->ec <token> diff; <answer> + 
<token> = root->rb_node; <answer> p 
<token> (p) { <answer> while 
struct <token> *e1; <answer> ubi_wl_entry 
e1 <token> rb_entry(p, struct ubi_wl_entry, u.rb); <answer> = 
if (e1->ec <token> max) { <answer> >= 
<token> (pick_max) <answer> if 
e = <token> <answer> e1; 
p = <token> <answer> p->rb_left; 
<token> else { <answer> } 
p = <token> <answer> p->rb_right; 
<token> = e1; <answer> e 
return <token> <answer> e; 
static struct <token> *find_mean_wl_entry(struct ubi_device *ubi, <answer> ubi_wl_entry 
struct rb_root <token> <answer> *root) 
struct ubi_wl_entry *e, *first, <token> <answer> *last; 
<token> = rb_entry(rb_first(root), struct ubi_wl_entry, u.rb); <answer> first 
<token> = rb_entry(rb_last(root), struct ubi_wl_entry, u.rb); <answer> last 
if (last->ec - <token> < WL_FREE_MAX_DIFF) { <answer> first->ec 
<token> = rb_entry(root->rb_node, struct ubi_wl_entry, u.rb); <answer> e 
e = may_reserve_for_fm(ubi, e, <token> <answer> root); 
<token> else <answer> } 
e = find_wl_entry(ubi, root, <token> 0); <answer> WL_FREE_MAX_DIFF/2, 
<token> e; <answer> return 
static <token> ubi_wl_entry *wl_get_wle(struct ubi_device *ubi) <answer> struct 
struct <token> *e; <answer> ubi_wl_entry 
e <token> find_mean_wl_entry(ubi, &ubi->free); <answer> = 
if <token> { <answer> (!e) 
<token> "no free eraseblocks"); <answer> ubi_err(ubi, 
return <token> <answer> NULL; 
<token> e, &ubi->free); <answer> self_check_in_wl_tree(ubi, 
rb_erase(&e->u.rb, <token> <answer> &ubi->free); 
<token> %d EC %d", e->pnum, e->ec); <answer> dbg_wl("PEB 
<token> e; <answer> return 
static int prot_queue_del(struct ubi_device <token> int pnum) <answer> *ubi, 
<token> ubi_wl_entry *e; <answer> struct 
<token> = ubi->lookuptbl[pnum]; <answer> e 
if <token> <answer> (!e) 
<token> -ENODEV; <answer> return 
if (self_check_in_pq(ubi, <token> <answer> e)) 
<token> -ENODEV; <answer> return 
<token> PEB %d from the protection queue", e->pnum); <answer> dbg_wl("deleted 
return <token> <answer> 0; 
int ubi_sync_erase(struct ubi_device *ubi, struct <token> *e, int torture) <answer> ubi_wl_entry 
int <token> <answer> err; 
struct <token> *ec_hdr; <answer> ubi_ec_hdr 
<token> long long ec = e->ec; <answer> unsigned 
dbg_wl("erase PEB %d, <token> EC %llu", e->pnum, ec); <answer> old 
<token> = self_check_ec(ubi, e->pnum, e->ec); <answer> err 
if <token> <answer> (err) 
return <token> <answer> -EINVAL; 
ec_hdr = kzalloc(ubi->ec_hdr_alsize, <token> <answer> GFP_NOFS); 
if <token> <answer> (!ec_hdr) 
return <token> <answer> -ENOMEM; 
err = ubi_io_sync_erase(ubi, <token> torture); <answer> e->pnum, 
<token> (err < 0) <answer> if 
goto <token> <answer> out_free; 
ec <token> err; <answer> += 
if (ec <token> UBI_MAX_ERASECOUNTER) { <answer> > 
ubi_err(ubi, "erase counter overflow at PEB <token> EC %llu", <answer> %d, 
e->pnum, <token> <answer> ec); 
err <token> -EINVAL; <answer> = 
goto <token> <answer> out_free; 
dbg_wl("erased <token> %d, new EC %llu", e->pnum, ec); <answer> PEB 
<token> = cpu_to_be64(ec); <answer> ec_hdr->ec 
err <token> ubi_io_write_ec_hdr(ubi, e->pnum, ec_hdr); <answer> = 
if <token> <answer> (err) 
goto <token> <answer> out_free; 
<token> = ec; <answer> e->ec 
if <token> > ubi->max_ec) <answer> (e->ec 
ubi->max_ec = <token> <answer> e->ec; 
return <token> <answer> err; 
static void <token> ubi_device *ubi) <answer> serve_prot_queue(struct 
<token> ubi_wl_entry *e, *tmp; <answer> struct 
int <token> <answer> count; 
<token> = 0; <answer> count 
list_for_each_entry_safe(e, tmp, &ubi->pq[ubi->pq_head], u.list) <token> <answer> { 
dbg_wl("PEB %d EC %d protection over, move to used <token> <answer> tree", 
e->pnum, <token> <answer> e->ec); 
<token> &ubi->used); <answer> wl_tree_add(e, 
if (count++ > 32) <token> <answer> { 
goto <token> <answer> repeat; 
<token> += 1; <answer> ubi->pq_head 
if (ubi->pq_head <token> UBI_PROT_QUEUE_LEN) <answer> == 
ubi->pq_head = <token> <answer> 0; 
ubi_assert(ubi->pq_head >= 0 <token> ubi->pq_head < UBI_PROT_QUEUE_LEN); <answer> && 
static void <token> ubi_device *ubi, struct ubi_work *wrk) <answer> __schedule_ubi_work(struct 
<token> &ubi->works); <answer> list_add_tail(&wrk->list, 
ubi_assert(ubi->works_count <token> 0); <answer> >= 
ubi->works_count <token> 1; <answer> += 
<token> (ubi->thread_enabled && !ubi_dbg_is_bgt_disabled(ubi)) <answer> if 
static void <token> ubi_device *ubi, struct ubi_work *wrk) <answer> schedule_ubi_work(struct 
__schedule_ubi_work(ubi, <token> <answer> wrk); 
static int erase_worker(struct ubi_device *ubi, struct ubi_work <token> <answer> *wl_wrk, 
<token> shutdown); <answer> int 
<token> int schedule_erase(struct ubi_device *ubi, struct ubi_wl_entry *e, <answer> static 
int <token> int lnum, int torture, bool nested) <answer> vol_id, 
<token> ubi_work *wl_wrk; <answer> struct 
dbg_wl("schedule erasure of <token> %d, EC %d, torture %d", <answer> PEB 
e->pnum, <token> torture); <answer> e->ec, 
wl_wrk = kmalloc(sizeof(struct ubi_work), <token> <answer> GFP_NOFS); 
<token> (!wl_wrk) <answer> if 
<token> -ENOMEM; <answer> return 
<token> = &erase_worker; <answer> wl_wrk->func 
<token> = e; <answer> wl_wrk->e 
wl_wrk->vol_id <token> vol_id; <answer> = 
wl_wrk->lnum = <token> <answer> lnum; 
wl_wrk->torture <token> torture; <answer> = 
if <token> <answer> (nested) 
<token> wl_wrk); <answer> __schedule_ubi_work(ubi, 
<token> wl_wrk); <answer> schedule_ubi_work(ubi, 
<token> 0; <answer> return 
static int __erase_worker(struct ubi_device *ubi, struct <token> *wl_wrk); <answer> ubi_work 
static int do_sync_erase(struct ubi_device *ubi, struct <token> *e, <answer> ubi_wl_entry 
<token> vol_id, int lnum, int torture) <answer> int 
struct <token> wl_wrk; <answer> ubi_work 
dbg_wl("sync erase of PEB %i", <token> <answer> e->pnum); 
wl_wrk.e = <token> <answer> e; 
<token> = vol_id; <answer> wl_wrk.vol_id 
wl_wrk.lnum = <token> <answer> lnum; 
wl_wrk.torture <token> torture; <answer> = 
<token> __erase_worker(ubi, &wl_wrk); <answer> return 
static int ensure_wear_leveling(struct ubi_device <token> int nested); <answer> *ubi, 
static int wear_leveling_worker(struct ubi_device *ubi, <token> ubi_work *wrk, <answer> struct 
int <token> <answer> shutdown) 
int err, scrubbing = 0, torture = 0, protect <token> 0, erroneous = 0; <answer> = 
int erase = 0, keep = 0, vol_id <token> -1, lnum = -1; <answer> = 
struct ubi_wl_entry <token> *e2; <answer> *e1, 
struct <token> *vidb; <answer> ubi_vid_io_buf 
<token> ubi_vid_hdr *vid_hdr; <answer> struct 
int dst_leb_clean <token> 0; <answer> = 
if <token> <answer> (shutdown) 
return <token> <answer> 0; 
<token> = ubi_alloc_vid_buf(ubi, GFP_NOFS); <answer> vidb 
<token> (!vidb) <answer> if 
<token> -ENOMEM; <answer> return 
<token> = ubi_get_vid_hdr(vidb); <answer> vid_hdr 
ubi_assert(!ubi->move_from && <token> <answer> !ubi->move_to); 
<token> CONFIG_MTD_UBI_FASTMAP <answer> #ifdef 
if (!next_peb_for_wl(ubi) <token> <answer> || 
if (!ubi->free.rb_node <token> <answer> || 
<token> && !ubi->scrub.rb_node)) { <answer> (!ubi->used.rb_node 
dbg_wl("cancel WL, a list is <token> free %d, used %d", <answer> empty: 
<token> !ubi->used.rb_node); <answer> !ubi->free.rb_node, 
<token> out_cancel; <answer> goto 
<token> CONFIG_MTD_UBI_FASTMAP <answer> #ifdef 
<token> = find_anchor_wl_entry(&ubi->used); <answer> e1 
if (e1 && ubi->fm_anchor <token> <answer> && 
<token> - e1->ec >= UBI_WL_THRESHOLD)) { <answer> (ubi->fm_anchor->ec 
ubi->fm_do_produce_anchor <token> 1; <answer> = 
wl_tree_add(ubi->fm_anchor, <token> <answer> &ubi->free); 
<token> = NULL; <answer> ubi->fm_anchor 
if <token> { <answer> (ubi->fm_do_produce_anchor) 
<token> (!e1) <answer> if 
goto <token> <answer> out_cancel; 
e2 <token> get_peb_for_wl(ubi); <answer> = 
if <token> <answer> (!e2) 
<token> out_cancel; <answer> goto 
<token> e1, &ubi->used); <answer> self_check_in_wl_tree(ubi, 
rb_erase(&e1->u.rb, <token> <answer> &ubi->used); 
<token> PEB %d to PEB %d", e1->pnum, e2->pnum); <answer> dbg_wl("anchor-move 
ubi->fm_do_produce_anchor = <token> <answer> 0; 
<token> else if (!ubi->scrub.rb_node) { <answer> } 
if <token> { <answer> (!ubi->scrub.rb_node) 
<token> = rb_entry(rb_first(&ubi->used), struct ubi_wl_entry, u.rb); <answer> e1 
e2 = <token> <answer> get_peb_for_wl(ubi); 
if <token> <answer> (!e2) 
<token> out_cancel; <answer> goto 
if (!(e2->ec - e1->ec <token> UBI_WL_THRESHOLD)) { <answer> >= 
<token> WL needed: min used EC %d, max free EC %d", <answer> dbg_wl("no 
<token> e2->ec); <answer> e1->ec, 
<token> = ubi_io_read_vid_hdr(ubi, e1->pnum, vidb, 0); <answer> err 
<token> (err && err != UBI_IO_BITFLIPS) { <answer> if 
dst_leb_clean <token> 1; <answer> = 
<token> (err == UBI_IO_FF) { <answer> if 
dbg_wl("PEB %d <token> no VID header", e1->pnum); <answer> has 
<token> = 1; <answer> protect 
goto <token> <answer> out_not_moved; 
} else <token> (err == UBI_IO_FF_BITFLIPS) { <answer> if 
dbg_wl("PEB %d has <token> VID header but has bit-flips", <answer> no 
scrubbing = <token> <answer> 1; 
<token> out_not_moved; <answer> goto 
} else if <token> && err == UBI_IO_BAD_HDR_EBADMSG) { <answer> (ubi->fast_attach 
dbg_wl("PEB %d has ECC errors, maybe from <token> interrupted erasure", <answer> an 
<token> = 1; <answer> erase 
<token> out_not_moved; <answer> goto 
ubi_err(ubi, "error %d while reading VID header from PEB <token> <answer> %d", 
<token> e1->pnum); <answer> err, 
goto <token> <answer> out_error; 
vol_id <token> be32_to_cpu(vid_hdr->vol_id); <answer> = 
<token> = be32_to_cpu(vid_hdr->lnum); <answer> lnum 
err = ubi_eba_copy_leb(ubi, e1->pnum, e2->pnum, <token> <answer> vidb); 
<token> (err) { <answer> if 
if (err <token> MOVE_CANCEL_RACE) { <answer> == 
protect <token> 1; <answer> = 
dst_leb_clean = <token> <answer> 1; 
<token> out_not_moved; <answer> goto 
if <token> == MOVE_RETRY) { <answer> (err 
<token> = 1; <answer> scrubbing 
<token> = 1; <answer> dst_leb_clean 
goto <token> <answer> out_not_moved; 
if (err == MOVE_TARGET_BITFLIPS || err == <token> || <answer> MOVE_TARGET_WR_ERR 
<token> == MOVE_TARGET_RD_ERR) { <answer> err 
torture <token> 1; <answer> = 
<token> = 1; <answer> keep 
goto <token> <answer> out_not_moved; 
if (err == MOVE_SOURCE_RD_ERR) <token> <answer> { 
if (ubi->erroneous_peb_count > <token> { <answer> ubi->max_erroneous) 
ubi_err(ubi, "too many erroneous eraseblocks <token> <answer> (%d)", 
goto <token> <answer> out_error; 
dst_leb_clean <token> 1; <answer> = 
<token> = 1; <answer> erroneous 
goto <token> <answer> out_not_moved; 
<token> (err < 0) <answer> if 
<token> out_error; <answer> goto 
dbg_wl("PEB %d (LEB %d:%d) was put meanwhile, <token> <answer> erase", 
e2->pnum, <token> lnum); <answer> vol_id, 
err = do_sync_erase(ubi, <token> vol_id, lnum, 0); <answer> e2, 
if <token> <answer> (err) 
<token> out_ro; <answer> goto 
<token> 0; <answer> return 
if (vol_id != <token> <answer> -1) 
dbg_wl("cancel moving PEB %d (LEB %d:%d) to <token> %d (%d)", <answer> PEB 
e1->pnum, vol_id, lnum, <token> err); <answer> e2->pnum, 
dbg_wl("cancel moving PEB %d to PEB <token> (%d)", <answer> %d 
<token> e2->pnum, err); <answer> e1->pnum, 
<token> (protect) <answer> if 
prot_queue_add(ubi, <token> <answer> e1); 
else if (erroneous) <token> <answer> { 
wl_tree_add(e1, <token> <answer> &ubi->erroneous); 
ubi->erroneous_peb_count += <token> <answer> 1; 
} <token> if (scrubbing) <answer> else 
wl_tree_add(e1, <token> <answer> &ubi->scrub); 
<token> if (keep) <answer> else 
wl_tree_add(e1, <token> <answer> &ubi->used); 
if (dst_leb_clean) <token> <answer> { 
wl_tree_add(e2, <token> <answer> &ubi->free); 
ubi->move_from = <token> = NULL; <answer> ubi->move_to 
<token> = 0; <answer> ubi->wl_scheduled 
<token> (dst_leb_clean) { <answer> if 
ensure_wear_leveling(ubi, <token> <answer> 1); 
<token> else { <answer> } 
err = do_sync_erase(ubi, e2, vol_id, <token> torture); <answer> lnum, 
if <token> <answer> (err) 
<token> out_ro; <answer> goto 
if (erase) <token> <answer> { 
<token> = do_sync_erase(ubi, e1, vol_id, lnum, 1); <answer> err 
if <token> <answer> (err) 
<token> out_ro; <answer> goto 
<token> 0; <answer> return 
if (vol_id <token> -1) <answer> != 
ubi_err(ubi, "error %d while moving <token> %d to PEB %d", <answer> PEB 
err, e1->pnum, <token> <answer> e2->pnum); 
ubi_err(ubi, "error %d while moving PEB %d (LEB %d:%d) <token> PEB %d", <answer> to 
err, e1->pnum, vol_id, <token> e2->pnum); <answer> lnum, 
<token> = ubi->move_to = NULL; <answer> ubi->move_from 
<token> = ubi->wl_scheduled = 0; <answer> ubi->move_to_put 
<token> e1); <answer> wl_entry_destroy(ubi, 
wl_entry_destroy(ubi, <token> <answer> e2); 
ubi_assert(err <token> 0); <answer> != 
return err <token> 0 ? err : -EIO; <answer> < 
ubi->wl_scheduled = <token> <answer> 0; 
return <token> <answer> 0; 
static int ensure_wear_leveling(struct ubi_device *ubi, <token> nested) <answer> int 
int <token> = 0; <answer> err 
struct ubi_work <token> <answer> *wrk; 
if <token> <answer> (ubi->wl_scheduled) 
<token> (!ubi->scrub.rb_node) { <answer> if 
<token> CONFIG_MTD_UBI_FASTMAP <answer> #ifdef 
<token> (!need_wear_leveling(ubi)) <answer> if 
goto <token> <answer> out_unlock; 
struct ubi_wl_entry <token> <answer> *e1; 
<token> ubi_wl_entry *e2; <answer> struct 
if <token> || !ubi->free.rb_node) <answer> (!ubi->used.rb_node 
e1 = rb_entry(rb_first(&ubi->used), struct ubi_wl_entry, <token> <answer> u.rb); 
<token> = find_wl_entry(ubi, &ubi->free, WL_FREE_MAX_DIFF, 0); <answer> e2 
if (!(e2->ec <token> e1->ec >= UBI_WL_THRESHOLD)) <answer> - 
<token> out_unlock; <answer> goto 
<token> wear-leveling"); <answer> dbg_wl("schedule 
} <token> <answer> else 
dbg_wl("schedule <token> <answer> scrubbing"); 
ubi->wl_scheduled = <token> <answer> 1; 
wrk = kmalloc(sizeof(struct ubi_work), <token> <answer> GFP_NOFS); 
<token> (!wrk) { <answer> if 
err = <token> <answer> -ENOMEM; 
goto <token> <answer> out_cancel; 
wrk->func <token> &wear_leveling_worker; <answer> = 
<token> (nested) <answer> if 
__schedule_ubi_work(ubi, <token> <answer> wrk); 
<token> wrk); <answer> schedule_ubi_work(ubi, 
return <token> <answer> err; 
ubi->wl_scheduled = <token> <answer> 0; 
return <token> <answer> err; 
static int <token> ubi_device *ubi, struct ubi_work *wl_wrk) <answer> __erase_worker(struct 
<token> ubi_wl_entry *e = wl_wrk->e; <answer> struct 
int pnum <token> e->pnum; <answer> = 
<token> vol_id = wl_wrk->vol_id; <answer> int 
int lnum <token> wl_wrk->lnum; <answer> = 
int err, <token> = 0; <answer> available_consumed 
dbg_wl("erase PEB %d EC <token> LEB %d:%d", <answer> %d 
pnum, e->ec, <token> wl_wrk->lnum); <answer> wl_wrk->vol_id, 
err = ubi_sync_erase(ubi, e, <token> <answer> wl_wrk->torture); 
if <token> { <answer> (!err) 
if (!ubi->fm_disabled && <token> && <answer> !ubi->fm_anchor 
e->pnum < <token> { <answer> UBI_FM_MAX_START) 
ubi->fm_anchor <token> e; <answer> = 
ubi->fm_do_produce_anchor = <token> <answer> 0; 
} <token> { <answer> else 
<token> &ubi->free); <answer> wl_tree_add(e, 
<token> out_ro; <answer> goto 
ubi->avail_pebs += <token> <answer> 1; 
available_consumed = <token> <answer> 0; 
ubi->beb_rsvd_pebs -= <token> <answer> 1; 
ubi->bad_peb_count <token> 1; <answer> += 
ubi->good_peb_count <token> 1; <answer> -= 
if <token> <answer> (available_consumed) 
ubi_warn(ubi, "no <token> in the reserved pool, used an available PEB"); <answer> PEBs 
<token> if (ubi->beb_rsvd_pebs) <answer> else 
ubi_msg(ubi, "%d PEBs left <token> the reserve", <answer> in 
ubi_warn(ubi, "last <token> from the reserve was used"); <answer> PEB 
return <token> <answer> err; 
if (available_consumed) <token> <answer> { 
ubi->avail_pebs += <token> <answer> 1; 
<token> err; <answer> return 
static <token> erase_worker(struct ubi_device *ubi, struct ubi_work *wl_wrk, <answer> int 
int <token> <answer> shutdown) 
<token> ret; <answer> int 
if <token> { <answer> (shutdown) 
struct ubi_wl_entry *e = <token> <answer> wl_wrk->e; 
dbg_wl("cancel erasure of PEB %d EC %d", e->pnum, <token> <answer> e->ec); 
<token> e); <answer> wl_entry_destroy(ubi, 
<token> 0; <answer> return 
ret <token> __erase_worker(ubi, wl_wrk); <answer> = 
<token> ret; <answer> return 
int ubi_wl_put_peb(struct ubi_device *ubi, <token> vol_id, int lnum, <answer> int 
int pnum, <token> torture) <answer> int 
<token> err; <answer> int 
struct <token> *e; <answer> ubi_wl_entry 
dbg_wl("PEB <token> pnum); <answer> %d", 
ubi_assert(pnum <token> 0); <answer> >= 
<token> < ubi->peb_count); <answer> ubi_assert(pnum 
e = <token> <answer> ubi->lookuptbl[pnum]; 
<token> (!e) { <answer> if 
return <token> <answer> 0; 
if <token> == ubi->move_from) { <answer> (e 
dbg_wl("PEB %d is being moved, <token> pnum); <answer> wait", 
dbg_wl("PEB %d is the target of data moving", <token> <answer> pnum); 
<token> = 1; <answer> ubi->move_to_put 
return <token> <answer> 0; 
<token> else { <answer> } 
if (in_wl_tree(e, &ubi->used)) <token> <answer> { 
self_check_in_wl_tree(ubi, <token> &ubi->used); <answer> e, 
<token> &ubi->used); <answer> rb_erase(&e->u.rb, 
<token> else if (in_wl_tree(e, &ubi->scrub)) { <answer> } 
<token> e, &ubi->scrub); <answer> self_check_in_wl_tree(ubi, 
rb_erase(&e->u.rb, <token> <answer> &ubi->scrub); 
} else if <token> &ubi->erroneous)) { <answer> (in_wl_tree(e, 
<token> e, &ubi->erroneous); <answer> self_check_in_wl_tree(ubi, 
<token> &ubi->erroneous); <answer> rb_erase(&e->u.rb, 
ubi->erroneous_peb_count -= <token> <answer> 1; 
ubi_assert(ubi->erroneous_peb_count >= <token> <answer> 0); 
int ubi_wl_scrub_peb(struct ubi_device *ubi, int <token> <answer> pnum) 
struct <token> *e; <answer> ubi_wl_entry 
ubi_msg(ubi, "schedule PEB %d for scrubbing", <token> <answer> pnum); 
e = <token> <answer> ubi->lookuptbl[pnum]; 
if (e == <token> || in_wl_tree(e, &ubi->scrub) || <answer> ubi->move_from 
in_wl_tree(e, &ubi->erroneous)) <token> <answer> { 
return <token> <answer> 0; 
<token> (e == ubi->move_to) { <answer> if 
dbg_wl("the PEB <token> is not in proper tree, retry", pnum); <answer> %d 
<token> retry; <answer> goto 
if (in_wl_tree(e, &ubi->used)) <token> <answer> { 
self_check_in_wl_tree(ubi, <token> &ubi->used); <answer> e, 
rb_erase(&e->u.rb, <token> <answer> &ubi->used); 
} <token> { <answer> else 
int <token> <answer> err; 
err = <token> e->pnum); <answer> prot_queue_del(ubi, 
if (err) <token> <answer> { 
ubi_err(ubi, "PEB %d <token> found", pnum); <answer> not 
return <token> <answer> err; 
<token> &ubi->scrub); <answer> wl_tree_add(e, 
return ensure_wear_leveling(ubi, <token> <answer> 0); 
int ubi_wl_flush(struct ubi_device *ubi, int vol_id, int <token> <answer> lnum) 
int err = <token> <answer> 0; 
int found = <token> <answer> 1; 
dbg_wl("flush <token> work for LEB %d:%d (%d pending works)", <answer> pending 
<token> lnum, ubi->works_count); <answer> vol_id, 
<token> (found) { <answer> while 
<token> ubi_work *wrk, *tmp; <answer> struct 
<token> = 0; <answer> found 
list_for_each_entry_safe(wrk, tmp, <token> list) { <answer> &ubi->works, 
if ((vol_id == <token> || wrk->vol_id == vol_id) && <answer> UBI_ALL 
(lnum <token> UBI_ALL || wrk->lnum == lnum)) { <answer> == 
ubi->works_count <token> 1; <answer> -= 
ubi_assert(ubi->works_count <token> 0); <answer> >= 
<token> = wrk->func(ubi, wrk, 0); <answer> err 
<token> (err) { <answer> if 
return <token> <answer> err; 
found = <token> <answer> 1; 
<token> err; <answer> return 
static bool scrub_possible(struct ubi_device <token> struct ubi_wl_entry *e) <answer> *ubi, 
if (in_wl_tree(e, <token> <answer> &ubi->scrub)) 
<token> false; <answer> return 
else <token> (in_wl_tree(e, &ubi->erroneous)) <answer> if 
<token> false; <answer> return 
else if <token> == e) <answer> (ubi->move_from 
<token> false; <answer> return 
else if <token> == e) <answer> (ubi->move_to 
return <token> <answer> false; 
return <token> <answer> true; 
int ubi_bitflip_check(struct ubi_device *ubi, int pnum, int <token> <answer> force) 
int <token> = 0; <answer> err 
struct <token> *e; <answer> ubi_wl_entry 
if (pnum < 0 || pnum <token> ubi->peb_count) { <answer> >= 
<token> = -EINVAL; <answer> err 
<token> out; <answer> goto 
e = <token> <answer> ubi->lookuptbl[pnum]; 
<token> (!e) { <answer> if 
<token> = -ENOENT; <answer> err 
<token> out_resume; <answer> goto 
<token> (!scrub_possible(ubi, e)) { <answer> if 
err = <token> <answer> -EBUSY; 
<token> out_resume; <answer> goto 
<token> (!force) { <answer> if 
err = ubi_io_read(ubi, ubi->peb_buf, pnum, 0, <token> <answer> ubi->peb_size); 
if <token> || err == UBI_IO_BITFLIPS) { <answer> (force 
e <token> ubi->lookuptbl[pnum]; <answer> = 
if <token> { <answer> (!e) 
err = <token> <answer> -ENOENT; 
<token> out_resume; <answer> goto 
if (!scrub_possible(ubi, <token> { <answer> e)) 
<token> = -EBUSY; <answer> err 
goto <token> <answer> out_resume; 
if (in_pq(ubi, e)) <token> <answer> { 
<token> e->pnum); <answer> prot_queue_del(ubi, 
wl_tree_add(e, <token> <answer> &ubi->scrub); 
<token> = ensure_wear_leveling(ubi, 1); <answer> err 
} <token> if (in_wl_tree(e, &ubi->used)) { <answer> else 
rb_erase(&e->u.rb, <token> <answer> &ubi->used); 
<token> &ubi->scrub); <answer> wl_tree_add(e, 
err = <token> 1); <answer> ensure_wear_leveling(ubi, 
} else <token> (in_wl_tree(e, &ubi->free)) { <answer> if 
<token> &ubi->free); <answer> rb_erase(&e->u.rb, 
err = schedule_erase(ubi, <token> UBI_UNKNOWN, UBI_UNKNOWN, <answer> e, 
force ? 0 : 1, <token> <answer> true); 
<token> else { <answer> } 
err <token> -EAGAIN; <answer> = 
if (!err <token> !force) <answer> && 
err <token> -EUCLEAN; <answer> = 
} <token> { <answer> else 
err <token> 0; <answer> = 
return <token> <answer> err; 
<token> void tree_destroy(struct ubi_device *ubi, struct rb_root *root) <answer> static 
<token> rb_node *rb; <answer> struct 
struct ubi_wl_entry <token> <answer> *e; 
rb = <token> <answer> root->rb_node; 
<token> (rb) { <answer> while 
<token> (rb->rb_left) <answer> if 
rb <token> rb->rb_left; <answer> = 
else if <token> <answer> (rb->rb_right) 
rb <token> rb->rb_right; <answer> = 
<token> { <answer> else 
e = rb_entry(rb, struct <token> u.rb); <answer> ubi_wl_entry, 
rb = <token> <answer> rb_parent(rb); 
if (rb) <token> <answer> { 
if <token> == &e->u.rb) <answer> (rb->rb_left 
<token> = NULL; <answer> rb->rb_left 
rb->rb_right <token> NULL; <answer> = 
wl_entry_destroy(ubi, <token> <answer> e); 
<token> ubi_thread(void *u) <answer> int 
int failures <token> 0; <answer> = 
struct ubi_device *ubi = <token> <answer> u; 
ubi_msg(ubi, "background thread \"%s\" started, PID <token> <answer> %d", 
<token> task_pid_nr(current)); <answer> ubi->bgt_name, 
<token> (;;) { <answer> for 
int <token> <answer> err; 
<token> (kthread_should_stop()) <answer> if 
<token> (try_to_freeze()) <answer> if 
if (list_empty(&ubi->works) <token> ubi->ro_mode || <answer> || 
<token> || ubi_dbg_is_bgt_disabled(ubi)) { <answer> !ubi->thread_enabled 
<token> (kthread_should_stop()) { <answer> if 
err = do_work(ubi, <token> <answer> NULL); 
if (err) <token> <answer> { 
ubi_err(ubi, "%s: work failed with error <token> %d", <answer> code 
<token> err); <answer> ubi->bgt_name, 
if (failures++ > <token> { <answer> WL_MAX_FAILURES) 
ubi_msg(ubi, "%s: <token> consecutive failures", <answer> %d 
ubi->bgt_name, <token> <answer> WL_MAX_FAILURES); 
ubi->thread_enabled <token> 0; <answer> = 
} <token> <answer> else 
failures = <token> <answer> 0; 
dbg_wl("background thread <token> is killed", ubi->bgt_name); <answer> \"%s\" 
ubi->thread_enabled = <token> <answer> 0; 
<token> 0; <answer> return 
static void <token> ubi_device *ubi) <answer> shutdown_work(struct 
while (!list_empty(&ubi->works)) <token> <answer> { 
struct <token> *wrk; <answer> ubi_work 
wrk <token> list_entry(ubi->works.next, struct ubi_work, list); <answer> = 
<token> wrk, 1); <answer> wrk->func(ubi, 
ubi->works_count -= <token> <answer> 1; 
ubi_assert(ubi->works_count <token> 0); <answer> >= 
static int erase_aeb(struct ubi_device *ubi, struct <token> *aeb, bool sync) <answer> ubi_ainf_peb 
<token> ubi_wl_entry *e; <answer> struct 
int <token> <answer> err; 
e = <token> GFP_KERNEL); <answer> kmem_cache_alloc(ubi_wl_entry_slab, 
<token> (!e) <answer> if 
<token> -ENOMEM; <answer> return 
e->pnum = <token> <answer> aeb->pnum; 
e->ec <token> aeb->ec; <answer> = 
ubi->lookuptbl[e->pnum] = <token> <answer> e; 
<token> (sync) { <answer> if 
err = <token> e, false); <answer> ubi_sync_erase(ubi, 
<token> (err) <answer> if 
<token> out_free; <answer> goto 
wl_tree_add(e, <token> <answer> &ubi->free); 
} else <token> <answer> { 
<token> = schedule_erase(ubi, e, aeb->vol_id, aeb->lnum, 0, false); <answer> err 
<token> (err) <answer> if 
goto <token> <answer> out_free; 
return <token> <answer> 0; 
<token> e); <answer> wl_entry_destroy(ubi, 
<token> err; <answer> return 
int ubi_wl_init(struct ubi_device *ubi, <token> ubi_attach_info *ai) <answer> struct 
int err, <token> reserved_pebs, found_pebs = 0; <answer> i, 
struct rb_node *rb1, <token> <answer> *rb2; 
struct <token> *av; <answer> ubi_ainf_volume 
struct ubi_ainf_peb *aeb, <token> <answer> *tmp; 
struct ubi_wl_entry <token> <answer> *e; 
ubi->used = ubi->erroneous <token> ubi->free = ubi->scrub = RB_ROOT; <answer> = 
ubi->max_ec <token> ai->max_ec; <answer> = 
sprintf(ubi->bgt_name, <token> ubi->ubi_num); <answer> UBI_BGT_NAME_PATTERN, 
<token> = -ENOMEM; <answer> err 
ubi->lookuptbl <token> kcalloc(ubi->peb_count, sizeof(void *), GFP_KERNEL); <answer> = 
if <token> <answer> (!ubi->lookuptbl) 
<token> err; <answer> return 
for (i = <token> i < UBI_PROT_QUEUE_LEN; i++) <answer> 0; 
ubi->pq_head <token> 0; <answer> = 
ubi->free_count = <token> <answer> 0; 
list_for_each_entry_safe(aeb, <token> &ai->erase, u.list) { <answer> tmp, 
err <token> erase_aeb(ubi, aeb, false); <answer> = 
if <token> <answer> (err) 
<token> out_free; <answer> goto 
list_for_each_entry(aeb, &ai->free, u.list) <token> <answer> { 
<token> = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL); <answer> e 
<token> (!e) { <answer> if 
<token> = -ENOMEM; <answer> err 
goto <token> <answer> out_free; 
e->pnum = <token> <answer> aeb->pnum; 
<token> = aeb->ec; <answer> e->ec 
ubi_assert(e->ec <token> 0); <answer> >= 
<token> &ubi->free); <answer> wl_tree_add(e, 
ubi->lookuptbl[e->pnum] = <token> <answer> e; 
ubi_rb_for_each_entry(rb1, <token> &ai->volumes, rb) { <answer> av, 
ubi_rb_for_each_entry(rb2, aeb, &av->root, u.rb) <token> <answer> { 
e = kmem_cache_alloc(ubi_wl_entry_slab, <token> <answer> GFP_KERNEL); 
<token> (!e) { <answer> if 
err = <token> <answer> -ENOMEM; 
goto <token> <answer> out_free; 
<token> = aeb->pnum; <answer> e->pnum 
e->ec <token> aeb->ec; <answer> = 
<token> = e; <answer> ubi->lookuptbl[e->pnum] 
if (!aeb->scrub) <token> <answer> { 
dbg_wl("add <token> %d EC %d to the used tree", <answer> PEB 
<token> e->ec); <answer> e->pnum, 
wl_tree_add(e, <token> <answer> &ubi->used); 
} else <token> <answer> { 
dbg_wl("add PEB %d EC %d to the <token> tree", <answer> scrub 
e->pnum, <token> <answer> e->ec); 
<token> &ubi->scrub); <answer> wl_tree_add(e, 
<token> &ai->fastmap, u.list) { <answer> list_for_each_entry(aeb, 
e = <token> aeb->pnum); <answer> ubi_find_fm_block(ubi, 
<token> (e) { <answer> if 
<token> = e; <answer> ubi->lookuptbl[e->pnum] 
} <token> { <answer> else 
bool sync <token> false; <answer> = 
<token> (ubi->lookuptbl[aeb->pnum]) <answer> if 
if (aeb->vol_id == <token> <answer> UBI_FM_SB_VOLUME_ID) 
sync = <token> <answer> true; 
err <token> erase_aeb(ubi, aeb, sync); <answer> = 
<token> (err) <answer> if 
goto <token> <answer> out_free; 
<token> %i PEBs", found_pebs); <answer> dbg_wl("found 
<token> == found_pebs); <answer> ubi_assert(ubi->good_peb_count 
reserved_pebs <token> WL_RESERVED_PEBS; <answer> = 
<token> &reserved_pebs); <answer> ubi_fastmap_init(ubi, 
if (ubi->avail_pebs <token> reserved_pebs) { <answer> < 
ubi_err(ubi, "no enough physical eraseblocks (%d, need <token> <answer> %d)", 
<token> reserved_pebs); <answer> ubi->avail_pebs, 
if <token> <answer> (ubi->corr_peb_count) 
ubi_err(ubi, "%d PEBs are corrupted <token> not used", <answer> and 
err = <token> <answer> -ENOSPC; 
<token> out_free; <answer> goto 
ubi->avail_pebs -= <token> <answer> reserved_pebs; 
ubi->rsvd_pebs <token> reserved_pebs; <answer> += 
static void protection_queue_destroy(struct ubi_device <token> <answer> *ubi) 
int <token> <answer> i; 
struct ubi_wl_entry <token> *tmp; <answer> *e, 
for (i = 0; i <token> UBI_PROT_QUEUE_LEN; ++i) { <answer> < 
<token> tmp, &ubi->pq[i], u.list) { <answer> list_for_each_entry_safe(e, 
wl_entry_destroy(ubi, <token> <answer> e); 
void <token> ubi_device *ubi) <answer> ubi_wl_close(struct 
dbg_wl("close the <token> sub-system"); <answer> WL 
tree_destroy(ubi, <token> <answer> &ubi->used); 
tree_destroy(ubi, <token> <answer> &ubi->erroneous); 
<token> &ubi->free); <answer> tree_destroy(ubi, 
tree_destroy(ubi, <token> <answer> &ubi->scrub); 
static int self_check_ec(struct ubi_device *ubi, int <token> int ec) <answer> pnum, 
<token> err; <answer> int 
<token> long read_ec; <answer> long 
<token> ubi_ec_hdr *ec_hdr; <answer> struct 
<token> (!ubi_dbg_chk_gen(ubi)) <answer> if 
<token> 0; <answer> return 
ec_hdr = <token> GFP_NOFS); <answer> kzalloc(ubi->ec_hdr_alsize, 
<token> (!ec_hdr) <answer> if 
<token> -ENOMEM; <answer> return 
err = ubi_io_read_ec_hdr(ubi, pnum, ec_hdr, <token> <answer> 0); 
if (err && err != UBI_IO_BITFLIPS) <token> <answer> { 
static int self_check_in_wl_tree(const struct ubi_device <token> <answer> *ubi, 
struct ubi_wl_entry *e, struct rb_root <token> <answer> *root) 
if <token> <answer> (!ubi_dbg_chk_gen(ubi)) 
<token> 0; <answer> return 
if (in_wl_tree(e, <token> <answer> root)) 
<token> 0; <answer> return 
<token> "self-check failed for PEB %d, EC %d, RB-tree %p ", <answer> ubi_err(ubi, 
e->pnum, e->ec, <token> <answer> root); 
<token> -EINVAL; <answer> return 
static <token> self_check_in_pq(const struct ubi_device *ubi, <answer> int 
<token> ubi_wl_entry *e) <answer> struct 
if <token> <answer> (!ubi_dbg_chk_gen(ubi)) 
<token> 0; <answer> return 
<token> (in_pq(ubi, e)) <answer> if 
return <token> <answer> 0; 
<token> "self-check failed for PEB %d, EC %d, Protect queue", <answer> ubi_err(ubi, 
<token> e->ec); <answer> e->pnum, 
return <token> <answer> -EINVAL; 
#ifndef <token> <answer> CONFIG_MTD_UBI_FASTMAP 
static struct <token> *get_peb_for_wl(struct ubi_device *ubi) <answer> ubi_wl_entry 
struct ubi_wl_entry <token> <answer> *e; 
e <token> find_wl_entry(ubi, &ubi->free, WL_FREE_MAX_DIFF, 0); <answer> = 
<token> e, &ubi->free); <answer> self_check_in_wl_tree(ubi, 
ubi_assert(ubi->free_count >= <token> <answer> 0); 
<token> &ubi->free); <answer> rb_erase(&e->u.rb, 
<token> e; <answer> return 
static int produce_free_peb(struct ubi_device <token> <answer> *ubi) 
<token> err; <answer> int 
while (!ubi->free.rb_node && ubi->works_count) <token> <answer> { 
dbg_wl("do one work <token> <answer> synchronously"); 
err = do_work(ubi, <token> <answer> NULL); 
<token> (err) <answer> if 
return <token> <answer> err; 
<token> 0; <answer> return 
int ubi_wl_get_peb(struct <token> *ubi) <answer> ubi_device 
int <token> <answer> err; 
<token> ubi_wl_entry *e; <answer> struct 
<token> (!ubi->free.rb_node) { <answer> if 
if (ubi->works_count == 0) <token> <answer> { 
<token> "no free eraseblocks"); <answer> ubi_err(ubi, 
return <token> <answer> -ENOSPC; 
err <token> produce_free_peb(ubi); <answer> = 
<token> (err < 0) { <answer> if 
return <token> <answer> err; 
<token> retry; <answer> goto 
e = <token> <answer> wl_get_wle(ubi); 
prot_queue_add(ubi, <token> <answer> e); 
err = ubi_self_check_all_ff(ubi, e->pnum, <token> <answer> ubi->vid_hdr_aloffset, 
ubi->peb_size <token> ubi->vid_hdr_aloffset); <answer> - 
<token> (err) { <answer> if 
ubi_err(ubi, "new PEB %d does not contain all 0xFF <token> e->pnum); <answer> bytes", 
return <token> <answer> err; 
return <token> <answer> e->pnum; 
<token> "fastmap-wl.c" <answer> #include 
#include <token> <answer> <linux/kernel.h> 
#include <token> <answer> <linux/module.h> 
<token> <linux/init.h> <answer> #include 
#include <token> <answer> <linux/delay.h> 
#include <token> <answer> <linux/pci.h> 
#include <token> <answer> <linux/interrupt.h> 
#include <token> <answer> <scsi/scsi_host.h> 
<token> "esp_scsi.h" <answer> #include 
#define DRV_MODULE_NAME <token> <answer> "am53c974" 
#define DRV_MODULE_VERSION <token> <answer> "1.00" 
static bool <token> <answer> am53c974_debug; 
<token> bool am53c974_fenab = true; <answer> static 
#define esp_dma_log(f, a...) <token> <answer> \ 
do <token> \ <answer> { 
<token> (am53c974_debug) \ <answer> if 
shost_printk(KERN_DEBUG, <token> f, ##a); \ <answer> esp->host, 
<token> while (0) <answer> } 
<token> ESP_DMA_CMD 0x10 <answer> #define 
#define ESP_DMA_STC <token> <answer> 0x11 
#define ESP_DMA_SPA <token> <answer> 0x12 
#define ESP_DMA_WBC <token> <answer> 0x13 
#define <token> 0x14 <answer> ESP_DMA_WAC 
#define <token> 0x15 <answer> ESP_DMA_STATUS 
#define ESP_DMA_SMDLA <token> <answer> 0x16 
<token> ESP_DMA_WMAC 0x17 <answer> #define 
<token> ESP_DMA_CMD_IDLE 0x00 <answer> #define 
#define <token> 0x01 <answer> ESP_DMA_CMD_BLAST 
#define <token> 0x02 <answer> ESP_DMA_CMD_ABORT 
#define <token> 0x03 <answer> ESP_DMA_CMD_START 
#define ESP_DMA_CMD_MASK <token> <answer> 0x03 
#define <token> 0x04 <answer> ESP_DMA_CMD_DIAG 
#define ESP_DMA_CMD_MDL <token> <answer> 0x10 
<token> ESP_DMA_CMD_INTE_P 0x20 <answer> #define 
#define <token> 0x40 <answer> ESP_DMA_CMD_INTE_D 
<token> ESP_DMA_CMD_DIR 0x80 <answer> #define 
#define ESP_DMA_STAT_PWDN <token> <answer> 0x01 
<token> ESP_DMA_STAT_ERROR 0x02 <answer> #define 
#define ESP_DMA_STAT_ABORT <token> <answer> 0x04 
#define <token> 0x08 <answer> ESP_DMA_STAT_DONE 
#define <token> 0x10 <answer> ESP_DMA_STAT_SCSIINT 
<token> ESP_DMA_STAT_BCMPLT 0x20 <answer> #define 
lim = <token> <answer> 1000; 
pci_esp_write8(esp, <token> | ESP_DMA_CMD_BLAST, ESP_DMA_CMD); <answer> ESP_DMA_CMD_DIR 
while (pci_esp_read8(esp, ESP_DMA_STATUS) & <token> { <answer> ESP_DMA_STAT_BCMPLT) 
if (--lim == <token> <answer> 0) 
pci_esp_write8(esp, ESP_DMA_CMD_DIR | <token> ESP_DMA_CMD); <answer> ESP_DMA_CMD_IDLE, 
esp_dma_log("DMA blast done (%d tries, %d bytes left)\n", lim, <token> <answer> resid); 
if (esp->config2 <token> ESP_CONFIG2_FENAB) <answer> & 
dma_limit <token> 24; <answer> = 
if (dma_len > (1U << <token> <answer> dma_limit)) 
dma_len = <token> << dma_limit); <answer> (1U 
base = dma_addr & ((1U << 24) <token> 1U); <answer> - 
<token> = base + dma_len; <answer> end 
<token> (end > (1U << 24)) <answer> if 
end = (1U <token> <answer> <<24); 
dma_len = end <token> base; <answer> - 
return <token> <answer> dma_len; 
static const struct esp_driver_ops pci_esp_ops <token> { <answer> = 
.esp_write8 = <token> <answer> pci_esp_write8, 
.esp_read8 <token> pci_esp_read8, <answer> = 
.irq_pending = <token> <answer> pci_esp_irq_pending, 
.reset_dma = <token> <answer> pci_esp_reset_dma, 
.dma_drain = <token> <answer> pci_esp_dma_drain, 
.dma_invalidate <token> pci_esp_dma_invalidate, <answer> = 
.send_dma_cmd <token> pci_esp_send_dma_cmd, <answer> = 
.dma_error = <token> <answer> pci_esp_dma_error, 
.dma_length_limit = <token> <answer> pci_esp_dma_length_limit, 
<token> void dc390_eeprom_prepare_read(struct pci_dev *pdev, u8 cmd) <answer> static 
u8 carry_flag <token> 1, j = 0x80, bval; <answer> = 
int <token> <answer> i; 
for (i = 0; i <token> 9; i++) { <answer> < 
<token> (carry_flag) { <answer> if 
pci_write_config_byte(pdev, <token> 0x40); <answer> 0x80, 
bval <token> 0xc0; <answer> = 
} <token> <answer> else 
<token> = 0x80; <answer> bval 
pci_write_config_byte(pdev, <token> bval); <answer> 0x80, 
pci_write_config_byte(pdev, <token> 0); <answer> 0x80, 
carry_flag = (cmd & j) ? 1 : <token> <answer> 0; 
j >>= <token> <answer> 1; 
static u16 dc390_eeprom_get_data(struct pci_dev <token> <answer> *pdev) 
int <token> <answer> i; 
u16 <token> = 0; <answer> wval 
u8 <token> <answer> bval; 
for <token> = 0; i < 16; i++) { <answer> (i 
wval <<= <token> <answer> 1; 
pci_write_config_byte(pdev, 0x80, <token> <answer> 0x80); 
pci_write_config_byte(pdev, 0x80, <token> <answer> 0x40); 
pci_read_config_byte(pdev, <token> &bval); <answer> 0x00, 
if (bval == <token> <answer> 0x22) 
<token> |= 1; <answer> wval 
<token> wval; <answer> return 
static void <token> pci_dev *pdev, u16 *ptr) <answer> dc390_read_eeprom(struct 
u8 cmd <token> DC390_EEPROM_READ, i; <answer> = 
for (i <token> 0; i < DC390_EEPROM_LEN; i++) { <answer> = 
pci_write_config_byte(pdev, <token> 0); <answer> 0xc0, 
<token> cmd++); <answer> dc390_eeprom_prepare_read(pdev, 
*ptr++ <token> dc390_eeprom_get_data(pdev); <answer> = 
pci_write_config_byte(pdev, <token> 0); <answer> 0x80, 
<token> 0x80, 0); <answer> pci_write_config_byte(pdev, 
static void <token> esp *esp) <answer> dc390_check_eeprom(struct 
struct pci_dev *pdev <token> to_pci_dev(esp->dev); <answer> = 
u8 <token> <answer> EEbuf[128]; 
u16 *ptr = (u16 *)EEbuf, wval = <token> <answer> 0; 
<token> i; <answer> int 
<token> ptr); <answer> dc390_read_eeprom(pdev, 
for (i = <token> i < DC390_EEPROM_LEN; i++, ptr++) <answer> 0; 
wval <token> *ptr; <answer> += 
<token> |= ESP_FLAG_USE_FIFO; <answer> esp->flags 
<token> (am53c974_fenab) <answer> if 
esp->config2 |= <token> <answer> ESP_CONFIG2_FENAB; 
pep->esp = <token> <answer> esp; 
if (pci_request_regions(pdev, DRV_MODULE_NAME)) <token> <answer> { 
dev_printk(KERN_ERR, <token> <answer> &pdev->dev, 
<token> memory selection failed\n"); <answer> "pci 
goto <token> <answer> fail_priv_alloc; 
esp->regs = pci_iomap(pdev, 0, <token> 0)); <answer> pci_resource_len(pdev, 
if <token> { <answer> (!esp->regs) 
dev_printk(KERN_ERR, <token> "pci I/O map failed\n"); <answer> &pdev->dev, 
<token> = -EINVAL; <answer> err 
<token> fail_release_regions; <answer> goto 
esp->dma_regs = <token> <answer> esp->regs; 
<token> = dma_alloc_coherent(&pdev->dev, 16, <answer> esp->command_block 
<token> GFP_KERNEL); <answer> &esp->command_block_dma, 
<token> (!esp->command_block) { <answer> if 
dev_printk(KERN_ERR, <token> <answer> &pdev->dev, 
<token> to allocate command block\n"); <answer> "failed 
<token> = -ENOMEM; <answer> err 
goto <token> <answer> fail_unmap_regs; 
<token> pep); <answer> pci_set_drvdata(pdev, 
<token> = request_irq(pdev->irq, scsi_esp_intr, IRQF_SHARED, <answer> err 
<token> esp); <answer> DRV_MODULE_NAME, 
if <token> < 0) { <answer> (err 
dev_printk(KERN_ERR, &pdev->dev, <token> to register IRQ\n"); <answer> "failed 
<token> fail_unmap_command_block; <answer> goto 
<token> = 7; <answer> esp->scsi_id 
shost->this_id <token> esp->scsi_id; <answer> = 
<token> = 8; <answer> shost->max_id 
shost->irq = <token> <answer> pdev->irq; 
shost->io_port = pci_resource_start(pdev, <token> <answer> 0); 
shost->n_io_port = <token> 0); <answer> pci_resource_len(pdev, 
shost->unique_id <token> shost->io_port; <answer> = 
esp->scsi_id_mask = (1 << <token> <answer> esp->scsi_id); 
#define KMSG_COMPONENT <token> <answer> "sclp_sd" 
#define <token> KMSG_COMPONENT ": " fmt <answer> pr_fmt(fmt) 
#include <token> <answer> <linux/completion.h> 
#include <token> <answer> <linux/kobject.h> 
<token> <linux/list.h> <answer> #include 
#include <token> <answer> <linux/printk.h> 
<token> <linux/slab.h> <answer> #include 
#include <token> <answer> <linux/vmalloc.h> 
<token> <linux/async.h> <answer> #include 
<token> <linux/export.h> <answer> #include 
#include <token> <answer> <linux/mutex.h> 
#include <token> <answer> <asm/pgalloc.h> 
<token> "sclp.h" <answer> #include 
#define SD_EQ_STORE_DATA <token> <answer> 0 
<token> SD_EQ_HALT 1 <answer> #define 
<token> SD_EQ_SIZE 2 <answer> #define 
#define <token> 3 <answer> SD_DI_CONFIG 
struct <token> { <answer> sclp_sd_evbuf 
<token> evbuf_header hdr; <answer> struct 
u8 <token> <answer> eq; 
u8 <token> <answer> di; 
u8 <token> <answer> rflags; 
u64 <token> <answer> :56; 
u32 <token> <answer> id; 
u16 <token> <answer> :16; 
<token> fmt; <answer> u8 
u8 <token> <answer> status; 
u64 <token> <answer> sat; 
u64 <token> <answer> sa; 
u32 <token> <answer> esize; 
u32 <token> <answer> dsize; 
} <token> <answer> __packed; 
<token> sclp_sd_sccb { <answer> struct 
struct sccb_header <token> <answer> hdr; 
struct sclp_sd_evbuf <token> <answer> evbuf; 
} <token> __aligned(PAGE_SIZE); <answer> __packed 
struct sclp_sd_data <token> <answer> { 
size_t <token> <answer> esize_bytes; 
size_t <token> <answer> dsize_bytes; 
<token> *data; <answer> void 
<token> sclp_sd_listener { <answer> struct 
<token> list_head list; <answer> struct 
u32 <token> <answer> id; 
struct completion <token> <answer> completion; 
struct <token> evbuf; <answer> sclp_sd_evbuf 
struct <token> { <answer> sclp_sd_file 
struct <token> kobj; <answer> kobject 
struct <token> data_attr; <answer> bin_attribute 
<token> mutex data_mutex; <answer> struct 
struct sclp_sd_data <token> <answer> data; 
u8 <token> <answer> di; 
#define to_sd_file(x) container_of(x, struct sclp_sd_file, <token> <answer> kobj) 
static <token> kset *sclp_sd_kset; <answer> struct 
static struct sclp_sd_file <token> <answer> *config_file; 
static <token> <answer> LIST_HEAD(sclp_sd_queue); 
<token> DEFINE_SPINLOCK(sclp_sd_queue_lock); <answer> static 
static void sclp_sd_listener_add(struct sclp_sd_listener <token> <answer> *listener) 
list_add_tail(&listener->list, <token> <answer> &sclp_sd_queue); 
static <token> sclp_sd_listener_remove(struct sclp_sd_listener *listener) <answer> void 
static <token> sclp_sd_listener_init(struct sclp_sd_listener *listener, u32 id) <answer> void 
memset(listener, <token> sizeof(*listener)); <answer> 0, 
listener->id = <token> <answer> id; 
static void sclp_sd_receiver(struct <token> *evbuf_hdr) <answer> evbuf_header 
<token> sclp_sd_evbuf *evbuf = (struct sclp_sd_evbuf *) evbuf_hdr; <answer> struct 
struct <token> *listener; <answer> sclp_sd_listener 
int <token> = 0; <answer> found 
pr_debug("received event (id=0x%08x)\n", <token> <answer> evbuf->id); 
list_for_each_entry(listener, &sclp_sd_queue, list) <token> <answer> { 
if (listener->id != <token> <answer> evbuf->id) 
listener->evbuf = <token> <answer> *evbuf; 
found <token> 1; <answer> = 
<token> (!found) <answer> if 
pr_debug("unsolicited event (id=0x%08x)\n", <token> <answer> evbuf->id); 
static struct sclp_register sclp_sd_register <token> { <answer> = 
.send_mask = <token> <answer> EVTYP_STORE_DATA_MASK, 
.receive_mask = <token> <answer> EVTYP_STORE_DATA_MASK, 
.receiver_fn = <token> <answer> sclp_sd_receiver, 
static int sclp_sd_sync(unsigned long page, u8 eq, u8 di, <token> sat, u64 sa, <answer> u64 
u32 *dsize_ptr, u32 <token> <answer> *esize_ptr) 
struct sclp_sd_sccb *sccb = (void <token> page; <answer> *) 
<token> sclp_sd_listener listener; <answer> struct 
<token> sclp_sd_evbuf *evbuf; <answer> struct 
int <token> <answer> rc; 
<token> __pa(sccb)); <answer> sclp_sd_listener_init(&listener, 
static int sclp_sd_store_data(struct sclp_sd_data <token> u8 di) <answer> *result, 
u32 dsize <token> 0, esize = 0; <answer> = 
unsigned long page, asce <token> 0; <answer> = 
void *data = <token> <answer> NULL; 
int <token> <answer> rc; 
<token> = __get_free_page(GFP_KERNEL | GFP_DMA); <answer> page 
if <token> <answer> (!page) 
<token> -ENOMEM; <answer> return 
static void sclp_sd_data_reset(struct <token> *data) <answer> sclp_sd_data 
<token> = NULL; <answer> data->data 
<token> = 0; <answer> data->dsize_bytes 
data->esize_bytes <token> 0; <answer> = 
static void <token> kobject *kobj) <answer> sclp_sd_file_release(struct 
struct sclp_sd_file <token> = to_sd_file(kobj); <answer> *sd_file 
<token> int sclp_sd_file_update(struct sclp_sd_file *sd_file) <answer> static 
<token> char *name = kobject_name(&sd_file->kobj); <answer> const 
struct <token> data; <answer> sclp_sd_data 
<token> rc; <answer> int 
rc = sclp_sd_store_data(&data, <token> <answer> sd_file->di); 
<token> (rc) { <answer> if 
if (rc <token> -ENOENT) { <answer> == 
<token> data is available for the %s data entity\n", <answer> pr_info("No 
return <token> <answer> rc; 
sd_file->data <token> data; <answer> = 
pr_info("A %zu-byte %s data entity was <token> data.dsize_bytes, <answer> retrieved\n", 
<token> KOBJ_CHANGE); <answer> kobject_uevent(&sd_file->kobj, 
return <token> <answer> 0; 
static void sclp_sd_file_update_async(void <token> async_cookie_t cookie) <answer> *data, 
struct sclp_sd_file *sd_file = <token> <answer> data; 
static <token> reload_store(struct kobject *kobj, struct kobj_attribute *attr, <answer> ssize_t 
const char *buf, size_t <token> <answer> count) 
struct sclp_sd_file <token> = to_sd_file(kobj); <answer> *sd_file 
<token> count; <answer> return 
static struct <token> reload_attr = __ATTR_WO(reload); <answer> kobj_attribute 
<token> struct attribute *sclp_sd_file_default_attrs[] = { <answer> static 
static <token> kobj_type sclp_sd_file_ktype = { <answer> struct 
<token> = &kobj_sysfs_ops, <answer> .sysfs_ops 
.release <token> sclp_sd_file_release, <answer> = 
.default_groups <token> sclp_sd_file_default_groups, <answer> = 
static ssize_t <token> file *file, struct kobject *kobj, <answer> data_read(struct 
struct bin_attribute *attr, <token> *buffer, <answer> char 
<token> off, size_t size) <answer> loff_t 
<token> sclp_sd_file *sd_file = to_sd_file(kobj); <answer> struct 
<token> data_size; <answer> size_t 
char <token> <answer> *data; 
data <token> sd_file->data.data; <answer> = 
data_size = <token> <answer> sd_file->data.dsize_bytes; 
<token> (!data || off >= data_size) { <answer> if 
<token> = 0; <answer> size 
} <token> { <answer> else 
if <token> + size > data_size) <answer> (off 
size <token> data_size - off; <answer> = 
<token> data + off, size); <answer> memcpy(buffer, 
<token> size; <answer> return 
static __init struct sclp_sd_file <token> char *name, u8 di) <answer> *sclp_sd_file_create(const 
struct sclp_sd_file <token> <answer> *sd_file; 
int <token> <answer> rc; 
sd_file <token> kzalloc(sizeof(*sd_file), GFP_KERNEL); <answer> = 
if <token> <answer> (!sd_file) 
return <token> <answer> NULL; 
sd_file->di = <token> <answer> di; 
<token> KOBJ_ADD); <answer> kobject_uevent(&sd_file->kobj, 
<token> __init int sclp_sd_init(void) <answer> static 
<token> rc; <answer> int 
<token> = sclp_register(&sclp_sd_register); <answer> rc 
if <token> <answer> (rc) 
return <token> <answer> rc; 
#include <token> <answer> "mpi-internal.h" 
<token> "longlong.h" <answer> #include 
mpi_barrett_t mpi_barrett_init(MPI m, int <token> <answer> copy) 
mpi_barrett_t <token> <answer> ctx; 
MPI <token> <answer> tmp; 
<token> = kcalloc(1, sizeof(*ctx), GFP_KERNEL); <answer> ctx 
<token> (!ctx) <answer> if 
return <token> <answer> NULL; 
if (copy) <token> <answer> { 
ctx->m <token> mpi_copy(m); <answer> = 
ctx->m_copied = <token> <answer> 1; 
<token> else <answer> } 
ctx->m <token> m; <answer> = 
<token> = mpi_get_nlimbs(m); <answer> ctx->k 
tmp = mpi_alloc(ctx->k + <token> <answer> 1); 
void mpi_mod_barrett(MPI r, <token> x, mpi_barrett_t ctx) <answer> MPI 
MPI <token> = ctx->m; <answer> m 
<token> k = ctx->k; <answer> int 
<token> y = ctx->y; <answer> MPI 
MPI r1 = <token> <answer> ctx->r1; 
MPI r2 <token> ctx->r2; <answer> = 
<token> sign; <answer> int 
if <token> > 2*k) { <answer> (mpi_get_nlimbs(x) 
mpi_mod(r, x, <token> <answer> m); 
<token> = x->sign; <answer> sign 
x->sign = <token> <answer> 0; 
<token> x); <answer> mpi_set(r2, 
<token> k-1); <answer> mpi_rshift_limbs(r2, 
mpi_mul(r2, <token> y); <answer> r2, 
<token> k+1); <answer> mpi_rshift_limbs(r2, 
mpi_set(r1, <token> <answer> x); 
<token> <linux/module.h> <answer> #include 
#include <token> <answer> <linux/init.h> 
#include <token> <answer> <linux/interrupt.h> 
<token> <linux/hrtimer.h> <answer> #include 
#include <token> <answer> <linux/list.h> 
<token> <linux/proc_fs.h> <answer> #include 
#include <token> <answer> <linux/seq_file.h> 
#include <token> <answer> <linux/uio.h> 
#include <token> <answer> <linux/net.h> 
<token> <linux/netdevice.h> <answer> #include 
#include <token> <answer> <linux/socket.h> 
#include <token> <answer> <linux/if_arp.h> 
#include <token> <answer> <linux/skbuff.h> 
<token> <linux/can.h> <answer> #include 
<token> <linux/can/core.h> <answer> #include 
#include <token> <answer> <linux/can/skb.h> 
<token> <linux/can/bcm.h> <answer> #include 
#include <token> <answer> <linux/slab.h> 
<token> <net/sock.h> <answer> #include 
<token> <net/net_namespace.h> <answer> #include 
<token> MAX_NFRAMES 256 <answer> #define 
static inline u64 <token> struct canfd_frame *cp, int offset) <answer> get_u64(const 
return *(u64 *)(cp->data + <token> <answer> offset); 
struct <token> { <answer> bcm_op 
struct list_head <token> <answer> list; 
<token> rcu_head rcu; <answer> struct 
<token> ifindex; <answer> int 
<token> can_id; <answer> canid_t 
<token> flags; <answer> u32 
unsigned long <token> frames_filtered; <answer> frames_abs, 
struct bcm_timeval ival1, <token> <answer> ival2; 
<token> hrtimer timer, thrtimer; <answer> struct 
ktime_t rx_stamp, <token> kt_ival2, kt_lastmsg; <answer> kt_ival1, 
int <token> <answer> rx_ifindex; 
int <token> <answer> cfsiz; 
u32 <token> <answer> count; 
<token> nframes; <answer> u32 
u32 <token> <answer> currframe; 
static inline <token> int *bcm_flags(struct sk_buff *skb) <answer> unsigned 
<token> IS_ENABLED(CONFIG_PROC_FS) <answer> #if 
static char *bcm_proc_getifname(struct net *net, char *result, int <token> <answer> ifindex) 
<token> net_device *dev; <answer> struct 
if <token> <answer> (!ifindex) 
<token> "any"; <answer> return 
dev <token> dev_get_by_index_rcu(net, ifindex); <answer> = 
<token> (dev) <answer> if 
<token> dev->name); <answer> strcpy(result, 
strcpy(result, <token> <answer> "???"); 
<token> result; <answer> return 
static <token> bcm_proc_show(struct seq_file *m, void *v) <answer> int 
<token> ifname[IFNAMSIZ]; <answer> char 
struct <token> *net = m->private; <answer> net 
struct sock <token> = (struct sock *)pde_data(m->file->f_inode); <answer> *sk 
struct <token> *bo = bcm_sk(sk); <answer> bcm_sock 
struct bcm_op <token> <answer> *op; 
seq_printf(m, ">>> <token> %pK", sk->sk_socket); <answer> socket 
seq_printf(m, <token> / sk %pK", sk); <answer> " 
<token> " / bo %pK", bo); <answer> seq_printf(m, 
<token> " / dropped %lu", bo->dropped_usr_msgs); <answer> seq_printf(m, 
seq_printf(m, <token> / bound %s", bcm_proc_getifname(net, ifname, bo->ifindex)); <answer> " 
seq_printf(m, " <token> <answer> <<<\n"); 
list_for_each_entry(op, &bo->rx_ops, list) <token> <answer> { 
<token> long reduction; <answer> unsigned 
static void <token> bcm_op *op) <answer> bcm_can_tx(struct 
struct sk_buff <token> <answer> *skb; 
<token> net_device *dev; <answer> struct 
struct canfd_frame *cf = op->frames + op->cfsiz * <token> <answer> op->currframe; 
<token> err; <answer> int 
static <token> bcm_send_to_user(struct bcm_op *op, struct bcm_msg_head *head, <answer> void 
struct canfd_frame <token> int has_timestamp) <answer> *frames, 
struct <token> *skb; <answer> sk_buff 
<token> canfd_frame *firstframe; <answer> struct 
<token> sockaddr_can *addr; <answer> struct 
<token> sock *sk = op->sk; <answer> struct 
unsigned int <token> = head->nframes * op->cfsiz; <answer> datalen 
int <token> <answer> err; 
<token> int *pflags; <answer> unsigned 
skb <token> alloc_skb(sizeof(*head) + datalen, gfp_any()); <answer> = 
<token> (!skb) <answer> if 
<token> head, sizeof(*head)); <answer> skb_put_data(skb, 
if <token> == 1) { <answer> (head->nframes 
if (firstframe->flags <token> RX_LOCAL) <answer> & 
*pflags |= <token> <answer> MSG_DONTROUTE; 
if (firstframe->flags <token> RX_OWN) <answer> & 
*pflags |= <token> <answer> MSG_CONFIRM; 
<token> &= BCM_CAN_FLAGS_MASK; <answer> firstframe->flags 
if (has_timestamp) <token> <answer> { 
addr = <token> sockaddr_can *)skb->cb; <answer> (struct 
<token> 0, sizeof(*addr)); <answer> memset(addr, 
addr->can_family = <token> <answer> AF_CAN; 
addr->can_ifindex = <token> <answer> op->rx_ifindex; 
err = sock_queue_rcv_skb(sk, <token> <answer> skb); 
if <token> < 0) { <answer> (err 
struct bcm_sock *bo = <token> <answer> bcm_sk(sk); 
static <token> bcm_rx_changed(struct bcm_op *op, struct canfd_frame *data) <answer> void 
<token> bcm_msg_head head; <answer> struct 
static void bcm_rx_update_and_send(struct <token> *op, <answer> bcm_op 
struct <token> *lastdata, <answer> canfd_frame 
const <token> canfd_frame *rxdata, <answer> struct 
<token> char traffic_flags) <answer> unsigned 
memcpy(lastdata, rxdata, <token> <answer> op->cfsiz); 
static void <token> bcm_op *op, unsigned int index, <answer> bcm_rx_cmp_to_index(struct 
<token> struct canfd_frame *rxdata, <answer> const 
unsigned <token> traffic_flags) <answer> char 
struct canfd_frame *cf <token> op->frames + op->cfsiz * index; <answer> = 
struct canfd_frame *lcf = op->last_frames + op->cfsiz <token> index; <answer> * 
<token> i; <answer> int 
<token> (!(lcf->flags & RX_RECV)) { <answer> if 
<token> void bcm_rx_starttimer(struct bcm_op *op) <answer> static 
if (op->flags <token> RX_NO_AUTOTIMER) <answer> & 
if <token> <answer> (op->kt_ival1) 
<token> op->kt_ival1, HRTIMER_MODE_REL_SOFT); <answer> hrtimer_start(&op->timer, 
static inline <token> bcm_rx_do_flush(struct bcm_op *op, unsigned int index) <answer> int 
struct canfd_frame *lcf = op->last_frames <token> op->cfsiz * index; <answer> + 
if ((op->last_frames) && (lcf->flags & RX_THR)) <token> <answer> { 
<token> lcf); <answer> bcm_rx_changed(op, 
return <token> <answer> 1; 
return <token> <answer> 0; 
<token> int bcm_rx_thr_flush(struct bcm_op *op) <answer> static 
int <token> = 0; <answer> updated 
if <token> > 1) { <answer> (op->nframes 
unsigned int <token> <answer> i; 
static enum <token> bcm_rx_thr_handler(struct hrtimer *hrtimer) <answer> hrtimer_restart 
struct bcm_op <token> = container_of(hrtimer, struct bcm_op, thrtimer); <answer> *op 
if (bcm_rx_thr_flush(op)) <token> <answer> { 
hrtimer_forward_now(hrtimer, <token> <answer> op->kt_ival2); 
<token> HRTIMER_RESTART; <answer> return 
} else <token> <answer> { 
static void bcm_rx_handler(struct sk_buff <token> void *data) <answer> *skb, 
struct bcm_op *op = <token> bcm_op *)data; <answer> (struct 
const struct canfd_frame *rxframe = <token> canfd_frame *)skb->data; <answer> (struct 
unsigned <token> i; <answer> int 
unsigned char <token> <answer> traffic_flags; 
<token> (op->can_id != rxframe->can_id) <answer> if 
<token> (i = 1; i < op->nframes; i++) { <answer> for 
if ((get_u64(op->frames, <token> & get_u64(rxframe, 0)) == <answer> 0) 
(get_u64(op->frames, <token> & <answer> 0) 
get_u64(op->frames + op->cfsiz * i, 0))) <token> <answer> { 
bcm_rx_cmp_to_index(op, i, <token> <answer> rxframe, 
static struct <token> *bcm_find_op(struct list_head *ops, <answer> bcm_op 
struct <token> *mh, int ifindex) <answer> bcm_msg_head 
struct <token> *op; <answer> bcm_op 
list_for_each_entry(op, <token> list) { <answer> ops, 
<token> ((op->can_id == mh->can_id) && (op->ifindex == ifindex) && <answer> if 
(op->flags & CAN_FD_FRAME) == (mh->flags & <token> <answer> CAN_FD_FRAME)) 
<token> op; <answer> return 
<token> NULL; <answer> return 
static <token> bcm_free_op_rcu(struct rcu_head *rcu_head) <answer> void 
struct bcm_op *op = container_of(rcu_head, struct <token> rcu); <answer> bcm_op, 
if ((op->frames) <token> (op->frames != &op->sframe)) <answer> && 
<token> ((op->last_frames) && (op->last_frames != &op->last_sframe)) <answer> if 
static void bcm_remove_op(struct <token> *op) <answer> bcm_op 
call_rcu(&op->rcu, <token> <answer> bcm_free_op_rcu); 
static <token> bcm_rx_unreg(struct net_device *dev, struct bcm_op *op) <answer> void 
if (op->rx_reg_dev == dev) <token> <answer> { 
<token> dev, op->can_id, <answer> can_rx_unregister(dev_net(dev), 
<token> bcm_rx_handler, op); <answer> REGMASK(op->can_id), 
static int bcm_delete_rx_op(struct list_head *ops, <token> bcm_msg_head *mh, <answer> struct 
int <token> <answer> ifindex) 
<token> bcm_op *op, *n; <answer> struct 
<token> n, ops, list) { <answer> list_for_each_entry_safe(op, 
if ((op->can_id == mh->can_id) && (op->ifindex == <token> && <answer> ifindex) 
(op->flags & <token> == (mh->flags & CAN_FD_FRAME)) { <answer> CAN_FD_FRAME) 
<token> (op->ifindex) { <answer> if 
<token> (op->rx_reg_dev) { <answer> if 
struct <token> *dev; <answer> net_device 
<token> = dev_get_by_index(sock_net(op->sk), <answer> dev 
if (dev) <token> <answer> { 
bcm_rx_unreg(dev, <token> <answer> op); 
<token> else <answer> } 
can_rx_unregister(sock_net(op->sk), <token> <answer> NULL, 
bcm_rx_handler, <token> <answer> op); 
static int bcm_delete_tx_op(struct list_head <token> struct bcm_msg_head *mh, <answer> *ops, 
int <token> <answer> ifindex) 
struct bcm_op <token> *n; <answer> *op, 
list_for_each_entry_safe(op, n, ops, <token> { <answer> list) 
if ((op->can_id == mh->can_id) && (op->ifindex == <token> && <answer> ifindex) 
(op->flags & <token> == (mh->flags & CAN_FD_FRAME)) { <answer> CAN_FD_FRAME) 
<token> int bcm_read_op(struct list_head *ops, struct bcm_msg_head *msg_head, <answer> static 
<token> ifindex) <answer> int 
struct bcm_op *op <token> bcm_find_op(ops, msg_head, ifindex); <answer> = 
<token> (!op) <answer> if 
<token> -EINVAL; <answer> return 
static <token> bcm_tx_setup(struct bcm_msg_head *msg_head, struct msghdr *msg, <answer> int 
int ifindex, <token> sock *sk) <answer> struct 
<token> bcm_sock *bo = bcm_sk(sk); <answer> struct 
struct bcm_op <token> <answer> *op; 
struct canfd_frame <token> <answer> *cf; 
unsigned <token> i; <answer> int 
int <token> <answer> err; 
if (msg_head->nframes <token> op->nframes) <answer> > 
return <token> <answer> -E2BIG; 
static int bcm_rx_setup(struct bcm_msg_head <token> struct msghdr *msg, <answer> *msg_head, 
<token> ifindex, struct sock *sk) <answer> int 
struct bcm_sock <token> = bcm_sk(sk); <answer> *bo 
struct <token> *op; <answer> bcm_op 
int <token> <answer> do_rx_register; 
int err <token> 0; <answer> = 
if ((msg_head->flags & RX_FILTER_ID) || (!(msg_head->nframes))) <token> <answer> { 
if <token> > op->nframes) <answer> (msg_head->nframes 
<token> -E2BIG; <answer> return 
if (msg_head->nframes) <token> <answer> { 
if ((op->flags & TX_CP_CAN_ID) <token> <answer> || 
(frame0->can_id <token> op->can_id)) <answer> == 
frame0->can_id = op->can_id <token> ~CAN_RTR_FLAG; <answer> & 
} <token> { <answer> else 
if (op->flags & SETTIMER) <token> <answer> { 
op->kt_lastmsg = <token> <answer> 0; 
if ((op->flags & STARTTIMER) && <token> <answer> op->kt_ival1) 
<token> op->kt_ival1, <answer> hrtimer_start(&op->timer, 
static int bcm_tx_send(struct msghdr *msg, int ifindex, <token> sock *sk, <answer> struct 
<token> cfsiz) <answer> int 
struct <token> *skb; <answer> sk_buff 
struct net_device <token> <answer> *dev; 
int <token> <answer> err; 
static int bcm_sendmsg(struct <token> *sock, struct msghdr *msg, size_t size) <answer> socket 
struct sock *sk = <token> <answer> sock->sk; 
struct bcm_sock *bo <token> bcm_sk(sk); <answer> = 
static void bcm_notify(struct bcm_sock *bo, <token> long msg, <answer> unsigned 
<token> net_device *dev) <answer> struct 
struct sock *sk = <token> <answer> &bo->sk; 
struct bcm_op <token> <answer> *op; 
<token> notify_enodev = 0; <answer> int 
<token> (!net_eq(dev_net(dev), sock_net(sk))) <answer> if 
<token> (msg) { <answer> switch 
<token> NETDEV_UNREGISTER: <answer> case 
static int <token> sock *sk) <answer> bcm_init(struct 
struct bcm_sock *bo = <token> <answer> bcm_sk(sk); 
bo->bound = <token> <answer> 0; 
<token> = 0; <answer> bo->ifindex 
bo->dropped_usr_msgs <token> 0; <answer> = 
bo->bcm_proc_read <token> NULL; <answer> = 
static int bcm_release(struct <token> *sock) <answer> socket 
struct sock *sk = <token> <answer> sock->sk; 
struct <token> *net; <answer> net 
struct <token> *bo; <answer> bcm_sock 
struct <token> *op, *next; <answer> bcm_op 
if <token> <answer> (!sk) 
<token> 0; <answer> return 
net = <token> <answer> sock_net(sk); 
bo <token> bcm_sk(sk); <answer> = 
<token> (op->ifindex) { <answer> if 
<token> (op->rx_reg_dev) { <answer> if 
struct net_device <token> <answer> *dev; 
dev = <token> op->ifindex); <answer> dev_get_by_index(net, 
if <token> { <answer> (dev) 
<token> op); <answer> bcm_rx_unreg(dev, 
} <token> <answer> else 
can_rx_unregister(net, NULL, <token> <answer> op->can_id, 
<token> op); <answer> bcm_rx_handler, 
list_for_each_entry_safe(op, next, <token> list) <answer> &bo->rx_ops, 
#include <token> <answer> "mt7601u.h" 
<token> "mcu.h" <answer> #include 
<token> "eeprom.h" <answer> #include 
<token> "trace.h" <answer> #include 
<token> "initvals_phy.h" <answer> #include 
#include <token> <answer> <linux/etherdevice.h> 
static void mt7601u_agc_reset(struct mt7601u_dev <token> <answer> *dev); 
<token> int <answer> static 
mt7601u_rf_wr(struct <token> *dev, u8 bank, u8 offset, u8 value) <answer> mt7601u_dev 
int ret = <token> <answer> 0; 
<token> (WARN_ON(!test_bit(MT7601U_STATE_WLAN_RUNNING, &dev->state)) || <answer> if 
<token> > 63)) <answer> WARN_ON(offset 
<token> -EINVAL; <answer> return 
<token> (test_bit(MT7601U_STATE_REMOVED, &dev->state)) <answer> if 
return <token> <answer> 0; 
<token> (!mt76_poll(dev, MT_RF_CSR_CFG, MT_RF_CSR_CFG_KICK, 0, 100)) { <answer> if 
ret = <token> <answer> -ETIMEDOUT; 
goto <token> <answer> out; 
mt7601u_wr(dev, <token> <answer> MT_RF_CSR_CFG, 
FIELD_PREP(MT_RF_CSR_CFG_DATA, <token> | <answer> value) 
<token> bank) | <answer> FIELD_PREP(MT_RF_CSR_CFG_REG_BANK, 
<token> offset) | <answer> FIELD_PREP(MT_RF_CSR_CFG_REG_ID, 
<token> | <answer> MT_RF_CSR_CFG_WR 
trace_rf_write(dev, bank, offset, <token> <answer> value); 
if <token> < 0) <answer> (ret 
dev_err(dev->dev, "Error: RF write <token> failed:%d!!\n", <answer> %02hhx:%02hhx 
bank, <token> ret); <answer> offset, 
return <token> <answer> ret; 
<token> int <answer> static 
mt7601u_rf_rr(struct mt7601u_dev *dev, u8 bank, <token> offset) <answer> u8 
int ret <token> -ETIMEDOUT; <answer> = 
<token> val; <answer> u32 
if (WARN_ON(!test_bit(MT7601U_STATE_WLAN_RUNNING, &dev->state)) <token> <answer> || 
<token> > 63)) <answer> WARN_ON(offset 
return <token> <answer> -EINVAL; 
<token> (test_bit(MT7601U_STATE_REMOVED, &dev->state)) <answer> if 
<token> 0xff; <answer> return 
if <token> MT_RF_CSR_CFG, MT_RF_CSR_CFG_KICK, 0, 100)) <answer> (!mt76_poll(dev, 
goto <token> <answer> out; 
<token> MT_RF_CSR_CFG, <answer> mt7601u_wr(dev, 
FIELD_PREP(MT_RF_CSR_CFG_REG_BANK, <token> | <answer> bank) 
FIELD_PREP(MT_RF_CSR_CFG_REG_ID, offset) <token> <answer> | 
<token> (!mt76_poll(dev, MT_RF_CSR_CFG, MT_RF_CSR_CFG_KICK, 0, 100)) <answer> if 
goto <token> <answer> out; 
val = mt7601u_rr(dev, <token> <answer> MT_RF_CSR_CFG); 
<token> (FIELD_GET(MT_RF_CSR_CFG_REG_ID, val) == offset && <answer> if 
FIELD_GET(MT_RF_CSR_CFG_REG_BANK, val) <token> bank) { <answer> == 
ret = <token> val); <answer> FIELD_GET(MT_RF_CSR_CFG_DATA, 
trace_rf_read(dev, bank, offset, <token> <answer> ret); 
if <token> < 0) <answer> (ret 
dev_err(dev->dev, <token> RF read %02hhx:%02hhx failed:%d!!\n", <answer> "Error: 
bank, <token> ret); <answer> offset, 
return <token> <answer> ret; 
static <token> <answer> int 
mt7601u_rf_rmw(struct mt7601u_dev *dev, u8 bank, u8 <token> u8 mask, u8 val) <answer> offset, 
int <token> <answer> ret; 
ret = mt7601u_rf_rr(dev, bank, <token> <answer> offset); 
if <token> < 0) <answer> (ret 
<token> ret; <answer> return 
val |= ret & <token> <answer> ~mask; 
<token> = mt7601u_rf_wr(dev, bank, offset, val); <answer> ret 
if <token> <answer> (ret) 
return <token> <answer> ret; 
return <token> <answer> val; 
<token> int <answer> static 
mt7601u_rf_set(struct mt7601u_dev *dev, u8 bank, u8 offset, <token> val) <answer> u8 
return mt7601u_rf_rmw(dev, bank, <token> 0, val); <answer> offset, 
<token> int <answer> static 
mt7601u_rf_clear(struct mt7601u_dev <token> u8 bank, u8 offset, u8 mask) <answer> *dev, 
return mt7601u_rf_rmw(dev, bank, offset, <token> 0); <answer> mask, 
static void mt7601u_bbp_wr(struct mt7601u_dev *dev, <token> offset, u8 val) <answer> u8 
if (WARN_ON(!test_bit(MT7601U_STATE_WLAN_RUNNING, &dev->state)) <token> <answer> || 
test_bit(MT7601U_STATE_REMOVED, <token> <answer> &dev->state)) 
if (!mt76_poll(dev, <token> MT_BBP_CSR_CFG_BUSY, 0, 1000)) { <answer> MT_BBP_CSR_CFG, 
dev_err(dev->dev, "Error: BBP <token> %02hhx failed!!\n", offset); <answer> write 
goto <token> <answer> out; 
mt7601u_wr(dev, <token> <answer> MT_BBP_CSR_CFG, 
FIELD_PREP(MT_BBP_CSR_CFG_VAL, val) <token> <answer> | 
FIELD_PREP(MT_BBP_CSR_CFG_REG_NUM, offset) <token> <answer> | 
MT_BBP_CSR_CFG_RW_MODE <token> MT_BBP_CSR_CFG_BUSY); <answer> | 
trace_bbp_write(dev, offset, <token> <answer> val); 
static <token> mt7601u_bbp_rr(struct mt7601u_dev *dev, u8 offset) <answer> int 
u32 <token> <answer> val; 
int <token> = -ETIMEDOUT; <answer> ret 
if (WARN_ON(!test_bit(MT7601U_STATE_WLAN_RUNNING, <token> <answer> &dev->state))) 
<token> -EINVAL; <answer> return 
<token> (test_bit(MT7601U_STATE_REMOVED, &dev->state)) <answer> if 
return <token> <answer> 0xff; 
if (!mt76_poll(dev, MT_BBP_CSR_CFG, MT_BBP_CSR_CFG_BUSY, <token> 1000)) <answer> 0, 
<token> out; <answer> goto 
mt7601u_wr(dev, <token> <answer> MT_BBP_CSR_CFG, 
FIELD_PREP(MT_BBP_CSR_CFG_REG_NUM, offset) <token> <answer> | 
MT_BBP_CSR_CFG_RW_MODE | MT_BBP_CSR_CFG_BUSY <token> <answer> | 
if (!mt76_poll(dev, <token> MT_BBP_CSR_CFG_BUSY, 0, 1000)) <answer> MT_BBP_CSR_CFG, 
goto <token> <answer> out; 
val <token> mt7601u_rr(dev, MT_BBP_CSR_CFG); <answer> = 
if <token> val) == offset) { <answer> (FIELD_GET(MT_BBP_CSR_CFG_REG_NUM, 
ret = <token> val); <answer> FIELD_GET(MT_BBP_CSR_CFG_VAL, 
<token> offset, ret); <answer> trace_bbp_read(dev, 
if <token> < 0) <answer> (ret 
dev_err(dev->dev, "Error: BBP read <token> failed:%d!!\n", <answer> %02hhx 
offset, <token> <answer> ret); 
<token> ret; <answer> return 
static <token> mt7601u_bbp_rmw(struct mt7601u_dev *dev, u8 offset, u8 mask, u8 val) <answer> int 
<token> ret; <answer> int 
ret <token> mt7601u_bbp_rr(dev, offset); <answer> = 
if (ret < <token> <answer> 0) 
return <token> <answer> ret; 
val |= <token> & ~mask; <answer> ret 
<token> offset, val); <answer> mt7601u_bbp_wr(dev, 
return <token> <answer> val; 
static u8 mt7601u_bbp_rmc(struct mt7601u_dev *dev, u8 offset, u8 mask, u8 <token> <answer> val) 
<token> ret; <answer> int 
ret <token> mt7601u_bbp_rr(dev, offset); <answer> = 
if (ret < <token> <answer> 0) 
return <token> <answer> ret; 
val |= ret & <token> <answer> ~mask; 
<token> (ret != val) <answer> if 
<token> offset, val); <answer> mt7601u_bbp_wr(dev, 
<token> val; <answer> return 
int mt7601u_wait_bbp_ready(struct mt7601u_dev <token> <answer> *dev) 
int <token> = 20; <answer> i 
<token> val; <answer> u8 
do <token> <answer> { 
val = mt7601u_bbp_rr(dev, <token> <answer> MT_BBP_REG_VERSION); 
if (val && <token> != 0xff) <answer> val 
} <token> (--i); <answer> while 
if <token> { <answer> (!i) 
dev_err(dev->dev, <token> BBP is not ready\n"); <answer> "Error: 
<token> -EIO; <answer> return 
return <token> <answer> 0; 
u32 mt7601u_bbp_set_ctrlch(struct mt7601u_dev *dev, bool <token> <answer> below) 
<token> mt7601u_bbp_rmc(dev, 3, 0x20, below ? 0x20 : 0); <answer> return 
<token> mt7601u_phy_get_rssi(struct mt7601u_dev *dev, <answer> int 
struct <token> *rxwi, u16 rate) <answer> mt7601u_rxwi 
static const s8 <token> = { <answer> lna[2][2][3] 
static <token> mt7601u_bbp_r47_get(struct mt7601u_dev *dev, u8 reg, u8 flag) <answer> u8 
flag |= reg & <token> <answer> ~BBP_R47_FLAG; 
<token> 47, flag); <answer> mt7601u_bbp_wr(dev, 
usleep_range(500, <token> <answer> 700); 
<token> mt7601u_bbp_rr(dev, 49); <answer> return 
static s8 mt7601u_read_bootup_temp(struct mt7601u_dev <token> <answer> *dev) 
u8 bbp_val, <token> <answer> temp; 
<token> rf_bp, rf_set; <answer> u32 
int <token> <answer> i; 
rf_set = <token> MT_RF_SETTING_0); <answer> mt7601u_rr(dev, 
rf_bp = <token> MT_RF_BYPASS_0); <answer> mt7601u_rr(dev, 
mt7601u_wr(dev, <token> 0); <answer> MT_RF_BYPASS_0, 
mt7601u_wr(dev, <token> 0x00000010); <answer> MT_RF_SETTING_0, 
mt7601u_wr(dev, <token> 0x00000010); <answer> MT_RF_BYPASS_0, 
bbp_val <token> mt7601u_bbp_rmw(dev, 47, 0, 0x10); <answer> = 
mt7601u_bbp_wr(dev, 22, <token> <answer> 0x40); 
for (i <token> 100; i && (bbp_val & 0x10); i--) <answer> = 
bbp_val = <token> 47); <answer> mt7601u_bbp_rr(dev, 
temp <token> mt7601u_bbp_r47_get(dev, bbp_val, BBP_R47_F_TEMP); <answer> = 
mt7601u_bbp_wr(dev, <token> 0); <answer> 22, 
bbp_val = <token> 21); <answer> mt7601u_bbp_rr(dev, 
bbp_val |= <token> <answer> 0x02; 
<token> 21, bbp_val); <answer> mt7601u_bbp_wr(dev, 
bbp_val <token> ~0x02; <answer> &= 
mt7601u_bbp_wr(dev, <token> bbp_val); <answer> 21, 
mt7601u_wr(dev, MT_RF_BYPASS_0, <token> <answer> 0); 
mt7601u_wr(dev, <token> rf_set); <answer> MT_RF_SETTING_0, 
<token> MT_RF_BYPASS_0, rf_bp); <answer> mt7601u_wr(dev, 
<token> temp); <answer> trace_read_temp(dev, 
<token> temp; <answer> return 
static s8 <token> mt7601u_dev *dev) <answer> mt7601u_read_temp(struct 
<token> i; <answer> int 
<token> val; <answer> u8 
<token> temp; <answer> s8 
<token> = mt7601u_bbp_rmw(dev, 47, 0x7f, 0x10); <answer> val 
avg_rssi = <token> <answer> ewma_rssi_read(&dev->avg_rssi); 
if <token> == 0) <answer> (avg_rssi 
avg_rssi <token> -avg_rssi; <answer> = 
if <token> <= -70) <answer> (avg_rssi 
<token> -= 0x20; <answer> val 
<token> if (avg_rssi <= -60) <answer> else 
val <token> 0x10; <answer> -= 
if (val <token> mt7601u_bbp_rr(dev, 66)) <answer> != 
<token> 66, val); <answer> mt7601u_bbp_wr(dev, 
static void mt7601u_phy_calibrate(struct <token> *work) <answer> work_struct 
struct <token> *dev = container_of(work, struct mt7601u_dev, <answer> mt7601u_dev 
<token> mt7601u_set_rx_path(struct mt7601u_dev *dev, u8 path) <answer> void 
mt7601u_bbp_rmw(dev, 3, 0x18, path <token> 3); <answer> << 
void mt7601u_set_tx_dac(struct <token> *dev, u8 dac) <answer> mt7601u_dev 
mt7601u_bbp_rmc(dev, 1, 0x18, dac << <token> <answer> 3); 
int <token> mt7601u_dev *dev) <answer> mt7601u_phy_init(struct 
int <token> <answer> ret; 
dev->rf_pa_mode[0] <token> mt7601u_rr(dev, MT_RF_PA_MODE_CFG0); <answer> = 
dev->rf_pa_mode[1] = mt7601u_rr(dev, <token> <answer> MT_RF_PA_MODE_CFG1); 
ret <token> mt7601u_rf_wr(dev, 0, 12, dev->ee->rf_freq_off); <answer> = 
<token> (ret) <answer> if 
return <token> <answer> ret; 
<token> = mt7601u_write_reg_pairs(dev, 0, rf_central, <answer> ret 
<token> (ret) <answer> if 
<token> ret; <answer> return 
ret = mt7601u_write_reg_pairs(dev, <token> rf_channel, <answer> 0, 
<token> (ret) <answer> if 
return <token> <answer> ret; 
ret = <token> 0, rf_vga, ARRAY_SIZE(rf_vga)); <answer> mt7601u_write_reg_pairs(dev, 
if <token> <answer> (ret) 
<token> ret; <answer> return 
ret = <token> <answer> mt7601u_init_cal(dev); 
<token> (ret) <answer> if 
<token> ret; <answer> return 
dev->prev_pwr_diff = <token> <answer> 100; 
INIT_DELAYED_WORK(&dev->cal_work, <token> <answer> mt7601u_phy_calibrate); 
INIT_DELAYED_WORK(&dev->freq_cal.work, <token> <answer> mt7601u_phy_freq_cal); 
return <token> <answer> 0; 
<token> <kunit/test.h> <answer> #include 
<token> <linux/device.h> <answer> #include 
<token> <linux/fpga/fpga-mgr.h> <answer> #include 
#include <token> <answer> <linux/module.h> 
<token> <linux/scatterlist.h> <answer> #include 
<token> <linux/types.h> <answer> #include 
#define <token> 'H' <answer> HEADER_FILL 
#define <token> 'P' <answer> IMAGE_FILL 
#define <token> 1024 <answer> IMAGE_BLOCK 
#define HEADER_SIZE <token> <answer> IMAGE_BLOCK 
#define IMAGE_SIZE (IMAGE_BLOCK <token> 4) <answer> * 
<token> mgr_stats { <answer> struct 
<token> header_match; <answer> bool 
bool <token> <answer> image_match; 
<token> seq_num; <answer> u32 
<token> op_parse_header_seq; <answer> u32 
<token> op_write_init_seq; <answer> u32 
<token> op_write_seq; <answer> u32 
<token> op_write_sg_seq; <answer> u32 
u32 <token> <answer> op_write_complete_seq; 
enum <token> op_parse_header_state; <answer> fpga_mgr_states 
enum fpga_mgr_states <token> <answer> op_write_init_state; 
enum fpga_mgr_states <token> <answer> op_write_state; 
enum fpga_mgr_states <token> <answer> op_write_sg_state; 
enum <token> op_write_complete_state; <answer> fpga_mgr_states 
<token> mgr_ctx { <answer> struct 
struct <token> *img_info; <answer> fpga_image_info 
<token> fpga_manager *mgr; <answer> struct 
struct <token> *pdev; <answer> platform_device 
struct <token> stats; <answer> mgr_stats 
<token> char *init_test_buffer(struct kunit *test, size_t count) <answer> static 
char <token> <answer> *buf; 
KUNIT_ASSERT_GE(test, count, <token> <answer> HEADER_SIZE); 
buf <token> kunit_kzalloc(test, count, GFP_KERNEL); <answer> = 
<token> buf); <answer> KUNIT_ASSERT_NOT_ERR_OR_NULL(test, 
<token> HEADER_FILL, HEADER_SIZE); <answer> memset(buf, 
memset(buf + HEADER_SIZE, IMAGE_FILL, count <token> HEADER_SIZE); <answer> - 
return <token> <answer> buf; 
static int op_parse_header(struct fpga_manager *mgr, struct fpga_image_info <token> <answer> *info, 
const <token> *buf, size_t count) <answer> char 
<token> mgr_stats *stats = mgr->priv; <answer> struct 
<token> i; <answer> size_t 
stats->op_parse_header_state = <token> <answer> mgr->state; 
<token> = stats->seq_num++; <answer> stats->op_parse_header_seq 
static int op_write(struct fpga_manager *mgr, const char *buf, <token> count) <answer> size_t 
struct mgr_stats *stats = <token> <answer> mgr->priv; 
size_t <token> <answer> i; 
stats->op_write_state <token> mgr->state; <answer> = 
stats->op_write_seq <token> stats->seq_num++; <answer> = 
<token> = true; <answer> stats->image_match 
for (i = 0; i < count; i++) <token> <answer> { 
<token> (buf[i] != IMAGE_FILL) { <answer> if 
stats->image_match = <token> <answer> false; 
<token> 0; <answer> return 
<token> int op_write_sg(struct fpga_manager *mgr, struct sg_table *sgt) <answer> static 
struct mgr_stats <token> = mgr->priv; <answer> *stats 
struct <token> miter; <answer> sg_mapping_iter 
<token> *img; <answer> char 
size_t <token> <answer> i; 
<token> = mgr->state; <answer> stats->op_write_sg_state 
<token> = stats->seq_num++; <answer> stats->op_write_sg_seq 
stats->image_match = <token> <answer> true; 
sg_miter_start(&miter, sgt->sgl, sgt->nents, <token> <answer> SG_MITER_FROM_SG); 
if (!sg_miter_skip(&miter, HEADER_SIZE)) <token> <answer> { 
<token> = false; <answer> stats->image_match 
<token> out; <answer> goto 
<token> (sg_miter_next(&miter)) { <answer> while 
img = <token> <answer> miter.addr; 
for (i = 0; i < <token> i++) { <answer> miter.length; 
if (img[i] != IMAGE_FILL) <token> <answer> { 
stats->image_match = <token> <answer> false; 
goto <token> <answer> out; 
return <token> <answer> 0; 
static int op_write_complete(struct fpga_manager *mgr, struct fpga_image_info <token> <answer> *info) 
struct mgr_stats *stats <token> mgr->priv; <answer> = 
stats->op_write_complete_state = <token> <answer> mgr->state; 
stats->op_write_complete_seq <token> stats->seq_num++; <answer> = 
<token> 0; <answer> return 
static const struct <token> fake_mgr_ops = { <answer> fpga_manager_ops 
.skip_header = <token> <answer> true, 
<token> = op_parse_header, <answer> .parse_header 
.write_init = <token> <answer> op_write_init, 
<token> = op_write, <answer> .write 
.write_sg <token> op_write_sg, <answer> = 
.write_complete = <token> <answer> op_write_complete, 
static <token> fpga_mgr_test_get(struct kunit *test) <answer> void 
struct mgr_ctx <token> = test->priv; <answer> *ctx 
<token> fpga_manager *mgr; <answer> struct 
<token> = fpga_mgr_get(&ctx->pdev->dev); <answer> mgr 
<token> mgr, ctx->mgr); <answer> KUNIT_EXPECT_PTR_EQ(test, 
static void fpga_mgr_test_lock(struct kunit <token> <answer> *test) 
<token> mgr_ctx *ctx = test->priv; <answer> struct 
int <token> <answer> ret; 
<token> = fpga_mgr_lock(ctx->mgr); <answer> ret 
KUNIT_EXPECT_EQ(test, ret, <token> <answer> 0); 
ret = <token> <answer> fpga_mgr_lock(ctx->mgr); 
KUNIT_EXPECT_EQ(test, <token> -EBUSY); <answer> ret, 
#include <token> <answer> <linux/cpu.h> 
#include <token> <answer> <linux/debugfs.h> 
#include <token> <answer> <linux/kernel.h> 
#include <token> <answer> <linux/module.h> 
<token> <linux/notifier.h> <answer> #include 
<token> <linux/pci.h> <answer> #include 
#include <token> <answer> <linux/uaccess.h> 
<token> <asm/amd_nb.h> <answer> #include 
#include <token> <answer> <asm/apic.h> 
#include <token> <answer> <asm/irq_vectors.h> 
<token> <asm/mce.h> <answer> #include 
#include <token> <answer> <asm/nmi.h> 
<token> <asm/smp.h> <answer> #include 
#include <token> <answer> "internal.h" 
static bool <token> = true; <answer> hw_injection_possible 
<token> struct mce i_mce; <answer> static 
static struct dentry <token> <answer> *dfs_inj; 
<token> MAX_FLAG_OPT_SIZE 4 <answer> #define 
<token> NBCFG 0x44 <answer> #define 
enum injection_type <token> <answer> { 
case <token> <answer> MCJ_CTX_PROCESS: 
raise_exception(m, <token> <answer> NULL); 
<token> MCE context\n"); <answer> pr_info("Invalid 
ret <token> -EINVAL; <answer> = 
pr_info("MCE exception done on CPU <token> cpu); <answer> %d\n", 
} <token> if (m->status) { <answer> else 
pr_info("Starting machine check poll CPU %d\n", <token> <answer> cpu); 
pr_info("Machine check poll done on <token> %d\n", cpu); <answer> CPU 
} <token> <answer> else 
<token> = 0; <answer> m->finished 
<token> ret; <answer> return 
static void __maybe_unused raise_mce(struct <token> *m) <answer> mce 
int context <token> MCJ_CTX(m->inject_flags); <answer> = 
if (context <token> MCJ_CTX_RANDOM) <answer> == 
if <token> & (MCJ_IRQ_BROADCAST | MCJ_NMI_BROADCAST)) { <answer> (m->inject_flags 
unsigned long <token> <answer> start; 
int <token> <answer> cpu; 
<token> cpu_online_mask); <answer> cpumask_copy(mce_inject_cpumask, 
<token> mce_inject_cpumask); <answer> cpumask_clear_cpu(get_cpu(), 
<token> { <answer> for_each_online_cpu(cpu) 
struct mce <token> = &per_cpu(injectm, cpu); <answer> *mcpu 
if <token> || <answer> (!mcpu->finished 
<token> != MCJ_CTX_RANDOM) <answer> MCJ_CTX(mcpu->inject_flags) 
cpumask_clear_cpu(cpu, <token> <answer> mce_inject_cpumask); 
if (!cpumask_empty(mce_inject_cpumask)) <token> <answer> { 
if (m->inject_flags & MCJ_IRQ_BROADCAST) <token> <answer> { 
<token> NULL, 0); <answer> mce_irq_ipi, 
} else if (m->inject_flags & <token> <answer> MCJ_NMI_BROADCAST) 
__apic_send_IPI_mask(mce_inject_cpumask, <token> <answer> NMI_VECTOR); 
<token> = jiffies; <answer> start 
<token> (!cpumask_empty(mce_inject_cpumask)) { <answer> while 
if (!time_before(jiffies, start + 2*HZ)) <token> <answer> { 
pr_err("Timeout waiting for mce inject <token> <answer> %lx\n", 
} <token> { <answer> else 
static int mce_inject_raise(struct <token> *nb, unsigned long val, <answer> notifier_block 
void <token> <answer> *data) 
struct <token> *m = (struct mce *)data; <answer> mce 
if <token> <answer> (!m) 
<token> NOTIFY_DONE; <answer> return 
<token> NOTIFY_DONE; <answer> return 
<token> struct notifier_block inject_nb = { <answer> static 
.notifier_call <token> mce_inject_raise, <answer> = 
<token> int toggle_hw_mce_inject(unsigned int cpu, bool enable) <answer> static 
u32 <token> h; <answer> l, 
int <token> <answer> err; 
<token> = rdmsr_on_cpu(cpu, MSR_K7_HWCR, &l, &h); <answer> err 
if (err) <token> <answer> { 
pr_err("%s: <token> reading HWCR\n", __func__); <answer> error 
<token> err; <answer> return 
enable ? (l |= BIT(18)) : (l &= <token> <answer> ~BIT(18)); 
err = <token> MSR_K7_HWCR, l, h); <answer> wrmsr_on_cpu(cpu, 
if <token> <answer> (err) 
<token> error writing HWCR\n", __func__); <answer> pr_err("%s: 
<token> err; <answer> return 
static int __set_inj(const char <token> <answer> *buf) 
int <token> <answer> i; 
for (i = 0; i < N_INJ_TYPES; i++) <token> <answer> { 
if (!strncmp(flags_options[i], <token> strlen(flags_options[i]))) { <answer> buf, 
if (i <token> SW_INJ && !hw_injection_possible) <answer> > 
inj_type <token> i; <answer> = 
<token> 0; <answer> return 
<token> -EINVAL; <answer> return 
static ssize_t flags_read(struct <token> *filp, char __user *ubuf, <answer> file 
size_t cnt, <token> *ppos) <answer> loff_t 
char <token> <answer> buf[MAX_FLAG_OPT_SIZE]; 
int <token> <answer> n; 
n = <token> "%s\n", flags_options[inj_type]); <answer> sprintf(buf, 
return simple_read_from_buffer(ubuf, <token> ppos, buf, n); <answer> cnt, 
static ssize_t flags_write(struct file *filp, const <token> __user *ubuf, <answer> char 
size_t cnt, loff_t <token> <answer> *ppos) 
char <token> *__buf; <answer> buf[MAX_FLAG_OPT_SIZE], 
<token> err; <answer> int 
if (!cnt || cnt <token> MAX_FLAG_OPT_SIZE) <answer> > 
<token> -EINVAL; <answer> return 
if (copy_from_user(&buf, ubuf, <token> <answer> cnt)) 
<token> -EFAULT; <answer> return 
buf[cnt <token> 1] = 0; <answer> - 
static int inj_extcpu_set(void *data, <token> val) <answer> u64 
struct mce <token> = (struct mce *)data; <answer> *m 
if <token> >= nr_cpu_ids || !cpu_online(val)) { <answer> (val 
pr_err("%s: Invalid CPU: %llu\n", __func__, <token> <answer> val); 
<token> -EINVAL; <answer> return 
<token> = val; <answer> m->extcpu 
return <token> <answer> 0; 
DEFINE_SIMPLE_ATTRIBUTE(extcpu_fops, inj_extcpu_get, inj_extcpu_set, <token> <answer> "%llu\n"); 
static void <token> *info) <answer> trigger_mce(void 
asm <token> $18"); <answer> volatile("int 
static void <token> *info) <answer> trigger_dfr_int(void 
<token> volatile("int %0" :: "i" (DEFERRED_ERROR_VECTOR)); <answer> asm 
static void trigger_thr_int(void <token> <answer> *info) 
asm volatile("int <token> :: "i" (THRESHOLD_APIC_VECTOR)); <answer> %0" 
static u32 get_nbc_for_node(int <token> <answer> node_id) 
<token> cores_per_node; <answer> u32 
cores_per_node <token> topology_num_threads_per_package() / topology_amd_nodes_per_pkg(); <answer> = 
return <token> * node_id; <answer> cores_per_node 
static void toggle_nb_mca_mst_cpu(u16 <token> <answer> nid) 
struct amd_northbridge <token> <answer> *nb; 
struct <token> *F3; <answer> pci_dev 
u32 <token> <answer> val; 
<token> err; <answer> int 
<token> = node_to_amd_nb(nid); <answer> nb 
if <token> <answer> (!nb) 
<token> = nb->misc; <answer> F3 
if <token> <answer> (!F3) 
err = <token> NBCFG, &val); <answer> pci_read_config_dword(F3, 
if (err) <token> <answer> { 
<token> Error reading F%dx%03x.\n", <answer> pr_err("%s: 
<token> PCI_FUNC(F3->devfn), NBCFG); <answer> __func__, 
if (val <token> BIT(27)) <answer> & 
pr_err("%s: Set D18F3x44[NbMcaToMstCpuEn] which <token> hasn't done.\n", <answer> BIOS 
val <token> BIT(27); <answer> |= 
err <token> pci_write_config_dword(F3, NBCFG, val); <answer> = 
if <token> <answer> (err) 
<token> Error writing F%dx%03x.\n", <answer> pr_err("%s: 
__func__, <token> NBCFG); <answer> PCI_FUNC(F3->devfn), 
static void prepare_msrs(void <token> <answer> *info) 
struct <token> m = *(struct mce *)info; <answer> mce 
<token> b = m.bank; <answer> u8 
<token> m.mcgstatus); <answer> wrmsrl(MSR_IA32_MCG_STATUS, 
if <token> { <answer> (boot_cpu_has(X86_FEATURE_SMCA)) 
if <token> == DFR_INT_INJ) { <answer> (m.inject_flags 
wrmsrl(MSR_AMD64_SMCA_MCx_DESTAT(b), <token> <answer> m.status); 
<token> m.addr); <answer> wrmsrl(MSR_AMD64_SMCA_MCx_DEADDR(b), 
} else <token> <answer> { 
<token> m.status); <answer> wrmsrl(MSR_AMD64_SMCA_MCx_STATUS(b), 
wrmsrl(MSR_AMD64_SMCA_MCx_ADDR(b), <token> <answer> m.addr); 
wrmsrl(MSR_AMD64_SMCA_MCx_MISC(b), <token> <answer> m.misc); 
wrmsrl(MSR_AMD64_SMCA_MCx_SYND(b), <token> <answer> m.synd); 
<token> else { <answer> } 
<token> m.status); <answer> wrmsrl(MSR_IA32_MCx_STATUS(b), 
wrmsrl(MSR_IA32_MCx_ADDR(b), <token> <answer> m.addr); 
<token> m.misc); <answer> wrmsrl(MSR_IA32_MCx_MISC(b), 
<token> void do_inject(void) <answer> static 
u64 mcg_status = <token> <answer> 0; 
unsigned int cpu <token> i_mce.extcpu; <answer> = 
u8 b = <token> <answer> i_mce.bank; 
i_mce.tsc = <token> <answer> rdtsc_ordered(); 
i_mce.status <token> MCI_STATUS_VAL; <answer> |= 
if <token> <answer> (i_mce.misc) 
<token> |= MCI_STATUS_MISCV; <answer> i_mce.status 
<token> (i_mce.synd) <answer> if 
i_mce.status <token> MCI_STATUS_SYNDV; <answer> |= 
if (inj_type == <token> { <answer> SW_INJ) 
<token> (inj_type == DFR_INT_INJ) { <answer> if 
i_mce.status |= <token> <answer> MCI_STATUS_DEFERRED; 
i_mce.status &= <token> <answer> ~MCI_STATUS_UC; 
if <token> && <answer> (boot_cpu_has(X86_FEATURE_AMD_DCM) 
<token> == 4 && <answer> b 
<token> < 0x17) { <answer> boot_cpu_data.x86 
<token> = get_nbc_for_node(topology_amd_node_id(cpu)); <answer> cpu 
if <token> <answer> (!cpu_online(cpu)) 
<token> err; <answer> goto 
<token> true); <answer> toggle_hw_mce_inject(cpu, 
<token> = mcg_status; <answer> i_mce.mcgstatus 
i_mce.inject_flags <token> inj_type; <answer> = 
smp_call_function_single(cpu, <token> &i_mce, 0); <answer> prepare_msrs, 
<token> false); <answer> toggle_hw_mce_inject(cpu, 
<token> (inj_type) { <answer> switch 
<token> DFR_INT_INJ: <answer> case 
smp_call_function_single(cpu, trigger_dfr_int, NULL, <token> <answer> 0); 
<token> THR_INT_INJ: <answer> case 
smp_call_function_single(cpu, trigger_thr_int, <token> 0); <answer> NULL, 
smp_call_function_single(cpu, <token> NULL, 0); <answer> trigger_mce, 
static int inj_bank_set(void *data, <token> val) <answer> u64 
struct mce *m = (struct mce <token> <answer> *)data; 
u8 <token> <answer> n_banks; 
<token> cap; <answer> u64 
if (inj_type == <token> <answer> SW_INJ) 
<token> inject; <answer> goto 
if <token> { <answer> (cpu_feature_enabled(X86_FEATURE_SMCA)) 
u64 <token> <answer> ipid; 
<token> (rdmsrl_on_cpu(m->extcpu, MSR_AMD64_SMCA_MCx_IPID(val), &ipid)) { <answer> if 
pr_err("Error reading IPID on CPU%d\n", <token> <answer> m->extcpu); 
<token> -EINVAL; <answer> return 
if (!ipid) <token> <answer> { 
pr_err("Cannot inject into <token> bank %llu\n", val); <answer> unpopulated 
return <token> <answer> -ENODEV; 
<token> (!cpu_feature_enabled(X86_FEATURE_SMCA)) <answer> if 
<token> = get_cpu(); <answer> cpu 
for (bank <token> 0; bank < MAX_NR_BANKS; ++bank) { <answer> = 
<token> status = MCI_STATUS_VAL, ipid; <answer> u64 
<token> <linux/interrupt.h> <answer> #include 
#include <token> <answer> <linux/sched.h> 
#include <token> <answer> <linux/delay.h> 
<token> <linux/comedi/comedi_8255.h> <answer> #include 
#include <token> <answer> "mite.h" 
#ifdef <token> <answer> PCIDMA 
#define IS_PCIMIO <token> <answer> 1 
#define IS_PCIMIO <token> <answer> 0 
struct mio_regmap <token> <answer> { 
<token> int mio_reg; <answer> unsigned 
int <token> <answer> size; 
static const struct <token> m_series_stc_write_regmap[] = { <answer> mio_regmap 
[NISTC_INTA_ACK_REG] <token> { 0x104, 2 }, <answer> = 
[NISTC_INTB_ACK_REG] = { 0x106, <token> }, <answer> 2 
[NISTC_AI_CMD2_REG] = { 0x108, 2 <token> <answer> }, 
[NISTC_AO_CMD2_REG] = <token> 0x10a, 2 }, <answer> { 
[NISTC_G0_CMD_REG] = { 0x10c, <token> }, <answer> 2 
[NISTC_G1_CMD_REG] = { <token> 2 }, <answer> 0x10e, 
[NISTC_AI_CMD1_REG] = { 0x110, 2 <token> <answer> }, 
[NISTC_AO_CMD1_REG] = { 0x112, <token> }, <answer> 2 
bits = <token> <answer> NI_STC_DMA_CHAN_SEL(mite_chan->channel); 
<token> NI_M_CDIO_DMA_SEL_REG, <answer> ni_set_bitfield(dev, 
<token> flags); <answer> spin_unlock_irqrestore(&devpriv->mite_channel_lock, 
<token> 0; <answer> return 
if <token> == 0) { <answer> (gpct_index 
reg = <token> <answer> NISTC_INTA2_ENA_REG; 
if <token> <answer> (enable) 
val = <token> <answer> NISTC_INTA_ENA_G0_GATE; 
<token> else { <answer> } 
reg = <token> <answer> NISTC_INTB2_ENA_REG; 
if <token> <answer> (enable) 
<token> = NISTC_INTB_ENA_G1_GATE; <answer> val 
<token> val, reg); <answer> ni_stc_writew(dev, 
ni_stc_writew(dev, <token> <answer> NISTC_AI_CMD1_CONVERT_PULSE, 
<token> NISTC_AI_CMD1_CONVERT_PULSE, <answer> ni_stc_writew(dev, 
<token> NISTC_AI_CMD1_CONVERT_PULSE, <answer> ni_stc_writew(dev, 
static inline void <token> comedi_device *dev, <answer> ni_ao_win_outw(struct 
unsigned int data, <token> addr) <answer> int 
struct <token> *devpriv = dev->private; <answer> ni_private 
unsigned <token> flags; <answer> long 
<token> flags); <answer> spin_lock_irqsave(&devpriv->window_lock, 
<token> addr, NI611X_AO_WINDOW_ADDR_REG); <answer> ni_writew(dev, 
ni_writew(dev, data, <token> <answer> NI611X_AO_WINDOW_DATA_REG); 
spin_unlock_irqrestore(&devpriv->window_lock, <token> <answer> flags); 
static inline void <token> comedi_device *dev, <answer> ni_ao_win_outl(struct 
<token> int data, int addr) <answer> unsigned 
<token> ni_private *devpriv = dev->private; <answer> struct 
unsigned long <token> <answer> flags; 
<token> flags); <answer> spin_lock_irqsave(&devpriv->window_lock, 
<token> addr, NI611X_AO_WINDOW_ADDR_REG); <answer> ni_writew(dev, 
ni_writel(dev, data, <token> <answer> NI611X_AO_WINDOW_DATA_REG); 
<token> flags); <answer> spin_unlock_irqrestore(&devpriv->window_lock, 
static inline unsigned <token> ni_ao_win_inw(struct comedi_device *dev, int addr) <answer> short 
struct <token> *devpriv = dev->private; <answer> ni_private 
unsigned <token> flags; <answer> long 
unsigned short <token> <answer> data; 
<token> flags); <answer> spin_lock_irqsave(&devpriv->window_lock, 
ni_writew(dev, <token> NI611X_AO_WINDOW_ADDR_REG); <answer> addr, 
data = ni_readw(dev, <token> <answer> NI611X_AO_WINDOW_DATA_REG); 
<token> flags); <answer> spin_unlock_irqrestore(&devpriv->window_lock, 
<token> data; <answer> return 
static inline void ni_set_bits(struct <token> *dev, int reg, <answer> comedi_device 
unsigned <token> bits, unsigned int value) <answer> int 
<token> int bit_values; <answer> unsigned 
if <token> <answer> (value) 
bit_values <token> bits; <answer> = 
<token> = 0; <answer> bit_values 
ni_set_bitfield(dev, <token> bits, bit_values); <answer> reg, 
<token> PCIDMA <answer> #ifdef 
<token> void ni_sync_ai_dma(struct comedi_device *dev) <answer> static 
struct <token> *devpriv = dev->private; <answer> ni_private 
struct comedi_subdevice *s <token> dev->read_subdev; <answer> = 
unsigned <token> flags; <answer> long 
<token> flags); <answer> spin_lock_irqsave(&devpriv->mite_channel_lock, 
<token> (devpriv->ai_mite_chan) <answer> if 
mite_sync_dma(devpriv->ai_mite_chan, <token> <answer> s); 
spin_unlock_irqrestore(&devpriv->mite_channel_lock, <token> <answer> flags); 
static <token> ni_ai_drain_dma(struct comedi_device *dev) <answer> int 
struct <token> *devpriv = dev->private; <answer> ni_private 
int <token> <answer> i; 
static const <token> timeout = 10000; <answer> int 
unsigned long <token> <answer> flags; 
int <token> = 0; <answer> retval 
spin_lock_irqsave(&devpriv->mite_channel_lock, <token> <answer> flags); 
if (devpriv->ai_mite_chan) <token> <answer> { 
for (i = 0; i < timeout; i++) <token> <answer> { 
if ((ni_stc_readw(dev, NISTC_AI_STATUS1_REG) <token> <answer> & 
NISTC_AI_STATUS1_FIFO_E) <token> <answer> && 
mite_bytes_in_transit(devpriv->ai_mite_chan) <token> 0) <answer> == 
if (i == <token> { <answer> timeout) 
<token> "timed out\n"); <answer> dev_err(dev->class_dev, 
<token> AI_Status1_Register=0x%x\n", <answer> "mite_bytes_in_transit=%i, 
<token> NISTC_AI_STATUS1_REG)); <answer> ni_stc_readw(dev, 
retval <token> -1; <answer> = 
spin_unlock_irqrestore(&devpriv->mite_channel_lock, <token> <answer> flags); 
return <token> <answer> retval; 
static int <token> comedi_device *dev) <answer> ni_ao_wait_for_dma_load(struct 
static const <token> timeout = 10000; <answer> int 
<token> i; <answer> int 
for (i = 0; <token> < timeout; i++) { <answer> i 
<token> short b_status; <answer> unsigned 
b_status <token> ni_stc_readw(dev, NISTC_AO_STATUS1_REG); <answer> = 
if (b_status & <token> <answer> NISTC_AO_STATUS1_FIFO_HF) 
<token> 100); <answer> usleep_range(10, 
<token> (i == timeout) { <answer> if 
dev_err(dev->class_dev, "timed out waiting for dma <token> <answer> load\n"); 
return <token> <answer> -EPIPE; 
<token> 0; <answer> return 
static int ni_ao_fifo_half_empty(struct comedi_device <token> <answer> *dev, 
<token> comedi_subdevice *s) <answer> struct 
const struct ni_board_struct *board <token> dev->board_ptr; <answer> = 
unsigned <token> nbytes; <answer> int 
<token> int nsamples; <answer> unsigned 
nbytes <token> comedi_buf_read_n_available(s); <answer> = 
if (nbytes == <token> { <answer> 0) 
<token> |= COMEDI_CB_OVERFLOW; <answer> s->async->events 
return <token> <answer> 0; 
nsamples = <token> nbytes); <answer> comedi_bytes_to_samples(s, 
if (nsamples <token> board->ao_fifo_depth / 2) <answer> > 
nsamples = board->ao_fifo_depth <token> 2; <answer> / 
<token> s, nsamples); <answer> ni_ao_fifo_load(dev, 
<token> 1; <answer> return 
static int ni_ao_prep_fifo(struct <token> *dev, <answer> comedi_device 
<token> comedi_subdevice *s) <answer> struct 
const struct ni_board_struct *board <token> dev->board_ptr; <answer> = 
struct ni_private <token> = dev->private; <answer> *devpriv 
unsigned int <token> <answer> nbytes; 
unsigned int <token> <answer> nsamples; 
<token> (i = 0; i < n / 2; i++) { <answer> for 
dl <token> ni_readl(dev, NI6143_AI_FIFO_DATA_REG); <answer> = 
data <token> (dl >> 16) & 0xffff; <answer> = 
<token> &data, 1); <answer> comedi_buf_write_samples(s, 
data = dl & <token> <answer> 0xffff; 
comedi_buf_write_samples(s, &data, <token> <answer> 1); 
if (n % 2) <token> <answer> { 
<token> (comedi_is_subdevice_running(s)) <answer> if 
<token> |= COMEDI_CB_ERROR; <answer> s->async->events 
if (status & NISTC_AI_STATUS1_ERR) <token> <answer> { 
dev_err(dev->class_dev, "ai <token> a_status=%04x\n", <answer> error 
s->async->events |= <token> <answer> COMEDI_CB_ERROR; 
if <token> & NISTC_AI_STATUS1_OVER) <answer> (status 
s->async->events |= <token> <answer> COMEDI_CB_OVERFLOW; 
if <token> & NISTC_AI_STATUS1_SC_TC) { <answer> (status 
if (cmd->stop_src == <token> <answer> TRIG_COUNT) 
#ifndef <token> <answer> PCIDMA 
<token> (status & NISTC_AI_STATUS1_FIFO_HF) { <answer> if 
<token> i; <answer> int 
static const int <token> = 10; <answer> timeout 
for (i = 0; <token> < timeout; ++i) { <answer> i 
if ((ni_stc_readw(dev, <token> & <answer> NISTC_AI_STATUS1_REG) 
<token> == 0) <answer> NISTC_AI_STATUS1_FIFO_HF) 
mite_prep_dma(devpriv->ao_mite_chan, <token> 32); <answer> 16, 
} <token> { <answer> else 
retval <token> -EIO; <answer> = 
<token> flags); <answer> spin_unlock_irqrestore(&devpriv->mite_channel_lock, 
<token> retval; <answer> return 
static int ni_ai_reset(struct comedi_device *dev, <token> comedi_subdevice *s) <answer> struct 
<token> ni_private *devpriv = dev->private; <answer> struct 
<token> int ai_personal; <answer> unsigned 
unsigned <token> ai_out_ctrl; <answer> int 
<token> void ni_load_channelgain_list(struct comedi_device *dev, <answer> static 
<token> comedi_subdevice *s, <answer> struct 
unsigned <token> n_chan, unsigned int *list) <answer> int 
const struct <token> *board = dev->board_ptr; <answer> ni_board_struct 
struct ni_private *devpriv = <token> <answer> dev->private; 
unsigned int offset = <token> + 1) >> 1; <answer> (s->maxdata 
<token> int chan, range, aref; <answer> unsigned 
<token> int i; <answer> unsigned 
unsigned <token> hi, lo; <answer> int 
unsigned int <token> <answer> dither; 
if <token> { <answer> (devpriv->is_m_series) 
<token> n_chan, list); <answer> ni_m_series_load_channelgain_list(dev, 
<token> (n_chan == 1 && !devpriv->is_611x && !devpriv->is_6143) { <answer> if 
if <token> && <answer> (devpriv->changain_state 
devpriv->changain_spec <token> list[0]) { <answer> == 
d = <token> <answer> 0; 
for (i = 0; i < NI_TIMEOUT; i++) <token> <answer> { 
<token> (ni_readl(dev, NI6143_AI_FIFO_STATUS_REG) & <answer> if 
<token> { <answer> 0x01) 
<token> = sdev->async->prealloc_bufsz; <answer> nbytes 
<token> sdev, nbytes); <answer> mite_init_ring_descriptors(ring, 
"%s: exact data transfer limits not implemented yet without <token> <answer> DMA\n", 
<token> unsigned int ni_min_ai_scan_period_ns(struct comedi_device *dev, <answer> static 
unsigned int <token> <answer> num_channels) 
const struct ni_board_struct *board = <token> <answer> dev->board_ptr; 
<token> ni_private *devpriv = dev->private; <answer> struct 
devpriv->an_trig_etc_reg &= <token> <answer> ~NISTC_ATRIG_ETC_ENA; 
ni_stc_writew(dev, devpriv->an_trig_etc_reg, <token> <answer> NISTC_ATRIG_ETC_REG); 
ai_trig = NISTC_AI_TRIG_START2_SEL(0) | <token> <answer> NISTC_AI_TRIG_START1_SYNC; 
switch (cmd->start_src) <token> <answer> { 
case <token> <answer> TRIG_INT: 
<token> TRIG_NOW: <answer> case 
ai_trig <token> NISTC_AI_TRIG_START1_EDGE | <answer> |= 
<token> TRIG_EXT: <answer> case 
ai_trig |= <token> <answer> NISTC_AI_TRIG_START1_SEL( 
&devpriv->routing_tables, <token> <answer> 1)); 
if <token> & CR_INVERT) <answer> (cmd->start_arg 
<token> |= NISTC_AI_TRIG_START1_POLARITY; <answer> ai_trig 
if <token> & CR_EDGE) <answer> (cmd->start_arg 
<token> |= NISTC_AI_TRIG_START1_EDGE; <answer> ai_trig 
ni_stc_writew(dev, ai_trig, <token> <answer> NISTC_AI_TRIG_SEL_REG); 
<token> &= ~NISTC_AI_MODE2_PRE_TRIGGER; <answer> mode2 
mode2 &= <token> <answer> ~NISTC_AI_MODE2_SC_INIT_LOAD_SRC; 
mode2 &= <token> <answer> ~NISTC_AI_MODE2_SC_RELOAD_MODE; 
ni_stc_writew(dev, mode2, <token> <answer> NISTC_AI_MODE2_REG); 
if (cmd->chanlist_len <token> 1 || devpriv->is_611x || devpriv->is_6143) { <answer> == 
if (cmd->chanlist_len <token> 1) <answer> > 
start_stop_select <token> NISTC_AI_STOP_POLARITY | <answer> |= 
case <token> <answer> TRIG_NONE: 
start_stop_select <token> NISTC_AI_START_EDGE | NISTC_AI_START_SYNC; <answer> |= 
ni_stc_writew(dev, <token> NISTC_AI_START_STOP_REG); <answer> start_stop_select, 
if (comedi_range_is_bipolar(s, <token> <answer> range)) 
val = <token> val); <answer> comedi_offset_munge(s, 
#ifdef <token> <answer> PCIDMA 
buf <token> cpu_to_le16(val); <answer> = 
barray[i] = <token> <answer> buf; 
array[i] = <token> <answer> val; 
chan_index %= <token> <answer> cmd->chanlist_len; 
<token> int ni_m_series_ao_config_chanlist(struct comedi_device *dev, <answer> static 
<token> comedi_subdevice *s, <answer> struct 
unsigned int <token> <answer> chanspec[], 
unsigned <token> n_chans, int timed) <answer> int 
struct ni_private <token> = dev->private; <answer> *devpriv 
unsigned <token> range; <answer> int 
unsigned int <token> <answer> chan; 
<token> int conf; <answer> unsigned 
int <token> <answer> i; 
int <token> = 0; <answer> invert 
if (timed) <token> <answer> { 
for (i = 0; i < <token> ++i) { <answer> s->n_chan; 
devpriv->ao_conf[i] <token> ~NI_M_AO_CFG_BANK_UPDATE_TIMED; <answer> &= 
ni_writeb(dev, <token> <answer> devpriv->ao_conf[i], 
ni_writeb(dev, <token> NI_M_AO_WAVEFORM_ORDER_REG(i)); <answer> 0xf, 
for (i = 0; i < <token> i++) { <answer> n_chans; 
const struct <token> *krange; <answer> comedi_krange 
chan = <token> <answer> CR_CHAN(chanspec[i]); 
<token> = CR_RANGE(chanspec[i]); <answer> range 
krange = s->range_table->range <token> range; <answer> + 
invert = <token> <answer> 0; 
conf = <token> <answer> 0; 
switch (krange->max <token> krange->min) { <answer> - 
case <token> <answer> 20000000: 
conf |= <token> <answer> NI_M_AO_CFG_BANK_REF_INT_10V; 
<token> 0, NI_M_AO_REF_ATTENUATION_REG(chan)); <answer> ni_writeb(dev, 
<token> 10000000: <answer> case 
conf |= <token> <answer> NI_M_AO_CFG_BANK_REF_INT_5V; 
ni_writeb(dev, <token> NI_M_AO_REF_ATTENUATION_REG(chan)); <answer> 0, 
<token> 4000000: <answer> case 
<token> |= NI_M_AO_CFG_BANK_REF_INT_10V; <answer> conf 
ni_writeb(dev, <token> <answer> NI_M_AO_REF_ATTENUATION_X5, 
<token> 2000000: <answer> case 
conf <token> NI_M_AO_CFG_BANK_REF_INT_5V; <answer> |= 
ni_writeb(dev, <token> <answer> NI_M_AO_REF_ATTENUATION_X5, 
"bug! unhandled ao reference <token> <answer> voltage\n"); 
switch (krange->max + krange->min) <token> <answer> { 
case <token> <answer> 0: 
conf |= <token> <answer> NI_M_AO_CFG_BANK_OFFSET_0V; 
case <token> <answer> 10000000: 
conf |= <token> <answer> NI_M_AO_CFG_BANK_OFFSET_5V; 
"bug! <token> ao offset voltage\n"); <answer> unhandled 
<token> (timed) <answer> if 
conf |= <token> <answer> NI_M_AO_CFG_BANK_UPDATE_TIMED; 
ni_writeb(dev, <token> NI_M_AO_CFG_BANK_REG(chan)); <answer> conf, 
devpriv->ao_conf[chan] <token> conf; <answer> = 
<token> i, NI_M_AO_WAVEFORM_ORDER_REG(chan)); <answer> ni_writeb(dev, 
<token> invert; <answer> return 
<token> int ni_old_ao_config_chanlist(struct comedi_device *dev, <answer> static 
struct <token> *s, <answer> comedi_subdevice 
unsigned int <token> <answer> chanspec[], 
<token> int n_chans) <answer> unsigned 
struct ni_private *devpriv <token> dev->private; <answer> = 
<token> int range; <answer> unsigned 
unsigned int <token> <answer> chan; 
unsigned int <token> <answer> conf; 
int <token> <answer> i; 
int <token> = 0; <answer> invert 
for (i = 0; i < <token> i++) { <answer> n_chans; 
chan <token> CR_CHAN(chanspec[i]); <answer> = 
range <token> CR_RANGE(chanspec[i]); <answer> = 
conf <token> NI_E_AO_DACSEL(chan); <answer> = 
if <token> range)) { <answer> (comedi_range_is_bipolar(s, 
<token> |= NI_E_AO_CFG_BIP; <answer> conf 
invert <token> (s->maxdata + 1) >> 1; <answer> = 
<token> else { <answer> } 
invert = <token> <answer> 0; 
if <token> range)) <answer> (comedi_range_is_external(s, 
conf |= <token> <answer> NI_E_AO_EXT_REF; 
val = comedi_offset_munge(s, <token> <answer> val); 
ni_ao_win_outw(dev, val, <token> <answer> reg); 
} else if <token> { <answer> (devpriv->is_m_series) 
<token> val, reg); <answer> ni_writew(dev, 
} <token> { <answer> else 
if <token> range)) <answer> (comedi_range_is_bipolar(s, 
val = <token> val); <answer> comedi_offset_munge(s, 
<token> val, reg); <answer> ni_writew(dev, 
<token> insn->n; <answer> return 
static <token> ni_ao_arm(struct comedi_device *dev, <answer> int 
<token> comedi_subdevice *s) <answer> struct 
struct ni_private *devpriv <token> dev->private; <answer> = 
<token> ret; <answer> int 
<token> interrupt_b_bits; <answer> int 
int <token> <answer> i; 
<token> const int timeout = 1000; <answer> static 
if (!devpriv->ao_needs_arming) <token> <answer> { 
dev_dbg(dev->class_dev, "%s: device does <token> need arming!\n", <answer> not 
return <token> <answer> -EINVAL; 
devpriv->ao_needs_arming <token> 0; <answer> = 
ni_set_bits(dev, <token> <answer> NISTC_INTB_ENA_REG, 
NISTC_INTB_ENA_AO_FIFO | NISTC_INTB_ENA_AO_ERR, <token> <answer> 0); 
<token> = NISTC_INTB_ENA_AO_ERR; <answer> interrupt_b_bits 
#ifdef <token> <answer> PCIDMA 
ni_stc_writew(dev, <token> NISTC_DAC_FIFO_CLR_REG); <answer> 1, 
<token> (devpriv->is_6xxx) <answer> if 
<token> 0x6, NI611X_AO_FIFO_OFFSET_LOAD_REG); <answer> ni_ao_win_outl(dev, 
ret <token> ni_ao_setup_MITE_dma(dev); <answer> = 
if <token> <answer> (ret) 
<token> ret; <answer> return 
ret <token> ni_ao_wait_for_dma_load(dev); <answer> = 
if (ret <token> 0) <answer> < 
return <token> <answer> ret; 
ret <token> ni_ao_prep_fifo(dev, s); <answer> = 
if (ret == <token> <answer> 0) 
return <token> <answer> -EPIPE; 
interrupt_b_bits |= <token> <answer> NISTC_INTB_ENA_AO_FIFO; 
<token> devpriv->ao_mode3 | NISTC_AO_MODE3_NOT_AN_UPDATE, <answer> ni_stc_writew(dev, 
ni_stc_writew(dev, devpriv->ao_mode3, <token> <answer> NISTC_AO_MODE3_REG); 
ni_stc_writew(dev, <token> NISTC_INTB_ACK_REG); <answer> NISTC_INTB_ACK_AO_ERR, 
ni_set_bits(dev, NISTC_INTB_ENA_REG, interrupt_b_bits, <token> <answer> 1); 
ni_stc_writew(dev, NISTC_AO_CMD1_UI_ARM <token> <answer> | 
NISTC_AO_CMD1_UC_ARM <token> <answer> | 
NISTC_AO_CMD1_BC_ARM <token> <answer> | 
<token> 0; <answer> return 
<token> int ni_ao_insn_config(struct comedi_device *dev, <answer> static 
struct comedi_subdevice <token> <answer> *s, 
struct <token> *insn, unsigned int *data) <answer> comedi_insn 
<token> struct ni_board_struct *board = dev->board_ptr; <answer> const 
struct ni_private <token> = dev->private; <answer> *devpriv 
unsigned int <token> <answer> nbytes; 
switch <token> { <answer> (data[0]) 
case <token> <answer> INSN_CONFIG_GET_HARDWARE_BUFFER_SIZE: 
<token> (data[1]) { <answer> switch 
<token> COMEDI_OUTPUT: <answer> case 
<token> = comedi_samples_to_bytes(s, <answer> nbytes 
data[2] <token> 1 + nbytes; <answer> = 
if <token> <answer> (devpriv->mite) 
data[2] <token> devpriv->mite->fifo_size; <answer> += 
case <token> <answer> COMEDI_INPUT: 
data[2] <token> 0; <answer> = 
return <token> <answer> -EINVAL; 
return <token> <answer> 0; 
<token> INSN_CONFIG_ARM: <answer> case 
return ni_ao_arm(dev, <token> <answer> s); 
case <token> <answer> INSN_CONFIG_GET_CMD_TIMING_CONSTRAINTS: 
if (!(trig_num == <token> || <answer> cmd->start_arg 
(trig_num == 0 && <token> != TRIG_INT))) <answer> cmd->start_src 
return <token> <answer> -EINVAL; 
<token> = NULL; <answer> s->async->inttrig 
if (devpriv->ao_needs_arming) <token> <answer> { 
static void <token> comedi_device *dev, <answer> ni_ao_cmd_personalize(struct 
<token> struct comedi_cmd *cmd) <answer> const 
<token> struct ni_board_struct *board = dev->board_ptr; <answer> const 
unsigned int <token> <answer> bits; 
ni_stc_writew(dev, <token> NISTC_RESET_REG); <answer> NISTC_RESET_AO_CFG_START, 
<token> = <answer> bits 
NISTC_AO_PERSONAL_UPDATE_PW <token> <answer> | 
<token> (devpriv->is_m_series) <answer> if 
<token> |= NISTC_AO_PERSONAL_NUM_DAC; <answer> bits 
ni_stc_writew(dev, bits, <token> <answer> NISTC_AO_PERSONAL_REG); 
<token> NISTC_RESET_AO_CFG_END, NISTC_RESET_REG); <answer> ni_stc_writew(dev, 
static void ni_ao_cmd_set_trigger(struct <token> *dev, <answer> comedi_device 
const struct <token> *cmd) <answer> comedi_cmd 
struct ni_private *devpriv <token> dev->private; <answer> = 
unsigned <token> trigsel; <answer> int 
<token> NISTC_RESET_AO_CFG_START, NISTC_RESET_REG); <answer> ni_stc_writew(dev, 
<token> devpriv->ao_mode1, NISTC_AO_MODE1_REG); <answer> ni_stc_writew(dev, 
<token> int stop_arg = cmd->stop_arg > 0 ? <answer> unsigned 
(cmd->stop_arg & <token> : 0xffffff; <answer> 0xffffff) 
if <token> { <answer> (devpriv->is_m_series) 
ni_stc_writel(dev, stop_arg - <token> NISTC_AO_UC_LOADA_REG); <answer> 1, 
ni_stc_writel(dev, stop_arg - <token> NISTC_AO_UC_LOADA_REG); <answer> 1, 
<token> NISTC_RESET_AO_CFG_END, NISTC_RESET_REG); <answer> ni_stc_writew(dev, 
static void ni_ao_cmd_set_update(struct comedi_device <token> <answer> *dev, 
<token> struct comedi_cmd *cmd) <answer> const 
struct ni_private *devpriv <token> dev->private; <answer> = 
ni_stc_writew(dev, <token> NISTC_RESET_REG); <answer> NISTC_RESET_AO_CFG_START, 
devpriv->ao_mode1 <token> ~(NISTC_AO_MODE1_UI_SRC_MASK | <answer> &= 
<token> | <answer> NISTC_AO_MODE1_UI_SRC_POLARITY 
NISTC_AO_MODE1_UPDATE_SRC_MASK <token> <answer> | 
<token> (cmd->scan_begin_src == TRIG_TIMER) { <answer> if 
unsigned <token> trigvar; <answer> int 
<token> &= ~NISTC_AO_CMD2_BC_GATE_ENA; <answer> devpriv->ao_cmd2 
trigvar <token> ni_ns_to_timer(dev, cmd->scan_begin_arg, <answer> = 
ni_stc_writew(dev, <token> NISTC_RESET_REG); <answer> NISTC_RESET_AO_CFG_END, 
static <token> ni_ao_cmd_set_fifo_mode(struct comedi_device *dev) <answer> void 
<token> ni_private *devpriv = dev->private; <answer> struct 
<token> NISTC_RESET_AO_CFG_START, NISTC_RESET_REG); <answer> ni_stc_writew(dev, 
devpriv->ao_mode2 &= <token> <answer> ~NISTC_AO_MODE2_FIFO_MODE_MASK; 
<token> PCIDMA <answer> #ifdef 
devpriv->ao_mode2 <token> NISTC_AO_MODE2_FIFO_MODE_HF_F; <answer> |= 
<token> |= NISTC_AO_MODE2_FIFO_MODE_HF; <answer> devpriv->ao_mode2 
devpriv->ao_needs_arming <token> 1; <answer> = 
<token> 0; <answer> return 
struct ni_private *devpriv <token> dev->private; <answer> = 
devpriv->ao_cmd1 = <token> <answer> 0; 
<token> = 0; <answer> devpriv->ao_cmd2 
devpriv->ao_mode1 <token> 0; <answer> = 
devpriv->ao_mode2 <token> 0; <answer> = 
<token> (devpriv->is_m_series) <answer> if 
devpriv->ao_mode3 <token> NISTC_AO_MODE3_LAST_GATE_DISABLE; <answer> = 
<token> = 0; <answer> devpriv->ao_mode3 
ni_stc_writew(dev, 0, <token> <answer> NISTC_AO_PERSONAL_REG); 
ni_stc_writew(dev, 0, <token> <answer> NISTC_AO_CMD1_REG); 
ni_stc_writew(dev, 0, <token> <answer> NISTC_AO_CMD2_REG); 
<token> 0, NISTC_AO_MODE1_REG); <answer> ni_stc_writew(dev, 
ni_stc_writew(dev, <token> NISTC_AO_MODE2_REG); <answer> 0, 
<token> 0, NISTC_AO_OUT_CTRL_REG); <answer> ni_stc_writew(dev, 
ni_stc_writew(dev, devpriv->ao_mode3, <token> <answer> NISTC_AO_MODE3_REG); 
<token> 0, NISTC_AO_START_SEL_REG); <answer> ni_stc_writew(dev, 
ni_stc_writew(dev, <token> NISTC_AO_TRIG_SEL_REG); <answer> 0, 
err <token> ni_check_trigger_arg(CR_CHAN(cmd->scan_begin_arg), <answer> |= 
if (CR_RANGE(cmd->scan_begin_arg) != 0 <token> <answer> || 
CR_AREF(cmd->scan_begin_arg) <token> 0) <answer> != 
<token> |= -EINVAL; <answer> err 
err |= comedi_check_trigger_arg_is(&cmd->convert_arg, <token> <answer> 0); 
<token> |= comedi_check_trigger_arg_is(&cmd->scan_end_arg, <answer> err 
<token> = comedi_bytes_per_scan_cmd(s, cmd); <answer> bytes_per_scan 
<token> (bytes_per_scan) { <answer> if 
err |= <token> <answer> comedi_check_trigger_arg_max(&cmd->stop_arg, 
<token> / <answer> s->async->prealloc_bufsz 
<token> (err) <answer> if 
<token> 3; <answer> return 
for (i = 0; i < timeout; <token> { <answer> ++i) 
if <token> NI_M_CDIO_STATUS_REG) & <answer> (ni_readl(dev, 
usleep_range(10, <token> <answer> 100); 
if (i == <token> { <answer> timeout) 
dev_err(dev->class_dev, "dma failed to <token> cdo fifo!\n"); <answer> fill 
<token> s); <answer> s->cancel(dev, 
return <token> <answer> -EIO; 
ni_writel(dev, NI_M_CDO_CMD_ARM <token> <answer> | 
NI_M_CDO_CMD_ERR_INT_ENA_SET <token> <answer> | 
<token> retval; <answer> return 
static int <token> comedi_device *dev, struct comedi_subdevice *s) <answer> ni_cdio_cmd(struct 
struct ni_private *devpriv = <token> <answer> dev->private; 
const struct comedi_cmd <token> = &s->async->cmd; <answer> *cmd 
unsigned <token> cdo_mode_bits; <answer> int 
int <token> <answer> retval; 
ni_writel(dev, <token> NI_M_CDIO_CMD_REG); <answer> NI_M_CDO_CMD_RESET, 
cdo_mode_bits <token> NI_M_CDO_MODE_FIFO_MODE | <answer> = 
<token> | <answer> NI_M_CDO_MODE_HALT_ON_ERROR 
if (cmd->scan_begin_arg & <token> <answer> CR_INVERT) 
cdo_mode_bits <token> NI_M_CDO_MODE_POLARITY; <answer> |= 
<token> cdo_mode_bits, NI_M_CDO_MODE_REG); <answer> ni_writel(dev, 
<token> (s->io_bits) { <answer> if 
ni_writel(dev, <token> NI_M_CDO_FIFO_DATA_REG); <answer> s->state, 
ni_writel(dev, NI_M_CDO_CMD_SW_UPDATE, <token> <answer> NI_M_CDIO_CMD_REG); 
ni_writel(dev, s->io_bits, <token> <answer> NI_M_CDO_MASK_ENA_REG); 
<token> else { <answer> } 
"attempted to run digital output command with no lines configured <token> outputs\n"); <answer> as 
return <token> <answer> -EIO; 
<token> = ni_request_cdo_mite_channel(dev); <answer> retval 
if (retval < <token> <answer> 0) 
return <token> <answer> retval; 
ni_cmd_set_mite_transfer(devpriv->cdo_mite_ring, <token> cmd, <answer> s, 
<token> / <answer> s->async->prealloc_bufsz 
<token> = ni_cdo_inttrig; <answer> s->async->inttrig 
<token> 0; <answer> return 
static int ni_cdio_cancel(struct <token> *dev, struct comedi_subdevice *s) <answer> comedi_device 
ni_writel(dev, <token> | <answer> NI_M_CDO_CMD_DISARM 
<token> | <answer> NI_M_CDO_CMD_ERR_INT_ENA_CLR 
<token> | <answer> NI_M_CDO_CMD_F_E_INT_ENA_CLR 
ni_writel(dev, 0, <token> <answer> NI_M_CDO_MASK_ENA_REG); 
<token> 0; <answer> return 
static void <token> comedi_device *dev) <answer> handle_cdio_interrupt(struct 
struct <token> *devpriv = dev->private; <answer> ni_private 
unsigned int <token> <answer> cdio_status; 
struct comedi_subdevice <token> = &dev->subdevices[NI_DIO_SUBDEV]; <answer> *s 
<token> long flags; <answer> unsigned 
<token> flags); <answer> spin_lock_irqsave(&devpriv->mite_channel_lock, 
if <token> <answer> (devpriv->cdo_mite_chan) 
mite_ack_linkc(devpriv->cdo_mite_chan, s, <token> <answer> true); 
<token> flags); <answer> spin_unlock_irqrestore(&devpriv->mite_channel_lock, 
<token> = ni_readl(dev, NI_M_CDIO_STATUS_REG); <answer> cdio_status 
if <token> & NI_M_CDIO_STATUS_CDO_ERROR) { <answer> (cdio_status 
udelay((devpriv->serial_interval_ns + 999) <token> 1000); <answer> / 
if <token> <answer> (data_in) 
*data_in = ni_stc_readw(dev, <token> <answer> NISTC_DIO_SERIAL_IN_REG); 
<token> devpriv->dio_control, NISTC_DIO_CTRL_REG); <answer> ni_stc_writew(dev, 
return <token> <answer> err; 
<token> int ni_serial_sw_readwrite8(struct comedi_device *dev, <answer> static 
struct <token> *s, <answer> comedi_subdevice 
unsigned char <token> <answer> data_out, 
unsigned <token> *data_in) <answer> char 
struct ni_private <token> = dev->private; <answer> *devpriv 
unsigned char mask, input <token> 0; <answer> = 
devpriv->dio_output &= <token> <answer> ~NISTC_DIO_SDOUT; 
if <token> & mask) <answer> (data_out 
devpriv->dio_output <token> NISTC_DIO_SDOUT; <answer> |= 
ni_stc_writew(dev, <token> NISTC_DIO_OUT_REG); <answer> devpriv->dio_output, 
devpriv->dio_control <token> NISTC_DIO_SDCLK; <answer> |= 
<token> devpriv->dio_control, NISTC_DIO_CTRL_REG); <answer> ni_stc_writew(dev, 
udelay((devpriv->serial_interval_ns + 999) / <token> <answer> 2000); 
devpriv->dio_control &= <token> <answer> ~NISTC_DIO_SDCLK; 
ni_stc_writew(dev, <token> NISTC_DIO_CTRL_REG); <answer> devpriv->dio_control, 
udelay((devpriv->serial_interval_ns + 999) / <token> <answer> 2000); 
devpriv->dio_control <token> ~NISTC_DIO_CTRL_HW_SER_TIMEBASE; <answer> &= 
<token> |= NISTC_CLK_FOUT_SLOW_TIMEBASE; <answer> clk_fout 
<token> &= ~NISTC_CLK_FOUT_DIO_SER_OUT_DIV2; <answer> clk_fout 
data[1] <token> SERIAL_600NS; <answer> = 
devpriv->serial_interval_ns = <token> <answer> data[1]; 
} else if <token> <= SERIAL_1_2US) { <answer> (data[1] 
<token> &= ~NISTC_DIO_CTRL_HW_SER_TIMEBASE; <answer> devpriv->dio_control 
<token> |= NISTC_CLK_FOUT_SLOW_TIMEBASE | <answer> clk_fout 
data[1] = <token> <answer> SERIAL_1_2US; 
devpriv->serial_interval_ns = <token> <answer> data[1]; 
} else <token> (data[1] <= SERIAL_10US) { <answer> if 
<token> |= NISTC_DIO_CTRL_HW_SER_TIMEBASE; <answer> devpriv->dio_control 
clk_fout |= NISTC_CLK_FOUT_SLOW_TIMEBASE <token> <answer> | 
<token> = SERIAL_10US; <answer> data[1] 
devpriv->serial_interval_ns <token> data[1]; <answer> = 
<token> else { <answer> } 
devpriv->dio_control &= <token> | <answer> ~(NISTC_DIO_CTRL_HW_SER_ENA 
<token> = 0; <answer> devpriv->serial_hw_mode 
data[1] = <token> / 1000) * 1000; <answer> (data[1] 
<token> = data[1]; <answer> devpriv->serial_interval_ns 
devpriv->clock_and_fout <token> clk_fout; <answer> = 
ni_stc_writew(dev, <token> NISTC_DIO_CTRL_REG); <answer> devpriv->dio_control, 
ni_stc_writew(dev, <token> NISTC_CLK_FOUT_REG); <answer> devpriv->clock_and_fout, 
return <token> <answer> 1; 
case <token> <answer> INSN_CONFIG_BIDIRECTIONAL_DATA: 
if (devpriv->serial_interval_ns <token> 0) <answer> == 
<token> -EINVAL; <answer> return 
byte_out = <token> & 0xFF; <answer> data[1] 
if (devpriv->serial_hw_mode) <token> <answer> { 
<token> = ni_serial_hw_readwrite8(dev, s, byte_out, <answer> err 
} else <token> (devpriv->serial_interval_ns > 0) { <answer> if 
err = <token> s, byte_out, <answer> ni_serial_sw_readwrite8(dev, 
} <token> { <answer> else 
dev_err(dev->class_dev, "serial <token> <answer> disabled!\n"); 
<token> -EINVAL; <answer> return 
if (err <token> 0) <answer> < 
<token> err; <answer> return 
data[1] <token> byte_in & 0xFF; <answer> = 
return <token> <answer> insn->n; 
return <token> <answer> -EINVAL; 
static <token> init_ao_67xx(struct comedi_device *dev, struct comedi_subdevice *s) <answer> void 
<token> i; <answer> int 
for (i = 0; i < s->n_chan; <token> { <answer> i++) 
<token> NI_E_AO_DACSEL(i) | 0x0, <answer> ni_ao_win_outw(dev, 
<token> 0x0, NI67XX_AO_SP_UPDATES_REG); <answer> ni_ao_win_outw(dev, 
<token> const struct mio_regmap ni_gpct_to_stc_regmap[] = { <answer> static 
[NITIO_G0_AUTO_INC] <token> { NISTC_G0_AUTOINC_REG, 2 }, <answer> = 
<token> = { NISTC_G1_AUTOINC_REG, 2 }, <answer> [NITIO_G1_AUTO_INC] 
<token> = { NISTC_G0_CMD_REG, 2 }, <answer> [NITIO_G0_CMD] 
<token> = { NISTC_G1_CMD_REG, 2 }, <answer> [NITIO_G1_CMD] 
[NITIO_G0_HW_SAVE] = { NISTC_G0_HW_SAVE_REG, <token> }, <answer> 4 
[NITIO_G1_HW_SAVE] = { <token> 4 }, <answer> NISTC_G1_HW_SAVE_REG, 
[NITIO_G0_SW_SAVE] = { NISTC_G0_SAVE_REG, 4 <token> <answer> }, 
[NITIO_G1_SW_SAVE] = { NISTC_G1_SAVE_REG, 4 <token> <answer> }, 
[NITIO_G0_MODE] = { <token> 2 }, <answer> NISTC_G0_MODE_REG, 
[NITIO_G1_MODE] = <token> NISTC_G1_MODE_REG, 2 }, <answer> { 
[NITIO_G0_LOADA] = { NISTC_G0_LOADA_REG, 4 <token> <answer> }, 
<token> = { NISTC_G1_LOADA_REG, 4 }, <answer> [NITIO_G1_LOADA] 
[NITIO_G0_LOADB] = { NISTC_G0_LOADB_REG, 4 <token> <answer> }, 
[NITIO_G1_LOADB] = { <token> 4 }, <answer> NISTC_G1_LOADB_REG, 
[NITIO_G0_INPUT_SEL] = <token> NISTC_G0_INPUT_SEL_REG, 2 }, <answer> { 
[NITIO_G1_INPUT_SEL] = <token> NISTC_G1_INPUT_SEL_REG, 2 }, <answer> { 
*bitstring = <token> & 0x1) << 11) | <answer> ((addr 
((addr & 0x2) << 9) <token> <answer> | 
((addr & 0x4) << 7) | ((addr & 0x8) << 5) <token> (val & 0xff); <answer> | 
<token> 12; <answer> return 
static int pack_dac8800(int addr, int <token> int *bitstring) <answer> val, 
*bitstring = ((addr & 0x7) <token> 8) | (val & 0xff); <answer> << 
<token> 11; <answer> return 
static int <token> addr, int val, int *bitstring) <answer> pack_dac8043(int 
*bitstring = val & <token> <answer> 0xfff; 
return <token> <answer> 12; 
static <token> pack_ad8522(int addr, int val, int *bitstring) <answer> int 
<token> = (val & 0xfff) | (addr ? 0xc000 : 0xa000); <answer> *bitstring 
<token> 16; <answer> return 
static int pack_ad8804(int addr, int <token> int *bitstring) <answer> val, 
*bitstring = ((addr <token> 0xf) << 8) | (val & 0xff); <answer> & 
<token> 12; <answer> return 
static int pack_ad8842(int <token> int val, int *bitstring) <answer> addr, 
*bitstring = ((addr + 1) << 8) | (val <token> 0xff); <answer> & 
<token> 12; <answer> return 
struct <token> { <answer> caldac_struct 
<token> n_chans; <answer> int 
int <token> <answer> n_bits; 
int (*packbits)(int <token> int value, int *bitstring); <answer> address, 
static <token> caldac_struct caldacs[] = { <answer> struct 
[mb88341] = {12, <token> pack_mb88341}, <answer> 8, 
[dac8800] <token> {8, 8, pack_dac8800}, <answer> = 
[dac8043] = {1, 12, <token> <answer> pack_dac8043}, 
[ad8522] <token> {2, 12, pack_ad8522}, <answer> = 
<token> = {12, 8, pack_ad8804}, <answer> [ad8804] 
[ad8842] = <token> 8, pack_ad8842}, <answer> {8, 
[ad8804_debug] = {16, <token> pack_ad8804}, <answer> 8, 
static void ni_write_caldac(struct <token> *dev, int addr, int val) <answer> comedi_device 
const struct ni_board_struct *board = <token> <answer> dev->board_ptr; 
struct ni_private *devpriv = <token> <answer> dev->private; 
unsigned int loadbit = <token> bits = 0, bit, bitstring = 0; <answer> 0, 
unsigned int <token> <answer> cmd; 
<token> i; <answer> int 
int <token> <answer> type; 
if (devpriv->caldacs[addr] <token> val) <answer> == 
devpriv->caldacs[addr] <token> val; <answer> = 
<token> (i = 0; i < 3; i++) { <answer> for 
type <token> board->caldac[i]; <answer> = 
if <token> == caldac_none) <answer> (type 
if (addr < <token> { <answer> caldacs[type].n_chans) 
<token> = caldacs[type].packbits(addr, val, &bitstring); <answer> bits 
loadbit <token> NI_E_SERIAL_CMD_DAC_LD(i); <answer> = 
<token> -= caldacs[type].n_chans; <answer> addr 
for (i = 0; <token> < timeout; i++) { <answer> i 
if (ni_ao_win_inw(dev, <token> & <answer> NI67XX_CAL_STATUS_REG) 
if (i <token> timeout) <answer> == 
"possible problem <token> never saw adc go busy?\n"); <answer> - 
static <token> cs5529_do_conversion(struct comedi_device *dev, <answer> int 
unsigned short <token> <answer> *data) 
<token> retval; <answer> int 
unsigned <token> status; <answer> short 
cs5529_command(dev, CS5529_CMD_CB | <token> <answer> CS5529_CMD_SINGLE_CONV); 
<token> = cs5529_wait_for_idle(dev); <answer> retval 
if <token> { <answer> (retval) 
"timeout or signal in <token> __func__); <answer> %s()\n", 
return <token> <answer> -ETIME; 
status = ni_ao_win_inw(dev, <token> <answer> NI67XX_CAL_STATUS_REG); 
if (status & <token> { <answer> NI67XX_CAL_STATUS_OSC_DETECT) 
"cs5529 conversion error, <token> CSS_OSC_DETECT\n"); <answer> status 
return <token> <answer> -EIO; 
if (status <token> NI67XX_CAL_STATUS_OVERRANGE) { <answer> & 
"cs5529 conversion <token> overrange (ignoring)\n"); <answer> error, 
if (data) <token> <answer> { 
*data = ni_ao_win_inw(dev, <token> <answer> NI67XX_CAL_DATA_REG); 
if <token> & CR_ALT_SOURCE) <answer> (insn->chanspec 
channel_select <token> INTERNAL_REF; <answer> = 
channel_select <token> CR_CHAN(insn->chanspec); <answer> = 
<token> channel_select, NI67XX_AO_CAL_CHAN_SEL_REG); <answer> ni_ao_win_outw(dev, 
for (n = <token> n < insn->n; n++) { <answer> 0; 
retval = cs5529_do_conversion(dev, <token> <answer> &sample); 
if (retval <token> 0) <answer> < 
return <token> <answer> retval; 
<token> = sample; <answer> data[n] 
return <token> <answer> insn->n; 
static void cs5529_config_write(struct comedi_device *dev, <token> int value, <answer> unsigned 
<token> int reg_select_bits) <answer> unsigned 
<token> (value >> 16) & 0xff, NI67XX_CAL_CFG_HI_REG); <answer> ni_ao_win_outw(dev, 
ni_ao_win_outw(dev, value & 0xffff, <token> <answer> NI67XX_CAL_CFG_LO_REG); 
<token> &= CS5529_CMD_REG_MASK; <answer> reg_select_bits 
cs5529_command(dev, CS5529_CMD_CB <token> reg_select_bits); <answer> | 
if <token> <answer> (cs5529_wait_for_idle(dev)) 
"timeout or signal <token> %s\n", __func__); <answer> in 
static int init_cs5529(struct comedi_device <token> <answer> *dev) 
unsigned int config_bits = <token> | <answer> CS5529_CFG_PORT_FLAG 
#if <token> <answer> 1 
static int ni_mseries_get_pll_parameters(unsigned int <token> <answer> reference_period_ns, 
unsigned int <token> <answer> *freq_divider, 
unsigned int <token> <answer> *freq_multiplier, 
unsigned int <token> <answer> *actual_period_ns) 
<token> int div; <answer> unsigned 
<token> int best_div = 1; <answer> unsigned 
unsigned int <token> <answer> mult; 
<token> int best_mult = 1; <answer> unsigned 
static const unsigned int <token> = 1000; <answer> pico_per_nano 
const unsigned int <token> = reference_period_ns * <answer> reference_picosec 
<token> const unsigned int target_picosec = 12500; <answer> static 
int best_period_picosec <token> 0; <answer> = 
for (div = <token> div <= NI_M_PLL_MAX_DIVISOR; ++div) { <answer> 1; 
<token> (mult = 1; mult <= NI_M_PLL_MAX_MULTIPLIER; ++mult) { <answer> for 
unsigned <token> new_period_ps = <answer> int 
<token> * div) / mult; <answer> (reference_picosec 
if (abs(new_period_ps - <token> < <answer> target_picosec) 
abs(best_period_picosec - target_picosec)) <token> <answer> { 
best_period_picosec = <token> <answer> new_period_ps; 
best_div = <token> <answer> div; 
<token> = mult; <answer> best_mult 
<token> (best_period_picosec == 0) <answer> if 
<token> -EIO; <answer> return 
*freq_divider <token> best_div; <answer> = 
*freq_multiplier = <token> <answer> best_mult; 
if <token> < min_period_ns || period_ns > max_period_ns) { <answer> (period_ns 
"%s: you must specify an input clock frequency between <token> and %i nanosec for the phased-lock loop\n", <answer> %i 
__func__, <token> max_period_ns); <answer> min_period_ns, 
<token> -EINVAL; <answer> return 
<token> &= ~NISTC_RTSI_TRIG_USE_CLK; <answer> devpriv->rtsi_trig_direction_reg 
ni_stc_writew(dev, <token> <answer> devpriv->rtsi_trig_direction_reg, 
pll_control_bits = NI_M_PLL_CTRL_ENA <token> NI_M_PLL_CTRL_VCO_MODE_75_150MHZ; <answer> | 
devpriv->clock_and_fout2 <token> NI_M_CLK_FOUT2_TIMEBASE1_PLL | <answer> |= 
devpriv->clock_and_fout2 <token> ~NI_M_CLK_FOUT2_PLL_SRC_MASK; <answer> &= 
<token> (source) { <answer> switch 
case <token> <answer> NI_MIO_PLL_PXI_STAR_TRIGGER_CLOCK: 
devpriv->clock_and_fout2 |= <token> <answer> NI_M_CLK_FOUT2_PLL_SRC_STAR; 
<token> NI_MIO_PLL_PXI10_CLOCK: <answer> case 
dev_err(dev->class_dev, "%s: unknown rtsi <token> __func__); <answer> channel\n", 
<token> -EINVAL; <answer> return 
return <token> <answer> 2; 
static unsigned <token> ni_get_rtsi_routing(struct comedi_device *dev, <answer> int 
<token> int chan) <answer> unsigned 
struct ni_private <token> = dev->private; <answer> *devpriv 
<token> (chan >= TRIGGER_LINE(0)) <answer> if 
static const int <token> = { <answer> default_rtsi_routing[] 
<token> = NI_RTSI_OUTPUT_ADR_START1, <answer> [0] 
<token> = NI_RTSI_OUTPUT_ADR_START2, <answer> [1] 
[2] <token> NI_RTSI_OUTPUT_SCLKG, <answer> = 
<token> = NI_RTSI_OUTPUT_DACUPDN, <answer> [3] 
<token> = NI_RTSI_OUTPUT_DA_START1, <answer> [4] 
[5] <token> NI_RTSI_OUTPUT_G_SRC0, <answer> = 
[6] <token> NI_RTSI_OUTPUT_G_GATE0, <answer> = 
[7] = <token> <answer> NI_RTSI_OUTPUT_RTSI_OSC, 
static <token> set_rgout0_reg(int reg, struct comedi_device *dev) <answer> void 
<token> ni_private *devpriv = dev->private; <answer> struct 
if <token> { <answer> (devpriv->is_m_series) 
devpriv->rtsi_trig_direction_reg <token> <answer> &= 
devpriv->rtsi_trig_direction_reg <token> <answer> |= 
(reg <token> NISTC_RTSI_TRIG_DIR_SUB_SEL1_SHIFT) & <answer> << 
<token> devpriv->rtsi_trig_direction_reg, <answer> ni_stc_writew(dev, 
} <token> { <answer> else 
<token> &= ~NISTC_RTSI_TRIGB_SUB_SEL1; <answer> devpriv->rtsi_trig_b_output_reg 
<token> |= <answer> devpriv->rtsi_trig_b_output_reg 
<token> << NISTC_RTSI_TRIGB_SUB_SEL1_SHIFT) & <answer> (reg 
ni_stc_writew(dev, <token> <answer> devpriv->rtsi_trig_b_output_reg, 
static int get_rgout0_reg(struct <token> *dev) <answer> comedi_device 
<token> ni_private *devpriv = dev->private; <answer> struct 
int <token> <answer> reg; 
if <token> <answer> (devpriv->is_m_series) 
reg <token> (devpriv->rtsi_trig_direction_reg & <answer> = 
<token> NISTC_RTSI_TRIG_DIR_SUB_SEL1_SHIFT; <answer> >> 
reg = <token> & <answer> (devpriv->rtsi_trig_b_output_reg 
<token> NISTC_RTSI_TRIGB_SUB_SEL1_SHIFT; <answer> >> 
return <token> <answer> reg; 
static inline int get_rgout0_src(struct <token> *dev) <answer> comedi_device 
<token> ni_private *devpriv = dev->private; <answer> struct 
int reg = <token> <answer> get_rgout0_reg(dev); 
return ni_find_route_source(reg, <token> &devpriv->routing_tables); <answer> NI_RGOUT0, 
<token> int incr_rgout0_src_use(int src, struct comedi_device *dev) <answer> static 
struct ni_private *devpriv = <token> <answer> dev->private; 
s8 reg = ni_lookup_route_register(CR_CHAN(src), <token> <answer> NI_RGOUT0, 
if <token> < 0) <answer> (reg 
return <token> <answer> -EINVAL; 
if (devpriv->rgout0_usage <token> 0 && get_rgout0_reg(dev) != reg) <answer> > 
return <token> <answer> -EBUSY; 
set_rgout0_reg(reg, <token> <answer> dev); 
return <token> <answer> 0; 
static int decr_rgout0_src_use(int src, struct <token> *dev) <answer> comedi_device 
<token> ni_private *devpriv = dev->private; <answer> struct 
<token> reg = ni_lookup_route_register(CR_CHAN(src), NI_RGOUT0, <answer> s8 
if (devpriv->rgout0_usage <token> 0 && get_rgout0_reg(dev) == reg) { <answer> > 
if <token> <answer> (!devpriv->rgout0_usage) 
static void set_ith_rtsi_brd_reg(int i, int reg, struct comedi_device <token> <answer> *dev) 
struct ni_private <token> = dev->private; <answer> *devpriv 
reg <token> get_ith_rtsi_brd_reg(brd_index, dev); <answer> = 
return <token> brd, &devpriv->routing_tables); <answer> ni_find_route_source(reg, 
static int incr_rtsi_brd_src_use(int <token> struct comedi_device *dev) <answer> src, 
struct <token> *devpriv = dev->private; <answer> ni_private 
<token> first_available = -1; <answer> int 
int err <token> -EINVAL; <answer> = 
<token> reg; <answer> s8 
int <token> <answer> i; 
<token> = -EBUSY; <answer> err 
if (get_ith_rtsi_brd_reg(i, dev) == <token> { <answer> reg) 
<token> success; <answer> goto 
if (first_available < <token> <answer> 0) 
<token> err; <answer> return 
static int decr_rtsi_brd_src_use(int <token> int rtsi_brd, <answer> src, 
<token> comedi_device *dev) <answer> struct 
struct ni_private *devpriv <token> dev->private; <answer> = 
s8 reg = ni_lookup_route_register(CR_CHAN(src), <token> <answer> rtsi_brd, 
const <token> i = rtsi_brd - NI_RTSI_BRD(0); <answer> int 
if <token> > 0 && <answer> (devpriv->rtsi_shared_mux_usage[i] 
get_ith_rtsi_brd_reg(i, dev) == <token> { <answer> reg) 
<token> (!devpriv->rtsi_shared_mux_usage[i]) <answer> if 
devpriv->clock_and_fout2 <token> NI_M_CLK_FOUT2_RTSI_10MHZ; <answer> = 
devpriv->rtsi_shared_mux_reg <token> 0; <answer> = 
<token> (i = 0; i < 4; ++i) <answer> for 
set_ith_rtsi_brd_reg(i, <token> dev); <answer> 0, 
memset(devpriv->rtsi_shared_mux_usage, <token> <answer> 0, 
static int get_output_select_source(int dest, struct comedi_device <token> <answer> *dev) 
struct ni_private *devpriv = <token> <answer> dev->private; 
int reg = <token> <answer> -1; 
if <token> { <answer> (channel_is_pfi(dest)) 
if (ni_get_pfi_direction(dev, <token> == COMEDI_OUTPUT) <answer> dest) 
<token> = ni_get_pfi_routing(dev, dest); <answer> reg 
<token> else if (channel_is_rtsi(dest)) { <answer> } 
<token> (ni_get_rtsi_direction(dev, dest) == COMEDI_OUTPUT) { <answer> if 
reg = <token> dest); <answer> ni_get_rtsi_routing(dev, 
if (reg == <token> { <answer> NI_RTSI_OUTPUT_RGOUT0) 
<token> -= NI_CtrOut(0); <answer> dest 
if (dest > <token> <answer> 1) 
static int test_route(unsigned int src, unsigned <token> dest, <answer> int 
struct comedi_device <token> <answer> *dev) 
struct ni_private <token> = dev->private; <answer> *devpriv 
s8 reg = ni_route_to_register(CR_CHAN(src), <token> <answer> dest, 
if (reg <token> 0) <answer> < 
<token> -1; <answer> return 
if (get_output_select_source(dest, dev) != <token> <answer> CR_CHAN(src)) 
return <token> <answer> 0; 
<token> 1; <answer> return 
dest <token> NI_CtrOut(0); <answer> -= 
if <token> > 1) <answer> (dest 
<token> dest, <answer> ni_tio_set_routing(devpriv->counter_dev, 
reg | (src & <token> <answer> ~CR_CHAN(-1))); 
} else <token> <answer> { 
<token> -EINVAL; <answer> return 
<token> 0; <answer> return 
static <token> disconnect_route(unsigned int src, unsigned int dest, <answer> int 
struct comedi_device <token> <answer> *dev) 
<token> ni_private *devpriv = dev->private; <answer> struct 
<token> reg = ni_route_to_register(CR_CHAN(src), dest, <answer> s8 
if <token> < 0) <answer> (reg 
dest <token> NI_CtrOut(0); <answer> -= 
if (dest <token> 1) <answer> > 
<token> -EINVAL; <answer> return 
<token> 1; <answer> return 
<token> PCIDMA <answer> #ifdef 
static int ni_gpct_cmd(struct <token> *dev, struct comedi_subdevice *s) <answer> comedi_device 
struct <token> *counter = s->private; <answer> ni_gpct 
int <token> <answer> retval; 
retval <token> ni_request_gpct_mite_channel(dev, counter->counter_index, <answer> = 
if (retval) <token> <answer> { 
"no <token> channel available for use by counter\n"); <answer> dma 
return <token> <answer> retval; 
ni_e_series_enable_second_irq(dev, <token> 1); <answer> counter->counter_index, 
return <token> s); <answer> ni_tio_cmd(dev, 
static int ni_gpct_cancel(struct comedi_device <token> struct comedi_subdevice *s) <answer> *dev, 
struct <token> *counter = s->private; <answer> ni_gpct 
int <token> <answer> retval; 
retval <token> ni_tio_cancel(counter); <answer> = 
ni_e_series_enable_second_irq(dev, <token> 0); <answer> counter->counter_index, 
<token> counter->counter_index); <answer> ni_release_gpct_mite_channel(dev, 
<token> retval; <answer> return 
static irqreturn_t ni_E_interrupt(int irq, <token> *d) <answer> void 
struct <token> *dev = d; <answer> comedi_device 
struct comedi_subdevice *s_ai <token> dev->read_subdev; <answer> = 
<token> comedi_subdevice *s_ao = dev->write_subdev; <answer> struct 
unsigned <token> a_status; <answer> short 
unsigned <token> b_status; <answer> short 
unsigned long <token> <answer> flags; 
<token> PCIDMA <answer> #ifdef 
<token> ni_private *devpriv = dev->private; <answer> struct 
<token> (!dev->attached) <answer> if 
return <token> <answer> IRQ_NONE; 
dev->insn_device_config <token> ni_global_insn_config; <answer> = 
dev->get_valid_routes <token> _ni_get_valid_routes; <answer> = 
if (board->n_aochan > MAX_N_AO_CHAN) <token> <answer> { 
<token> "bug! n_aochan > MAX_N_AO_CHAN\n"); <answer> dev_err(dev->class_dev, 
<token> -EINVAL; <answer> return 
if <token> && (board->ao_fifo_depth || devpriv->mite)) { <answer> (dev->irq 
dev->write_subdev = <token> <answer> s; 
s->subdev_flags |= <token> <answer> SDF_CMD_WRITE; 
s->len_chanlist <token> s->n_chan; <answer> = 
s->do_cmdtest <token> ni_ao_cmdtest; <answer> = 
s->do_cmd <token> ni_ao_cmd; <answer> = 
<token> = ni_ao_reset; <answer> s->cancel 
if <token> <answer> (!devpriv->is_m_series) 
s->munge = <token> <answer> ni_ao_munge; 
<token> (devpriv->mite) <answer> if 
s->async_dma_dir <token> DMA_TO_DEVICE; <answer> = 
<token> (devpriv->is_67xx) <answer> if 
<token> s); <answer> init_ao_67xx(dev, 
#include <token> <answer> <linux/errno.h> 
#include <token> <answer> <linux/init.h> 
#include <token> <answer> <linux/types.h> 
<token> <linux/interrupt.h> <answer> #include 
#include <token> <answer> <linux/ioport.h> 
#include <token> <answer> <linux/irqchip.h> 
#include <token> <answer> <linux/of_address.h> 
#include <token> <answer> <linux/of_irq.h> 
<token> <linux/timex.h> <answer> #include 
<token> <linux/slab.h> <answer> #include 
#include <token> <answer> <linux/delay.h> 
<token> <asm/io.h> <answer> #include 
<token> ingenic_intc_data { <answer> struct 
void __iomem <token> <answer> *base; 
struct <token> *domain; <answer> irq_domain 
unsigned <token> <answer> num_chips; 
#define <token> 0x00 <answer> JZ_REG_INTC_STATUS 
#define JZ_REG_INTC_MASK <token> <answer> 0x04 
#define <token> 0x08 <answer> JZ_REG_INTC_SET_MASK 
#define JZ_REG_INTC_CLEAR_MASK <token> <answer> 0x0c 
<token> JZ_REG_INTC_PENDING 0x10 <answer> #define 
#define <token> 0x20 <answer> CHIP_SIZE 
static irqreturn_t <token> irq, void *data) <answer> intc_cascade(int 
struct ingenic_intc_data <token> = irq_get_handler_data(irq); <answer> *intc 
struct irq_domain <token> = intc->domain; <answer> *domain 
struct <token> *gc; <answer> irq_chip_generic 
uint32_t <token> <answer> pending; 
unsigned <token> <answer> i; 
for <token> = 0; i < intc->num_chips; i++) { <answer> (i 
gc = irq_get_domain_generic_chip(domain, i * <token> <answer> 32); 
<token> = irq_reg_readl(gc, JZ_REG_INTC_PENDING); <answer> pending 
if <token> <answer> (!pending) 
while (pending) <token> <answer> { 
int bit = <token> <answer> __fls(pending); 
generic_handle_domain_irq(domain, bit + (i * <token> <answer> 32)); 
<token> &= ~BIT(bit); <answer> pending 
<token> IRQ_HANDLED; <answer> return 
static int __init ingenic_intc_of_init(struct <token> *node, <answer> device_node 
unsigned <token> <answer> num_chips) 
struct <token> *intc; <answer> ingenic_intc_data 
struct irq_chip_generic <token> <answer> *gc; 
struct irq_chip_type <token> <answer> *ct; 
struct irq_domain <token> <answer> *domain; 
int <token> err = 0; <answer> parent_irq, 
unsigned <token> <answer> i; 
intc = <token> GFP_KERNEL); <answer> kzalloc(sizeof(*intc), 
if <token> { <answer> (!intc) 
err <token> -ENOMEM; <answer> = 
<token> out_err; <answer> goto 
parent_irq = <token> 0); <answer> irq_of_parse_and_map(node, 
if <token> { <answer> (!parent_irq) 
<token> = -EINVAL; <answer> err 
<token> out_free; <answer> goto 
err = irq_set_handler_data(parent_irq, <token> <answer> intc); 
<token> (err) <answer> if 
<token> out_unmap_irq; <answer> goto 
<token> = num_chips; <answer> intc->num_chips 
intc->base <token> of_iomap(node, 0); <answer> = 
if (!intc->base) <token> <answer> { 
<token> = -ENODEV; <answer> err 
goto <token> <answer> out_unmap_irq; 
domain = <token> num_chips * 32, <answer> irq_domain_add_linear(node, 
&irq_generic_chip_ops, <token> <answer> NULL); 
<token> (!domain) { <answer> if 
err <token> -ENOMEM; <answer> = 
goto <token> <answer> out_unmap_base; 
<token> = domain; <answer> intc->domain 
err <token> irq_alloc_domain_generic_chips(domain, 32, 1, "INTC", <answer> = 
<token> 0, <answer> handle_level_irq, 
IRQ_NOPROBE | IRQ_LEVEL, <token> <answer> 0); 
if <token> <answer> (err) 
goto <token> <answer> out_domain_remove; 
for (i = 0; <token> < num_chips; i++) { <answer> i 
gc = <token> i * 32); <answer> irq_get_domain_generic_chip(domain, 
<token> = IRQ_MSK(32); <answer> gc->wake_enabled 
gc->reg_base = intc->base + <token> * CHIP_SIZE); <answer> (i 
<token> = gc->chip_types; <answer> ct 
ct->regs.enable <token> JZ_REG_INTC_CLEAR_MASK; <answer> = 
ct->regs.disable = <token> <answer> JZ_REG_INTC_SET_MASK; 
<token> = irq_gc_unmask_enable_reg; <answer> ct->chip.irq_unmask 
ct->chip.irq_mask = <token> <answer> irq_gc_mask_disable_reg; 
ct->chip.irq_mask_ack <token> irq_gc_mask_disable_reg; <answer> = 
ct->chip.irq_set_wake <token> irq_gc_set_wake; <answer> = 
ct->chip.flags = <token> <answer> IRQCHIP_MASK_ON_SUSPEND; 
<token> <linux/stddef.h> <answer> #include 
#include <token> <answer> <linux/kernel.h> 
<token> <linux/pci.h> <answer> #include 
#include <token> <answer> <linux/kdev_t.h> 
#include <token> <answer> <linux/delay.h> 
<token> <linux/seq_file.h> <answer> #include 
<token> <linux/interrupt.h> <answer> #include 
<token> <linux/of.h> <answer> #include 
#include <token> <answer> <asm/time.h> 
<token> <asm/machdep.h> <answer> #include 
<token> <asm/pci-bridge.h> <answer> #include 
#include <token> <answer> <mm/mmu_decl.h> 
<token> <asm/udbg.h> <answer> #include 
#include <token> <answer> <asm/mpic.h> 
<token> <asm/swiotlb.h> <answer> #include 
#include <token> <answer> <sysdev/fsl_soc.h> 
<token> <sysdev/fsl_pci.h> <answer> #include 
<token> "mpc85xx.h" <answer> #include 
static void <token> mpc8536_ds_pic_init(void) <answer> __init 
struct <token> *mpic = mpic_alloc(NULL, 0, MPIC_BIG_ENDIAN, <answer> mpic 
0, 256, <token> OpenPIC "); <answer> " 
<token> == NULL); <answer> BUG_ON(mpic 
static void <token> mpc8536_ds_setup_arch(void) <answer> __init 
<token> (ppc_md.progress) <answer> if 
<token> 0); <answer> ppc_md.progress("mpc8536_ds_setup_arch()", 
printk("MPC8536 DS <token> from Freescale Semiconductor\n"); <answer> board 
<token> mpc85xx_common_publish_devices); <answer> machine_arch_initcall(mpc8536_ds, 
define_machine(mpc8536_ds) <token> <answer> { 
.name = "MPC8536 <token> <answer> DS", 
.compatible = <token> <answer> "fsl,mpc8536ds", 
<token> = mpc8536_ds_setup_arch, <answer> .setup_arch 
.init_IRQ <token> mpc8536_ds_pic_init, <answer> = 
<token> CONFIG_PCI <answer> #ifdef 
.pcibios_fixup_bus <token> fsl_pcibios_fixup_bus, <answer> = 
.pcibios_fixup_phb <token> fsl_pcibios_fixup_phb, <answer> = 
.get_irq <token> mpic_get_irq, <answer> = 
.progress <token> udbg_progress, <answer> = 
<token> "clk-gate.h" <answer> #include 
<token> "clk-mtk.h" <answer> #include 
#include <token> <answer> <dt-bindings/clock/mt8195-clk.h> 
<token> <linux/clk-provider.h> <answer> #include 
#include <token> <answer> <linux/platform_device.h> 
static const struct mtk_gate_regs <token> = { <answer> ipe_cg_regs 
<token> = 0x0, <answer> .set_ofs 
<token> = 0x0, <answer> .clr_ofs 
.sta_ofs = <token> <answer> 0x0, 
#define GATE_IPE(_id, _name, _parent, <token> \ <answer> _shift) 
<token> _name, _parent, &ipe_cg_regs, _shift, &mtk_clk_gate_ops_no_setclr) <answer> GATE_MTK(_id, 
static const struct <token> ipe_clks[] = { <answer> mtk_gate 
<token> "ipe_dpe", "top_ipe", 0), <answer> GATE_IPE(CLK_IPE_DPE, 
<token> "ipe_fdvt", "top_ipe", 1), <answer> GATE_IPE(CLK_IPE_FDVT, 
GATE_IPE(CLK_IPE_ME, <token> "top_ipe", 2), <answer> "ipe_me", 
GATE_IPE(CLK_IPE_TOP, <token> "top_ipe", 3), <answer> "ipe_top", 
GATE_IPE(CLK_IPE_SMI_LARB12, "ipe_smi_larb12", "top_ipe", <token> <answer> 4), 
static const struct <token> ipe_desc = { <answer> mtk_clk_desc 
.clks <token> ipe_clks, <answer> = 
<token> = ARRAY_SIZE(ipe_clks), <answer> .num_clks 
<token> const struct of_device_id of_match_clk_mt8195_ipe[] = { <answer> static 
<token> = "mediatek,mt8195-ipesys", <answer> .compatible 
<token> = &ipe_desc, <answer> .data 
<token> { <answer> }, 
<token> <linux/amba/pl08x.h> <answer> #include 
<token> <linux/amba/bus.h> <answer> #include 
<token> <linux/bug.h> <answer> #include 
#include <token> <answer> <linux/err.h> 
#include <token> <answer> <linux/io.h> 
<token> <linux/spinlock_types.h> <answer> #include 
<token> "spear.h" <answer> #include 
#include <token> <answer> "misc_regs.h" 
#include <token> <answer> "pl080.h" 
static spinlock_t lock <token> __SPIN_LOCK_UNLOCKED(x); <answer> = 
<token> { <answer> struct 
unsigned <token> busy; <answer> char 
unsigned <token> val; <answer> char 
} signals[16] <token> {{0, 0}, }; <answer> = 
int pl080_get_signal(const struct <token> *cd) <answer> pl08x_channel_data 
unsigned <token> signal = cd->min_signal, val; <answer> int 
<token> long flags; <answer> unsigned 
<token> flags); <answer> spin_lock_irqsave(&lock, 
val &= <token> << (signal * 2)); <answer> ~(0x3 
val |= cd->muxval << (signal * <token> <answer> 2); 
writel(val, <token> <answer> DMA_CHN_CFG); 
<token> = cd->muxval; <answer> signals[signal].val 
<token> flags); <answer> spin_unlock_irqrestore(&lock, 
return <token> <answer> signal; 
void pl080_put_signal(const struct pl08x_channel_data *cd, int <token> <answer> signal) 
<token> long flags; <answer> unsigned 
<token> flags); <answer> spin_lock_irqsave(&lock, 
#include <token> <answer> <linux/backlight.h> 
<token> <linux/err.h> <answer> #include 
<token> <linux/i2c.h> <answer> #include 
#include <token> <answer> <linux/module.h> 
#include <token> <answer> <linux/of.h> 
#include <token> <answer> <linux/slab.h> 
<token> arcxcnn_chip_id { <answer> enum 
struct arcxcnn_platform_data <token> <answer> { 
<token> char *name; <answer> const 
u16 <token> <answer> initial_brightness; 
u8 <token> <answer> leden; 
<token> led_config_0; <answer> u8 
u8 <token> <answer> led_config_1; 
u8 <token> <answer> dim_freq; 
u8 <token> <answer> comp_config; 
<token> filter_config; <answer> u8 
u8 <token> <answer> trim_config; 
<token> "ptrace.h" <answer> #include 
<token> "ptrace-gpr.h" <answer> #include 
#include <token> <answer> "tm.h" 
shared_info = bpf_core_cast(kskb->head <token> kskb->end, struct skb_shared_info); <answer> + 
meta_len = <token> <answer> shared_info->meta_len; 
<token> = shared_info->frag_list->len; <answer> frag0_len 
#include <token> <answer> <acpi/acpi.h> 
<token> acpi_status <answer> static 
<token> obj_handle, <answer> acpi_ns_dump_one_device(acpi_handle 
u32 level, void <token> void **return_value) <answer> *context, 
struct <token> buffer; <answer> acpi_buffer 
<token> acpi_device_info *info; <answer> struct 
<token> status; <answer> acpi_status 
u32 <token> <answer> i; 
status <token> <answer> = 
<token> level, context, return_value); <answer> acpi_ns_dump_one_object(obj_handle, 
buffer.length = <token> <answer> ACPI_ALLOCATE_LOCAL_BUFFER; 
<token> = acpi_get_object_info(obj_handle, &buffer); <answer> status 
if <token> { <answer> (ACPI_SUCCESS(status)) 
info = <token> <answer> buffer.pointer; 
for (i <token> 0; i < level; i++) { <answer> = 
<token> " ")); <answer> ACPI_DEBUG_PRINT_RAW((ACPI_DB_TABLES, 
" HID: %s, <token> %8.8X%8.8X\n", <answer> ADR: 
<token> (status); <answer> return 
<token> acpi_ns_dump_root_devices(void) <answer> void 
<token> sys_bus_handle; <answer> acpi_handle 
acpi_status <token> <answer> status; 
#include <token> <answer> <linux/clk.h> 
<token> <linux/platform_device.h> <answer> #include 
#include <token> <answer> <linux/mfd/twl6040.h> 
<token> <linux/module.h> <answer> #include 
<token> <linux/of.h> <answer> #include 
#include <token> <answer> <sound/core.h> 
#include <token> <answer> <sound/pcm.h> 
#include <token> <answer> <sound/soc.h> 
#include <token> <answer> <sound/jack.h> 
#include <token> <answer> "omap-dmic.h" 
#include <token> <answer> "omap-mcpdm.h" 
#include <token> <answer> "../codecs/twl6040.h" 
struct <token> { <answer> abe_twl6040 
struct <token> card; <answer> snd_soc_card 
<token> snd_soc_dai_link dai_links[2]; <answer> struct 
hs_trim = <token> TWL6040_TRIM_HSOTRIM); <answer> twl6040_get_trim_value(component, 
omap_mcpdm_configure_dn_offsets(rtd, <token> <answer> TWL6040_HSF_TRIM_LEFT(hs_trim), 
#include <token> <answer> <linux/aperture.h> 
#include <token> <answer> <linux/kernel.h> 
#include <token> <answer> <linux/efi.h> 
<token> <linux/efi-bgrt.h> <answer> #include 
#include <token> <answer> <linux/errno.h> 
#include <token> <answer> <linux/fb.h> 
#include <token> <answer> <linux/platform_device.h> 
#include <token> <answer> <linux/printk.h> 
<token> <linux/screen_info.h> <answer> #include 
#include <token> <answer> <video/vga.h> 
<token> <asm/efi.h> <answer> #include 
if (regno >= <token> <answer> info->cmap.len) 
return <token> <answer> 1; 
if (regno <token> 16) { <answer> < 
red >>= 16 - <token> <answer> info->var.red.length; 
<token> >>= 16 - info->var.green.length; <answer> green 
blue >>= 16 - <token> <answer> info->var.blue.length; 
<token> *)(info->pseudo_palette))[regno] = <answer> ((u32 
<token> << info->var.red.offset) | <answer> (red 
(green << info->var.green.offset) <token> <answer> | 
(blue << <token> <answer> info->var.blue.offset); 
return <token> <answer> 0; 
<token> defined CONFIG_FRAMEBUFFER_CONSOLE_DEFERRED_TAKEOVER && \ <answer> #if 
defined <token> <answer> CONFIG_ACPI_BGRT 
static void efifb_copy_bmp(u8 *src, u32 *dst, int width, const <token> screen_info *si) <answer> struct 
u8 r, <token> b; <answer> g, 
while (width--) <token> <answer> { 
b <token> *src++; <answer> = 
<token> = *src++; <answer> g 
r = <token> <answer> *src++; 
*dst++ = <token> << si->red_pos) | <answer> (r 
<token> << si->green_pos) | <answer> (g 
<token> << si->blue_pos); <answer> (b 
#ifdef <token> <answer> CONFIG_X86 
static bool efifb_bgrt_sanity_check(const struct <token> *si, u32 bmp_width) <answer> screen_info 
u32 expected_xoffset = <token> - bmp_width) / 2; <answer> (si->lfb_width 
<token> bgrt_tab.image_offset_x == expected_xoffset; <answer> return 
static bool efifb_bgrt_sanity_check(const struct screen_info <token> u32 bmp_width) <answer> *si, 
return <token> <answer> true; 
static void efifb_show_boot_graphics(struct fb_info *info, const <token> screen_info *si) <answer> struct 
u32 bmp_width, bmp_height, <token> dst_x, y, src_y; <answer> bmp_pitch, 
struct <token> *file_header; <answer> bmp_file_header 
<token> bmp_dib_header *dib_header; <answer> struct 
void <token> = NULL; <answer> *bgrt_image 
u8 *dst <token> info->screen_base; <answer> = 
if <token> <answer> (!use_bgrt) 
if (!bgrt_tab.image_address) <token> <answer> { 
pr_info("efifb: No BGRT, not showing boot <token> <answer> graphics\n"); 
if (bgrt_tab.status <token> 0x06) { <answer> & 
pr_info("efifb: BGRT rotation <token> set, not showing boot graphics\n"); <answer> bits 
static void efifb_destroy(struct <token> *info) <answer> fb_info 
struct efifb_par *par <token> info->par; <answer> = 
if (info->screen_base) <token> <answer> { 
if (mem_flags & (EFI_MEMORY_UC | <token> <answer> EFI_MEMORY_WC)) 
if <token> <answer> (request_mem_succeeded) 
<token> par->size); <answer> release_mem_region(par->base, 
<token> const struct fb_ops efifb_ops = { <answer> static 
.owner = <token> <answer> THIS_MODULE, 
.fb_destroy = <token> <answer> efifb_destroy, 
.fb_setcolreg = <token> <answer> efifb_setcolreg, 
static int efifb_setup(struct screen_info *si, <token> *options) <answer> char 
char <token> <answer> *this_opt; 
if <token> && *options) { <answer> (options 
while ((this_opt = strsep(&options, ",")) <token> NULL) { <answer> != 
if (!*this_opt) <token> <answer> continue; 
<token> this_opt); <answer> efifb_setup_from_dmi(si, 
<token> (!strncmp(this_opt, "base:", 5)) <answer> if 
<token> = simple_strtoul(this_opt+5, NULL, 0); <answer> si->lfb_base 
else <token> (!strncmp(this_opt, "stride:", 7)) <answer> if 
si->lfb_linelength = simple_strtoul(this_opt+7, <token> 0) * 4; <answer> NULL, 
<token> if (!strncmp(this_opt, "height:", 7)) <answer> else 
<token> = simple_strtoul(this_opt+7, NULL, 0); <answer> si->lfb_height 
else <token> (!strncmp(this_opt, "width:", 6)) <answer> if 
si->lfb_width = simple_strtoul(this_opt+6, <token> 0); <answer> NULL, 
else if <token> "nowc")) <answer> (!strcmp(this_opt, 
<token> &= ~EFI_MEMORY_WC; <answer> mem_flags 
else if <token> "nobgrt")) <answer> (!strcmp(this_opt, 
use_bgrt = <token> <answer> false; 
<token> 0; <answer> return 
<token> inline bool fb_base_is_valid(struct screen_info *si) <answer> static 
if <token> <answer> (si->lfb_base) 
<token> true; <answer> return 
if <token> & VIDEO_CAPABILITY_64BIT_BASE)) <answer> (!(si->capabilities 
return <token> <answer> false; 
<token> (si->ext_lfb_base) <answer> if 
<token> true; <answer> return 
<token> false; <answer> return 
#define <token> fmt) \ <answer> efifb_attr_decl(name, 
static ssize_t <token> device *dev, \ <answer> name##_show(struct 
struct device_attribute *attr, <token> <answer> \ 
char <token> \ <answer> *buf) 
<token> \ <answer> { 
struct screen_info *si = <token> \ <answer> dev_get_platdata(dev); 
if <token> \ <answer> (!si) 
return <token> \ <answer> -ENODEV; 
return sprintf(buf, fmt "\n", <token> \ <answer> (si->lfb_##name)); 
} <token> <answer> \ 
static <token> <answer> DEVICE_ATTR_RO(name) 
<token> "0x%x"); <answer> efifb_attr_decl(base, 
<token> "%u"); <answer> efifb_attr_decl(linelength, 
efifb_attr_decl(height, <token> <answer> "%u"); 
<token> "%u"); <answer> efifb_attr_decl(width, 
efifb_attr_decl(depth, <token> <answer> "%u"); 
<token> struct attribute *efifb_attrs[] = { <answer> static 
static int efifb_probe(struct platform_device <token> <answer> *dev) 
<token> screen_info *si; <answer> struct 
<token> fb_info *info; <answer> struct 
struct efifb_par <token> <answer> *par; 
int err, <token> <answer> orientation; 
unsigned <token> size_vmode; <answer> int 
unsigned <token> size_remap; <answer> int 
<token> int size_total; <answer> unsigned 
char <token> = NULL; <answer> *option 
efi_memory_desc_t <token> <answer> md; 
si = <token> <answer> dev_get_platdata(&dev->dev); 
if <token> <answer> (!si) 
<token> -ENODEV; <answer> return 
si = devm_kmemdup(&dev->dev, si, sizeof(*si), <token> <answer> GFP_KERNEL); 
<token> (!si) <answer> if 
<token> -ENOMEM; <answer> return 
if (si->orig_video_isVGA != <token> <answer> VIDEO_TYPE_EFI) 
<token> -ENODEV; <answer> return 
if <token> &option)) <answer> (fb_get_options("efifb", 
return <token> <answer> -ENODEV; 
<token> option); <answer> efifb_setup(si, 
<token> (!si->lfb_linelength) <answer> if 
return <token> <answer> -ENODEV; 
<token> (!si->lfb_depth) <answer> if 
<token> = 32; <answer> si->lfb_depth 
if <token> <answer> (!si->pages) 
si->pages <token> 1; <answer> = 
if <token> { <answer> (!fb_base_is_valid(si)) 
printk(KERN_DEBUG "efifb: <token> framebuffer address\n"); <answer> invalid 
return <token> <answer> -ENODEV; 
printk(KERN_INFO "efifb: probing <token> efifb\n"); <answer> for 
size_vmode <token> efifb_defined.yres * efifb_fix.line_length; <answer> = 
size_total <token> si->lfb_size; <answer> = 
if (size_total < <token> <answer> size_vmode) 
<token> = size_vmode; <answer> size_total 
size_remap = size_vmode <token> 2; <answer> * 
if (size_remap > <token> <answer> size_total) 
size_remap = <token> <answer> size_total; 
if (size_remap % <token> <answer> PAGE_SIZE) 
size_remap += PAGE_SIZE <token> (size_remap % PAGE_SIZE); <answer> - 
efifb_fix.smem_len = <token> <answer> size_remap; 
if (request_mem_region(efifb_fix.smem_start, size_remap, <token> { <answer> "efifb")) 
<token> = true; <answer> request_mem_succeeded 
} else <token> <answer> { 
pr_warn("efifb: <token> reserve video memory at 0x%lx\n", <answer> cannot 
<token> = framebuffer_alloc(sizeof(*par), &dev->dev); <answer> info 
<token> (!info) { <answer> if 
err = <token> <answer> -ENOMEM; 
<token> err_release_mem; <answer> goto 
platform_set_drvdata(dev, <token> <answer> info); 
par <token> info->par; <answer> = 
<token> = par->pseudo_palette; <answer> info->pseudo_palette 
par->base = <token> <answer> efifb_fix.smem_start; 
par->size <token> size_remap; <answer> = 
<token> (efi_enabled(EFI_MEMMAP) && <answer> if 
!efi_mem_desc_lookup(efifb_fix.smem_start, <token> { <answer> &md)) 
if ((efifb_fix.smem_start + <token> > <answer> efifb_fix.smem_len) 
(md.phys_addr + (md.num_pages <token> EFI_PAGE_SHIFT))) { <answer> << 
pr_err("efifb: video memory @ 0x%lx spans <token> EFI memory regions\n", <answer> multiple 
err <token> -EIO; <answer> = 
goto <token> <answer> err_release_fb; 
md.attribute &= EFI_MEMORY_UC | <token> | <answer> EFI_MEMORY_WC 
<token> | EFI_MEMORY_WB; <answer> EFI_MEMORY_WT 
if <token> { <answer> (md.attribute) 
mem_flags |= EFI_MEMORY_WT <token> EFI_MEMORY_WB; <answer> | 
mem_flags <token> md.attribute; <answer> &= 
if (mem_flags <token> EFI_MEMORY_WC) <answer> & 
info->screen_base = <token> <answer> ioremap_wc(efifb_fix.smem_start, 
else if (mem_flags & <token> <answer> EFI_MEMORY_UC) 
info->screen_base <token> ioremap(efifb_fix.smem_start, <answer> = 
else if (mem_flags & <token> <answer> EFI_MEMORY_WT) 
info->screen_base = <token> <answer> memremap(efifb_fix.smem_start, 
efifb_fix.smem_len, <token> <answer> MEMREMAP_WT); 
else if <token> & EFI_MEMORY_WB) <answer> (mem_flags 
<token> = memremap(efifb_fix.smem_start, <answer> info->screen_base 
efifb_fix.smem_len, <token> <answer> MEMREMAP_WB); 
<token> (!info->screen_base) { <answer> if 
pr_err("efifb: abort, cannot remap video <token> 0x%x @ 0x%lx\n", <answer> memory 
<token> efifb_fix.smem_start); <answer> efifb_fix.smem_len, 
err = <token> <answer> -EIO; 
<token> err_release_fb; <answer> goto 
efifb_show_boot_graphics(info, <token> <answer> si); 
pr_info("efifb: framebuffer at 0x%lx, using %dk, total <token> <answer> %dk\n", 
efifb_fix.smem_start, size_remap/1024, <token> <answer> size_total/1024); 
pr_info("efifb: <token> is %dx%dx%d, linelength=%d, pages=%d\n", <answer> mode 
<token> efifb_defined.yres, <answer> efifb_defined.xres, 
efifb_defined.bits_per_pixel, <token> <answer> efifb_fix.line_length, 
efifb_defined.xres_virtual <token> efifb_defined.xres; <answer> = 
efifb_defined.yres_virtual <token> efifb_fix.smem_len / <answer> = 
<token> scrolling: redraw\n"); <answer> pr_info("efifb: 
efifb_defined.yres_virtual = <token> <answer> efifb_defined.yres; 
#include <token> <answer> <kunit/test.h> 
#include <token> <answer> <linux/jiffies.h> 
#include <token> <answer> <linux/kernel.h> 
<token> <linux/kfence.h> <answer> #include 
<token> <linux/mm.h> <answer> #include 
#include <token> <answer> <linux/random.h> 
<token> <linux/slab.h> <answer> #include 
<token> <linux/spinlock.h> <answer> #include 
<token> <linux/string.h> <answer> #include 
#include <token> <answer> <linux/tracepoint.h> 
#include <token> <answer> <trace/events/printk.h> 
#include <token> <answer> <asm/kfence.h> 
#include <token> <answer> "kfence.h" 
<token> buf, min(len + 1, sizeof(observed.lines[0]))); <answer> strscpy(observed.lines[0], 
nlines = <token> <answer> 1; 
} else if (nlines == 1 && (strnstr(buf, "at 0x", len) || <token> "of 0x", len))) { <answer> strnstr(buf, 
strscpy(observed.lines[nlines++], <token> min(len + 1, sizeof(observed.lines[0]))); <answer> buf, 
flags |= <token> | SLAB_ACCOUNT; <answer> SLAB_NO_MERGE 
test_cache = <token> size, 1, flags, ctor); <answer> kmem_cache_create("test", 
KUNIT_ASSERT_TRUE_MSG(test, test_cache, "could not create <token> <answer> cache"); 
<token> size; <answer> return 
static <token> test_cache_destroy(void) <answer> void 
if <token> <answer> (!test_cache) 
test_cache = <token> <answer> NULL; 
<token> inline size_t kmalloc_cache_alignment(size_t size) <answer> static 
<token> allocation_policy { <answer> enum 
static void *test_alloc(struct kunit *test, <token> size, gfp_t gfp, enum allocation_policy policy) <answer> size_t 
void <token> <answer> *alloc; 
unsigned long timeout, <token> <answer> resched_after; 
const <token> *policy_name; <answer> char 
<token> (policy) { <answer> switch 
case <token> <answer> ALLOCATE_ANY: 
policy_name = <token> <answer> "any"; 
case <token> <answer> ALLOCATE_LEFT: 
<token> = "left"; <answer> policy_name 
<token> ALLOCATE_RIGHT: <answer> case 
policy_name = <token> <answer> "right"; 
case <token> <answer> ALLOCATE_NONE: 
<token> = "none"; <answer> policy_name 
kunit_info(test, "%s: size=%zu, gfp=%x, policy=%s, <token> __func__, size, gfp, <answer> cache=%i\n", 
<token> !!test_cache); <answer> policy_name, 
timeout = <token> + msecs_to_jiffies(100 * kfence_sample_interval); <answer> jiffies 
resched_after = jiffies <token> msecs_to_jiffies(kfence_sample_interval); <answer> + 
do <token> <answer> { 
<token> (test_cache) <answer> if 
alloc = <token> gfp); <answer> kmem_cache_alloc(test_cache, 
<token> = kmalloc(size, gfp); <answer> alloc 
<token> (is_kfence_address(alloc)) { <answer> if 
struct slab <token> = virt_to_slab(alloc); <answer> *slab 
enum <token> type = kmalloc_type(GFP_KERNEL, _RET_IP_); <answer> kmalloc_cache_type 
struct kmem_cache <token> = test_cache ?: <answer> *s 
<token> false)]; <answer> kmalloc_caches[type][__kmalloc_index(size, 
KUNIT_EXPECT_EQ(test, <token> slab, alloc), 0U); <answer> obj_to_index(s, 
KUNIT_EXPECT_EQ(test, objs_per_slab(s, <token> 1); <answer> slab), 
if <token> == ALLOCATE_ANY) <answer> (policy 
return <token> <answer> alloc; 
if <token> == ALLOCATE_LEFT && PAGE_ALIGNED(alloc)) <answer> (policy 
<token> alloc; <answer> return 
if (policy == ALLOCATE_RIGHT && <token> <answer> !PAGE_ALIGNED(alloc)) 
return <token> <answer> alloc; 
} else if (policy <token> ALLOCATE_NONE) <answer> == 
return <token> <answer> alloc; 
<token> (time_after(jiffies, resched_after)) <answer> if 
} while (time_before(jiffies, <token> <answer> timeout)); 
KUNIT_ASSERT_TRUE_MSG(test, false, "failed to allocate <token> KFENCE"); <answer> from 
<token> (!test_cache) <answer> if 
<token> = kmalloc_cache_alignment(size); <answer> size 
<token> void test_kmalloc_aligned_oob_read(struct kunit *test) <answer> static 
const size_t <token> = 73; <answer> size 
const size_t <token> = kmalloc_cache_alignment(size); <answer> align 
struct expect_report expect <token> { <answer> = 
.type = <token> <answer> KFENCE_ERROR_OOB, 
<token> = test_kmalloc_aligned_oob_read, <answer> .fn 
.is_write <token> false, <answer> = 
<token> *buf; <answer> char 
buf = test_alloc(test, size, <token> ALLOCATE_RIGHT); <answer> GFP_KERNEL, 
READ_ONCE(*(buf <token> 1)); <answer> - 
<token> report_available()); <answer> KUNIT_EXPECT_FALSE(test, 
READ_ONCE(*(buf <token> size)); <answer> + 
KUNIT_EXPECT_FALSE(test, <token> <answer> report_available()); 
expect.addr = buf + <token> <answer> size; 
WRITE_ONCE(*expect.addr, <token> + 1); <answer> READ_ONCE(*expect.addr) 
<token> report_available()); <answer> KUNIT_EXPECT_FALSE(test, 
<token> report_matches(&expect)); <answer> KUNIT_EXPECT_TRUE(test, 
<token> expect.addr[i], (char)0); <answer> KUNIT_EXPECT_EQ(test, 
timeout = jiffies + msecs_to_jiffies(100 <token> kfence_sample_interval); <answer> * 
<token> { <answer> do 
<token> *objects[100]; <answer> void 
int i, num = kmem_cache_alloc_bulk(test_cache, GFP_ATOMIC, <token> <answer> ARRAY_SIZE(objects), 
<token> (!num) <answer> if 
for (i = <token> i < ARRAY_SIZE(objects); i++) { <answer> 0; 
if <token> { <answer> (is_kfence_address(objects[i])) 
pass = <token> <answer> true; 
<token> num, objects); <answer> kmem_cache_free_bulk(test_cache, 
} while (!pass && time_before(jiffies, <token> <answer> timeout)); 
KUNIT_EXPECT_TRUE(test, <token> <answer> pass); 
<token> report_available()); <answer> KUNIT_EXPECT_FALSE(test, 
#define <token> \ <answer> KFENCE_KUNIT_CASE(test_name) 
{ .run_case = test_name, <token> = #test_name }, \ <answer> .name 
{ .run_case = test_name, .name = <token> "-memcache" } <answer> #test_name 
static struct kunit_case kfence_test_cases[] <token> { <answer> = 
<token> "common.h" <answer> #include 
<token> <linux/types.h> <answer> #include 
<token> <linux/errno.h> <answer> #include 
#include <token> <answer> <linux/pci.h> 
#include <token> <answer> <linux/ktime.h> 
<token> <linux/netdevice.h> <answer> #include 
#include <token> <answer> <linux/etherdevice.h> 
#include <token> <answer> <linux/if_vlan.h> 
#include <token> <answer> <linux/skbuff.h> 
#include <token> <answer> <linux/mm.h> 
#include <token> <answer> <linux/tcp.h> 
<token> <linux/ip.h> <answer> #include 
<token> <linux/in.h> <answer> #include 
<token> <linux/if_arp.h> <answer> #include 
<token> <linux/slab.h> <answer> #include 
<token> <linux/prefetch.h> <answer> #include 
<token> "cpl5_cmd.h" <answer> #include 
<token> "sge.h" <answer> #include 
<token> "regs.h" <answer> #include 
#include <token> <answer> "espi.h" 
#define <token> (HZ / 4) <answer> TX_RECLAIM_PERIOD 
<token> M_CMD_LEN 0x7fffffff <answer> #define 
#define <token> (v) <answer> V_CMD_LEN(v) 
#define G_CMD_LEN(v) <token> & M_CMD_LEN) <answer> ((v) 
#define V_CMD_GEN1(v) ((v) <token> 31) <answer> << 
<token> V_CMD_GEN2(v) (v) <answer> #define 
#define <token> (1 << 1) <answer> F_CMD_DATAVALID 
#define <token> (1 << 2) <answer> F_CMD_SOP 
#define V_CMD_EOP(v) ((v) <token> 3) <answer> << 
#if <token> <answer> defined(__BIG_ENDIAN_BITFIELD) 
<token> cmdQ_e { <answer> struct 
<token> addr_lo; <answer> u32 
u32 <token> <answer> len_gen; 
u32 <token> <answer> flags; 
<token> addr_hi; <answer> u32 
<token> freelQ_e { <answer> struct 
<token> addr_lo; <answer> u32 
u32 <token> <answer> len_gen; 
<token> gen2; <answer> u32 
<token> addr_hi; <answer> u32 
<token> respQ_e { <answer> struct 
u32 <token> : 4; <answer> Qsleeping 
<token> Cmdq1CreditReturn : 5; <answer> u32 
<token> Cmdq1DmaComplete : 5; <answer> u32 
<token> Cmdq0CreditReturn : 5; <answer> u32 
u32 Cmdq0DmaComplete : <token> <answer> 5; 
u32 FreelistQid : <token> <answer> 2; 
<token> CreditValid : 1; <answer> u32 
u32 DataValid : <token> <answer> 1; 
u32 Offload : <token> <answer> 1; 
<token> Eop : 1; <answer> u32 
<token> Sop : 1; <answer> u32 
<token> GenerationBit : 1; <answer> u32 
<token> BufferLength; <answer> u32 
#elif <token> <answer> defined(__LITTLE_ENDIAN_BITFIELD) 
struct cmdQ_e <token> <answer> { 
<token> len_gen; <answer> u32 
<token> addr_lo; <answer> u32 
u32 <token> <answer> addr_hi; 
u32 <token> <answer> flags; 
<token> freelQ_e { <answer> struct 
u32 <token> <answer> len_gen; 
<token> addr_lo; <answer> u32 
u32 <token> <answer> addr_hi; 
u32 <token> <answer> gen2; 
<token> respQ_e { <answer> struct 
<token> BufferLength; <answer> u32 
<token> GenerationBit : 1; <answer> u32 
u32 Sop <token> 1; <answer> : 
u32 Eop <token> 1; <answer> : 
u32 Offload : <token> <answer> 1; 
<token> DataValid : 1; <answer> u32 
u32 <token> : 1; <answer> CreditValid 
u32 FreelistQid <token> 2; <answer> : 
u32 <token> : 5; <answer> Cmdq0DmaComplete 
<token> Cmdq0CreditReturn : 5; <answer> u32 
u32 Cmdq1DmaComplete : <token> <answer> 5; 
u32 <token> : 5; <answer> Cmdq1CreditReturn 
u32 <token> : 4; <answer> Qsleeping 
} <token> <answer> ; 
struct <token> { <answer> cmdQ_ce 
struct <token> *skb; <answer> sk_buff 
<token> freelQ_ce { <answer> struct 
<token> sk_buff *skb; <answer> struct 
struct <token> { <answer> cmdQ 
struct sge <token> <answer> { 
static <token> tx_sched_stop(struct sge *sge) <answer> void 
struct sched <token> = sge->tx_sched; <answer> *s 
<token> i; <answer> int 
for <token> = 0; i < MAX_NPORTS; i++) <answer> (i 
unsigned int t1_sched_update_parms(struct sge *sge, unsigned int <token> <answer> port, 
unsigned <token> mtu, unsigned int speed) <answer> int 
struct sched <token> = sge->tx_sched; <answer> *s 
struct <token> *p = &s->p[port]; <answer> sched_port 
unsigned <token> max_avail_segs; <answer> int 
pr_debug("%s mtu=%d speed=%d\n", __func__, <token> speed); <answer> mtu, 
if <token> <answer> (speed) 
<token> = speed; <answer> p->speed 
<token> (mtu) <answer> if 
p->mtu = <token> <answer> mtu; 
if (speed || <token> { <answer> mtu) 
unsigned long long drain = 1024ULL <token> p->speed * (p->mtu - 40); <answer> * 
do_div(drain, (p->mtu + 50) <token> 1000); <answer> * 
<token> = (unsigned int) drain; <answer> p->drain_bits_per_1024ns 
<token> (p->speed < 1000) <answer> if 
p->drain_bits_per_1024ns <token> <answer> = 
90 * p->drain_bits_per_1024ns / <token> <answer> 100; 
if (board_info(sge->adapter)->board == CHBT_BOARD_CHT204) <token> <answer> { 
<token> -= 16; <answer> p->drain_bits_per_1024ns 
s->max_avail = max(4096U, p->mtu <token> 16 + 14 + 4); <answer> + 
<token> = max(1U, 4096 / (p->mtu - 40)); <answer> max_avail_segs 
} else <token> <answer> { 
s->max_avail <token> 16384; <answer> = 
max_avail_segs <token> max(1U, 9000 / (p->mtu - 40)); <answer> = 
pr_debug("t1_sched_update_parms: mtu %u speed %u max_avail %u <token> <answer> " 
"max_avail_segs %u <token> %u\n", p->mtu, <answer> drain_bits_per_1024ns 
p->speed, <token> max_avail_segs, <answer> s->max_avail, 
return max_avail_segs * <token> - 40); <answer> (p->mtu 
<token> 0 <answer> #if 
void <token> sge *sge, unsigned int val) <answer> t1_sched_set_max_avail_bytes(struct 
struct sched *s <token> sge->tx_sched; <answer> = 
unsigned <token> i; <answer> int 
s->max_avail <token> val; <answer> = 
for (i = 0; <token> < MAX_NPORTS; i++) <answer> i 
t1_sched_update_parms(sge, i, <token> 0); <answer> 0, 
void t1_sched_set_drain_bits_per_us(struct sge *sge, <token> int port, <answer> unsigned 
<token> int val) <answer> unsigned 
struct sched <token> = sge->tx_sched; <answer> *s 
struct sched_port *p <token> &s->p[port]; <answer> = 
p->drain_bits_per_1024ns = <token> * 1024 / 1000; <answer> val 
<token> port, 0, 0); <answer> t1_sched_update_parms(sge, 
static int tx_sched_init(struct <token> *sge) <answer> sge 
struct sched <token> <answer> *s; 
<token> i; <answer> int 
s = kzalloc(sizeof (struct sched), <token> <answer> GFP_KERNEL); 
if <token> <answer> (!s) 
<token> -ENOMEM; <answer> return 
tasklet_setup(&s->sched_tsk, <token> <answer> restart_sched); 
s->sge <token> sge; <answer> = 
sge->tx_sched <token> s; <answer> = 
for (i = 0; i < MAX_NPORTS; i++) <token> <answer> { 
<token> i, 1500, 1000); <answer> t1_sched_update_parms(sge, 
return <token> <answer> 0; 
static inline int <token> sge *sge) <answer> sched_update_avail(struct 
struct sched *s <token> sge->tx_sched; <answer> = 
ktime_t now = <token> <answer> ktime_get(); 
<token> int i; <answer> unsigned 
long <token> delta_time_ns; <answer> long 
delta_time_ns <token> ktime_to_ns(ktime_sub(now, s->last_updated)); <answer> = 
pr_debug("sched_update_avail <token> delta_time_ns); <answer> delta=%lld\n", 
if (delta_time_ns <token> 15000) <answer> < 
return <token> <answer> 0; 
<token> (i = 0; i < MAX_NPORTS; i++) { <answer> for 
struct <token> *p = &s->p[i]; <answer> sched_port 
unsigned <token> delta_avail; <answer> int 
delta_avail = (p->drain_bits_per_1024ns * delta_time_ns) >> <token> <answer> 13; 
p->avail = min(p->avail <token> delta_avail, s->max_avail); <answer> + 
s->last_updated = <token> <answer> now; 
<token> 1; <answer> return 
static struct <token> *sched_skb(struct sge *sge, struct sk_buff *skb, <answer> sk_buff 
unsigned int <token> <answer> credits) 
struct sched <token> = sge->tx_sched; <answer> *s 
<token> sk_buff_head *skbq; <answer> struct 
unsigned int i, len, update <token> 1; <answer> = 
<token> %p\n", skb); <answer> pr_debug("sched_skb 
if (!skb) <token> <answer> { 
if <token> <answer> (!s->num) 
return <token> <answer> NULL; 
} <token> { <answer> else 
<token> = &s->p[skb->dev->if_port].skbq; <answer> skbq 
<token> skb); <answer> __skb_queue_tail(skbq, 
skb = <token> <answer> NULL; 
if (credits < <token> + 1) <answer> MAX_SKB_FRAGS 
goto <token> <answer> out; 
<token> (i = 0; i < MAX_NPORTS; i++) { <answer> for 
s->port = (s->port + 1) & (MAX_NPORTS <token> 1); <answer> - 
<token> = &s->p[s->port].skbq; <answer> skbq 
<token> = skb_peek(skbq); <answer> skb 
<token> (!skb) <answer> if 
len <token> skb->len; <answer> = 
if <token> <= s->p[s->port].avail) { <answer> (len 
s->p[s->port].avail -= <token> <answer> len; 
__skb_unlink(skb, <token> <answer> skbq); 
goto <token> <answer> out; 
skb = <token> <answer> NULL; 
<token> (update-- && sched_update_avail(sge)) <answer> if 
goto <token> <answer> again; 
if (s->num && !skb) <token> <answer> { 
struct cmdQ *q = <token> <answer> &sge->cmdQ[0]; 
<token> &q->status); <answer> clear_bit(CMDQ_STAT_LAST_PKT_DB, 
if (test_and_set_bit(CMDQ_STAT_RUNNING, &q->status) == 0) <token> <answer> { 
set_bit(CMDQ_STAT_LAST_PKT_DB, <token> <answer> &q->status); 
<token> sge->adapter->regs + A_SG_DOORBELL); <answer> writel(F_CMDQ0_ENABLE, 
<token> ret %p\n", skb); <answer> pr_debug("sched_skb 
return <token> <answer> skb; 
static inline <token> doorbell_pio(struct adapter *adapter, u32 val) <answer> void 
writel(val, <token> + A_SG_DOORBELL); <answer> adapter->regs 
static void free_freelQ_buffers(struct pci_dev *pdev, <token> freelQ *q) <answer> struct 
unsigned int cidx <token> q->cidx; <answer> = 
<token> (q->credits--) { <answer> while 
<token> freelQ_ce *ce = &q->centries[cidx]; <answer> struct 
<token> dma_unmap_addr(ce, dma_addr), <answer> dma_unmap_single(&pdev->dev, 
dma_unmap_len(ce, <token> DMA_FROM_DEVICE); <answer> dma_len), 
<token> = NULL; <answer> ce->skb 
<token> (++cidx == q->size) <answer> if 
cidx = <token> <answer> 0; 
static <token> free_rx_resources(struct sge *sge) <answer> void 
struct pci_dev *pdev = <token> <answer> sge->adapter->pdev; 
<token> int size, i; <answer> unsigned 
if (sge->respQ.entries) <token> <answer> { 
size = <token> respQ_e) * sge->respQ.size; <answer> sizeof(struct 
dma_free_coherent(&pdev->dev, size, <token> <answer> sge->respQ.entries, 
for (i = 0; i <token> SGE_FREELQ_N; i++) { <answer> < 
struct freelQ *q = <token> <answer> &sge->freelQ[i]; 
<token> (q->centries) { <answer> if 
<token> q); <answer> free_freelQ_buffers(pdev, 
if <token> { <answer> (q->entries) 
size = <token> freelQ_e) * q->size; <answer> sizeof(struct 
dma_free_coherent(&pdev->dev, size, <token> <answer> q->entries, 
<token> int alloc_rx_resources(struct sge *sge, struct sge_params *p) <answer> static 
<token> pci_dev *pdev = sge->adapter->pdev; <answer> struct 
unsigned <token> size, i; <answer> int 
for (i = 0; <token> < SGE_FREELQ_N; i++) { <answer> i 
struct freelQ *q <token> &sge->freelQ[i]; <answer> = 
<token> = 1; <answer> q->genbit 
q->size = <token> <answer> p->freelQ_size[i]; 
q->dma_offset = sge->rx_pkt_pad ? 0 <token> NET_IP_ALIGN; <answer> : 
<token> = sizeof(struct freelQ_e) * q->size; <answer> size 
<token> = dma_alloc_coherent(&pdev->dev, size, <answer> q->entries 
&q->dma_addr, <token> <answer> GFP_KERNEL); 
<token> (!q->entries) <answer> if 
<token> err_no_mem; <answer> goto 
size = sizeof(struct <token> * q->size; <answer> freelQ_ce) 
q->centries = kzalloc(size, <token> <answer> GFP_KERNEL); 
<token> (!q->centries) <answer> if 
<token> err_no_mem; <answer> goto 
<token> = SGE_RX_SM_BUF_SIZE + <answer> sge->freelQ[!sge->jumbo_fl].rx_buffer_size 
sizeof(struct cpl_rx_data) <token> <answer> + 
<token> = (16 * 1024) - SKB_DATA_ALIGN(sizeof(struct skb_shared_info)); <answer> size 
<token> = size; <answer> sge->freelQ[sge->jumbo_fl].rx_buffer_size 
sge->freelQ[!sge->jumbo_fl].recycleq_idx <token> 0; <answer> = 
<token> = 1; <answer> sge->freelQ[sge->jumbo_fl].recycleq_idx 
<token> = 1; <answer> sge->respQ.genbit 
<token> = SGE_RESPQ_E_N; <answer> sge->respQ.size 
<token> = 0; <answer> sge->respQ.credits 
<token> = sizeof(struct respQ_e) * sge->respQ.size; <answer> size 
sge->respQ.entries <token> <answer> = 
dma_alloc_coherent(&pdev->dev, <token> &sge->respQ.dma_addr, <answer> size, 
<token> (!sge->respQ.entries) <answer> if 
<token> err_no_mem; <answer> goto 
<token> 0; <answer> return 
return <token> <answer> -ENOMEM; 
static void free_cmdQ_buffers(struct sge *sge, <token> cmdQ *q, unsigned int n) <answer> struct 
struct cmdQ_ce <token> <answer> *ce; 
struct pci_dev *pdev <token> sge->adapter->pdev; <answer> = 
unsigned int cidx = <token> <answer> q->cidx; 
q->in_use -= <token> <answer> n; 
ce = <token> <answer> &q->centries[cidx]; 
while (n--) <token> <answer> { 
<token> (likely(dma_unmap_len(ce, dma_len))) { <answer> if 
<token> dma_addr), <answer> dma_unmap_addr(ce, 
<token> dma_len), <answer> dma_unmap_len(ce, 
<token> (q->sop) <answer> if 
q->sop = <token> <answer> 0; 
<token> (ce->skb) { <answer> if 
<token> = 1; <answer> q->sop 
if (++cidx == q->size) <token> <answer> { 
<token> = 0; <answer> cidx 
ce <token> q->centries; <answer> = 
q->cidx <token> cidx; <answer> = 
static void free_tx_resources(struct <token> *sge) <answer> sge 
struct <token> *pdev = sge->adapter->pdev; <answer> pci_dev 
unsigned int <token> i; <answer> size, 
for (i <token> 0; i < SGE_CMDQ_N; i++) { <answer> = 
struct <token> *q = &sge->cmdQ[i]; <answer> cmdQ 
if (q->centries) <token> <answer> { 
if <token> <answer> (q->in_use) 
free_cmdQ_buffers(sge, <token> q->in_use); <answer> q, 
<token> (q->entries) { <answer> if 
<token> = sizeof(struct cmdQ_e) * q->size; <answer> size 
dma_free_coherent(&pdev->dev, size, <token> <answer> q->entries, 
<token> int alloc_tx_resources(struct sge *sge, struct sge_params *p) <answer> static 
struct pci_dev *pdev = <token> <answer> sge->adapter->pdev; 
<token> int size, i; <answer> unsigned 
for (i = 0; i < SGE_CMDQ_N; i++) <token> <answer> { 
struct cmdQ *q = <token> <answer> &sge->cmdQ[i]; 
q->genbit = <token> <answer> 1; 
<token> = 1; <answer> q->sop 
<token> = p->cmdQ_size[i]; <answer> q->size 
<token> = 0; <answer> q->in_use 
q->status = <token> <answer> 0; 
q->processed <token> q->cleaned = 0; <answer> = 
q->stop_thres <token> 0; <answer> = 
size = sizeof(struct cmdQ_e) <token> q->size; <answer> * 
q->entries <token> dma_alloc_coherent(&pdev->dev, size, <answer> = 
&q->dma_addr, <token> <answer> GFP_KERNEL); 
<token> (!q->entries) <answer> if 
<token> err_no_mem; <answer> goto 
size <token> sizeof(struct cmdQ_ce) * q->size; <answer> = 
<token> = kzalloc(size, GFP_KERNEL); <answer> q->centries 
if <token> <answer> (!q->centries) 
goto <token> <answer> err_no_mem; 
sge->cmdQ[0].stop_thres = <token> * <answer> sge->adapter->params.nports 
(MAX_SKB_FRAGS <token> 1); <answer> + 
<token> 0; <answer> return 
<token> -ENOMEM; <answer> return 
static inline void setup_ring_params(struct adapter *adapter, <token> addr, <answer> u64 
u32 <token> int base_reg_lo, <answer> size, 
<token> base_reg_hi, int size_reg) <answer> int 
writel((u32)addr, adapter->regs <token> base_reg_lo); <answer> + 
writel(addr >> 32, adapter->regs <token> base_reg_hi); <answer> + 
<token> adapter->regs + size_reg); <answer> writel(size, 
void <token> adapter *adapter, netdev_features_t features) <answer> t1_vlan_mode(struct 
struct sge *sge = <token> <answer> adapter->sge; 
if <token> & NETIF_F_HW_VLAN_CTAG_RX) <answer> (features 
<token> |= F_VLAN_XTRACT; <answer> sge->sge_control 
sge->sge_control &= <token> <answer> ~F_VLAN_XTRACT; 
if <token> { <answer> (adapter->open_device_map) 
writel(sge->sge_control, adapter->regs + <token> <answer> A_SG_CONTROL); 
static void configure_sge(struct <token> *sge, struct sge_params *p) <answer> sge 
struct adapter *ap <token> sge->adapter; <answer> = 
writel(0, ap->regs + <token> <answer> A_SG_CONTROL); 
<token> sge->cmdQ[0].dma_addr, sge->cmdQ[0].size, <answer> setup_ring_params(ap, 
A_SG_CMD0BASELWR, <token> A_SG_CMD0SIZE); <answer> A_SG_CMD0BASEUPR, 
setup_ring_params(ap, <token> sge->cmdQ[1].size, <answer> sge->cmdQ[1].dma_addr, 
A_SG_CMD1BASELWR, <token> A_SG_CMD1SIZE); <answer> A_SG_CMD1BASEUPR, 
<token> sge->freelQ[0].dma_addr, <answer> setup_ring_params(ap, 
<token> A_SG_FL0BASELWR, <answer> sge->freelQ[0].size, 
<token> A_SG_FL0SIZE); <answer> A_SG_FL0BASEUPR, 
<token> sge->freelQ[1].dma_addr, <answer> setup_ring_params(ap, 
sge->freelQ[1].size, <token> <answer> A_SG_FL1BASELWR, 
A_SG_FL1BASEUPR, <token> <answer> A_SG_FL1SIZE); 
static inline unsigned int jumbo_payload_capacity(const <token> sge *sge) <answer> struct 
return <token> - <answer> sge->freelQ[sge->jumbo_fl].rx_buffer_size 
<token> - <answer> sge->freelQ[sge->jumbo_fl].dma_offset 
sizeof(struct <token> <answer> cpl_rx_data); 
<token> t1_sge_destroy(struct sge *sge) <answer> void 
int <token> <answer> i; 
for_each_port(sge->adapter, <token> <answer> i) 
<token> void refill_free_list(struct sge *sge, struct freelQ *q) <answer> static 
<token> pci_dev *pdev = sge->adapter->pdev; <answer> struct 
struct freelQ_ce *ce = <token> <answer> &q->centries[q->pidx]; 
struct freelQ_e <token> = &q->entries[q->pidx]; <answer> *e 
unsigned <token> dma_len = q->rx_buffer_size - q->dma_offset; <answer> int 
<token> (q->credits < q->size) { <answer> while 
